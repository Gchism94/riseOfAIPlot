System,Domain,Task,Organization,Organization Categorization,Authors,Publication date,Reference,Link,Citations,Inclusion criteria,Inclusion criteria notes,Parameters,Parameters Notes,Training compute (FLOP),Training compute notes,Training dataset,Training dataset notes,Training dataset size (datapoints),Dataset size notes,Inference compute (FLOP),Inference compute notes,Training time (hours),Training time notes,Training hardware,Approach,Training compute cost (2020 USD),Compute cost notes,Self-supervised training,Compute Sponsor Categorization,Epistemic status,Abstract,Last Modified
Gen-2,Text-to-Video,Video generation,Runway,Industry,Gen-2 authors,12/31/23,,https://research.runwayml.com/gen2,0.00E+00,SOTA Improvement,"SOTA improvement over Stable Diffusion and Text2Live, paper forthcoming",,,,,,,,,,,,,,,,,,,Unverified,,8/15/23 20:44
Falcon 180B,Language,Language modelling,Technology Innovation Institute,Government,,9/6/23,Falcon LLM - Falcon 180B,https://falconllm.tii.ae/falcon-180b.html,0,SOTA Improvement,"It's currently at the top of the Hugging Face Leaderboard for pre-trained Open Large Language Models and is available for both research and commercial use..

This model performs exceptionally well in various tasks like reasoning, coding, proficiency, and knowledge tests, even beating competitors like Meta's LLaMA 2.",1.8E+11,Falcon 180B is a super-powerful language model with 180 billion parameters,3.78E+24,C = 6ND = 6 FLOP/token/parameter * 3.5 trillion tokens * 180 billion parameters = 3.78*10^24 FLOP,,,2.63E+12,3.5 trillion tokens * (~3 words per 4 tokens) ~= 2.625 trillion words,3.60E+11,C_inference = 2 FLOP / token / param * N => 360B FLOP per token,,,,,,,,,Likely,"Falcon 180B is a super-powerful language model with 180 billion parameters, trained on 3.5 trillion tokens. It's currently at the top of the Hugging Face Leaderboard for pre-trained Open Large Language Models and is available for both research and commercial use..

This model performs exceptionally well in various tasks like reasoning, coding, proficiency, and knowledge tests, even beating competitors like Meta's LLaMA 2.

Among closed source models, it ranks just behind OpenAI's GPT 4, and performs on par with Google's PaLM 2 Large, which powers Bard, despite being half the size of the model.",9/6/23 21:40
Swift,Robotics,Helicopter driving,Intel Labs,Industry - Academia Collaboration (Industry leaning),"Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio, Matthias Müller, Vladlen Koltun, Davide Scaramuzza ",8/30/23,Champion-level drone racing using deep reinforcement learning,https://www.nature.com/articles/s41586-023-06419-4,1.00E+00,SOTA Improvement,"Our work marks the first time, to our knowledge, that an autonomous mobile robot achieved world-champion-level performance in a real-world competitive sport.",2.11E+04,"The control network is an MLP with input dimension 31, two hidden layers of size 128, and an output of dimension 4.
(31+1)*128+(128+1)*128+(128+1)*4 = 21124",5.34E+16,"Policies are trained for a total of 1 × 108 environment interactions, which takes 50 min on a workstation (i9 12900K, RTX 3090, 32 GB RAM DDR5). Fine-tuning is performed for 2 × 107 environment interactions.

35.58 TFLOPS * 50 min * 60 s/min * 0.50 utilization = 5.337*10^16 FLOP",,,,,,,0.833,"50 minutes (training details, page 8)",NVIDIA GeForce RTX 3090,Reinforcement learning,,,,Industry,Likely,"First-person view (FPV) drone racing is a televised sport in which professional competitors pilot high-speed aircraft through a 3D circuit. Each pilot sees the environment from the perspective of their drone by means of video streamed from an onboard camera. Reaching the level of professional pilots with an autonomous drone is challenging because the robot needs to fly at its physical limits while estimating its speed and location in the circuit exclusively from onboard sensors1. Here we introduce Swift, an autonomous system that can race physical vehicles at the level of the human world champions. The system combines deep reinforcement learning (RL) in simulation with data collected in the physical world. Swift competed against three human champions, including the world champions of two international leagues, in real-world head-to-head races. Swift won several races against each of the human champions and demonstrated the fastest recorded race time. This work represents a milestone for mobile robotics and machine intelligence2, which may inspire the deployment of hybrid learning-based solutions in other physical systems.",9/6/23 15:39
Jais,Language,Language modelling,"Cerebras Systems,Mohamed bin Zayed University of Artificial Intelligence,Inception",Industry - Academia Collaboration (Industry leaning),"Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mujahid, Massa Baali, Alham Fikri Aji, Zhengzhong Liu, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Preslav Nakov, Timothy Baldwin, Eric Xing",8/29/23,Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models,https://inceptioniai.org/jais/docs/Technicalpaper.pdf,0.00E+00,SOTA Improvement,SOTA at Arabic language tasks.,13000000000,"""With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic""",3.08E+22,C = 6ND = 6 * 13 billion params * 395 billion tokens = 3.081e+22 FLOP,"Abu El-Khair, Aranews, ArabicText 2022, C4 Arabic, Arabic Wikipedia, ArabicNews 2020, Maktabah, United Nations Parallel Corpus, The Pile, Books3, arXiv, PubMed Central, OpenWebText2, English Wikipedia, FreeLaw, PubMed Abstracts, DeepMind Mathematics, Project Gutenberg, BookCorpus2, EuroParl, PhilPapers, YouTube Subtitles, NIH Grant Abstracts, Enron Emails, GitHub","It was pretrained on 395 billion tokens, including 116 billion Arabic tokens, 232 billion English tokens, and 46 billion tokens of code.
The Arabic data consists of 72 billion tokens, which was augmented by 18 billion tokens of translated English text and then upsampled 1.6 times to reach 116 billion tokens.
The English data is sampled from the Pile dataset and consists of 232 billion tokens.
The code data consists of 46 billion tokens sampled from GitHub.",3E+11,395B tokens ~= 300B words,2.60E+10,26 billion FLOP per token,600,2023 June 25 to July 18 = 25 days = 600 hours,,,,,,Industry,Confident,"We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture
and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model —the foundation Jais model, and an instruction-tuned Jais-chat variant— with the aim of promoting research on Arabic LLMs.",9/21/23 3:56
IDEFICS,Multimodal,Language modelling,Hugging Face,Industry,"Hugo Laurencon, Daniel van Strien, Stas Bekman, Leo Tronchon, Lucile Saulnier, Thomas Wang, Siddharth Karamcheti, Amanpreet Singh, Giada Pistilli, Yacine Jernite, Victor Sanh",8/22/23,Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model,https://huggingface.co/blog/idefics,0.00E+00,,,8.00E+10,IDEFICS... comes in two variants—the base version and the instructed version. Each variant is available at the 9 billion and 80 billion parameter sizes.,,,"Wikipedia, Public Multimodal Dataset, LAION, OBELICS","IDEFICS was trained on a mixture of openly available datasets: Wikipedia, Public Multimodal Dataset, and LAION, as well as a new 115B token dataset called OBELICS that we created. OBELICS consists of 141 million interleaved image-text documents scraped from the web and contains 353 million images.",,,,,,,,,,,,Industry,Speculative,"We are excited to release IDEFICS (Image-aware Decoder Enhanced à la Flamingo with Interleaved Cross-attentionS), an open-access visual language model. IDEFICS is based on Flamingo, a state-of-the-art visual language model initially developed by DeepMind, which has not been released publicly. Similarly to GPT-4, the model accepts arbitrary sequences of image and text inputs and produces text outputs.",8/28/23 13:48
CALM,Robotics,,"NVIDIA,Technion- Israel Institute of Technology",Industry - Academia Collaboration (Industry Leaning),"Chen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor, Gal Chechik, Xue Bin Peng",8/6/23,CALM: Conditional Adversarial Latent Models for Directable Virtual Characters,https://research.nvidia.com/labs/par/calm/,0.00E+00,,,,,,The total pre-training of the networks involved 5 billion environment steps. The low-level policy operates at 30Hz while the high-level policy operates at 6Hz.,,"160 diverse motion clips totaling 30 minutes, from a motion capture dataset. These include motions like walking, running, sword strikes, etc.",,,,,,,NVIDIA A100,Reinforcement learning,,,,Industry,Speculative,"In this work, we present Conditional Adversarial Latent Models (CALM),
an approach for generating diverse and directable behaviors for user-controlled
interactive virtual characters. Using imitation learning, CALM learns a representation of movement that captures the complexity and diversity of human
motion, and enables direct control over character movements. The approach
jointly learns a control policy and a motion encoder that reconstructs key
characteristics of a given motion without merely replicating it. The results
show that CALM learns a semantic motion representation, enabling control
over the generated motions and style-conditioning for higher-level task training. Once trained, the character can be controlled using intuitive interfaces,
akin to those found in video games.",8/10/23 14:05
Llama 2,Language,Language modelling,Meta AI,Industry,"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez,Robert Stojnic, Sergey Edunov, Thomas Scialom
",7/18/23,Llama 2: Open Foundation and Fine-Tuned Chat Models,https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/,5.50E+01,"Historical significance,Significant use",Model has been open-sourced and frequently downloaded. The paper claims that Llama 2 is the current best open-source chat model as of its release date.,7.00E+10,"Llama has been released in 7B, 13B, and 70B variants.",9.00E+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.50 utilization = 9.656e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",Llama 2 dataset,"2 trillion tokens of publicly available text, with no text from Meta's products.
""Our training corpus includes a new mix of data from publicly available sources, which does not include data from Meta’s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this
provides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.""",1.50E+12,2 trillion tokens ~= 1.5 trillion words,1.40E+11,Inference compute usage for the 70B model is 140 billion operations per token of input.,4320,"Model was trained from January 2023 to July 2023, which is six months.",NVIDIA A100 SXM4 80 GB,Supervised,1620000,"A100 cost in 2023: $1.10/hour
Training time: 1720320 A100 GPU-hours
Inflation adjustment: $1.000 2020 = $1.145 2023",,Industry,Confident,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",8/24/23 22:13
Claude 2,Language,Language modelling,Anthropic,Industry,,7/11/23,,https://www.anthropic.com/index/claude-2,0.00E+00,Historical significance,,,,5.03E+24,https://colab.research.google.com/drive/1MdPuhS4Emaf23VXYZ-ooExDW-5GXZkw0#scrollTo=Ds0Q5X8aMnOY,,,,,,,,,,,,,,,Unverified,,9/21/23 3:28
InternLM,Language,Language modelling,"Shanghai AI Lab,SenseTime",Academia,,7/6/23,,https://internlm.org/,0.00E+00,SOTA Improvement,"In addition to using academic datasets to evaluate InternLM, we also use human examinations to assess its capabilities. InternLM can achieve good scores on examination benchmarks such as MMLU, AGIEval, C-Eval, and GAOKAO-bench that cover different languages and subjects, scoring higher than ChatGPT on multiple benchmarks",1.00E+11,Pre-training a bilingual 100B Foundation model on data with over a trillion tokens,,,,,7.50E+11,"""Pre-training a bilingual 100B Foundation model on data with over a trillion tokens"" equals approximately 750B words for English, but the tokenizer's conversion ratio may be different for Chinese.",,,,Training performance for the open-source InternLM-7B: https://github.com/InternLM/InternLM/blob/main/doc/en/train_performance.md,NVIDIA A100 SXM4 80 GB,,,,,,Speculative,"Pre-training a bilingual 100B Foundation model on data with over a trillion tokens, the model exhibits excellent performance in scenarios such as Chinese, English, and coding due to the appropriate data ratio. Based on the foundation model, the application of high-quality human annotated dialogue data combined with RLHF technology enables the InternLM large language model to respond to complex commands during human interaction, while also demonstrating responses in line with human morality and values.",9/19/23 16:39
Stable Diffusion XL,Drawing,Image generation,Stability AI,Industry,"Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, Robin Rombach",7/4/23,SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis,https://arxiv.org/abs/2307.01952,0.00E+00,,,3.40E+09,"""...result in a model size of 2.6B parameters in the UNet, see Tab. 1. The text encoders have a total size of 817M parameters.""",,,,,,,,,,,,,,,,Industry,Speculative,"We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at this https URL",8/28/23 15:48
ERNIE 3.5,Language,Language modelling,Baidu,Industry,,6/27/23,Introducing ERNIE 3.5: Baidu’s Knowledge-Enhanced Foundation Model Takes a Giant Leap Forward,http://research.baidu.com/Blog/index-view?id=185,0.00E+00,SOTA Improvement,SOTA scores on AGIEval and MMLU. See article in China Science Daily: https://mp.weixin.qq.com/s/QVdkmofRSTgjQ7UOFX7s1g,,,,,,,,,,,,,,,,,,,Unverified,,9/19/23 16:10
Inflection-1,Language,Language modelling,Inflection AI,Industry,,6/23/23,Inflection-1 technical memo,https://inflection.ai/assets/Inflection-1.pdf,0.00E+00,SOTA Improvement,Inflection-1 outperforms models trained with at most the same amount of compute as PaLM-540B on MMLU and the other benchmarks in Table 1.,,,,"They define two ""compute classes"", one for models with more compute than PaLM 540B, i.e. GPT-4 and PaLM 2, and one for models with as much compute or less, i.e. GPT-3.5, Chinchilla, LLaMA, and Inflection-1.",,"""Inflection-1 was trained using thousands of NVIDIA H100 GPUs on a very large dataset.""",,,,,,,NVIDIA H100 SXM5,,,,,Industry,Speculative,"Large language models (LLMs) based on the Transformer architecture have been shown to possess a range of advanced capabilities in language generation and understanding. These capabilities have paved the way for deployment of LLMs in products like OpenAI’s Chat-GPT and Google’s Bard. At Inflection AI, our mission is to create personal AIs for everyone, and in May 2023 we released Pi (pi.ai) – an LLM-based personal AI which is designed to be empathetic, useful, and safe. In this work we introduce the foundation model powering Pi, dubbed Inflection-1, and evaluate its performance characteristics across a variety of benchmarks.",9/19/23 16:10
RoboCat,Robotics,,"Google DeepMind,Google",Industry,"Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Coline Devin, Alex X. Lee, Maria Bauza, Todor Davchev, Yuxiang Zhou, Agrim Gupta, Akhil Raju, Antoine Laurens, Claudio Fantacci, Valentin Dalibard, Martina Zambelli, Murilo Martins, Rugile Pevceviciute, Michiel Blokzijl, Misha Denil, Nathan Batchelor, Thomas Lampe, Emilio Parisotto, Konrad Żołna, Scott Reed, Sergio Gómez Colmenarejo, Jon Scholz, Abbas Abdolmaleki, Oliver Groth, Jean-Baptiste Regli, Oleg Sushkov, Tom Rothörl, José Enrique Chen, Yusuf Aytar, Dave Barker, Joy Ortiz, Martin Riedmiller, Jost Tobias Springenberg, Raia Hadsell, Francesco Nori, Nicolas Heess",6/20/23,RoboCat: A Self-Improving Foundation Agent for Robotic Manipulation,https://arxiv.org/abs/2306.11706,4.00E+00,SOTA Improvement,,1.18E+09,"""Most of the experimental results are based on models with a 1.18B-parameter decoder-only transformer (Vaswani et al., 2017) with 24 layers, an embedding size of 2048, and a post-attention feedforward hidden size of 8196."" page 8",,,,"""We use a diverse and large number of datasets for training RoboCat. These include data from agent experience, human demonstrations and self-generated data, on both simulated and real-world robot environments. See Section 3.4 for details on our datasets.""",,,,,,,,,,,,Industry,Speculative,"The ability to leverage heterogeneous robotic experience from different robots and tasks to quickly master novel skills and embodiments has the potential to transform robot learning. Inspired by recent advances in foundation models for vision and language, we propose a foundation agent for robotic manipulation. This agent, named RoboCat, is a visual goal-conditioned decision transformer capable of consuming multi-embodiment action-labelled visual experience. This data spans a large repertoire of motor control skills from simulated and real robotic arms with varying sets of observations and actions. With RoboCat, we demonstrate the ability to generalise to new tasks and robots, both zero-shot as well as through adaptation using only 100--1000 examples for the target task. We also show how a trained model itself can be used to generate data for subsequent training iterations, thus providing a basic building block for an autonomous improvement loop. We investigate the agent's capabilities, with large-scale evaluations both in simulation and on three different real robot embodiments. We find that as we grow and diversify its training data, RoboCat not only shows signs of cross-task transfer, but also becomes more efficient at adapting to new tasks.",9/19/23 16:41
MusicGen,Audio,Audio generation,Meta AI,Industry,"Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Défossez",6/8/23,Simple and Controllable Music Generation,https://arxiv.org/abs/2306.05284,3.00E+00,,,,,,,ShutterStock and Pond5 music data collections,"We use 20K hours of licensed music to train MUSICGEN. Specifically, we rely on an internal dataset of 10K high-quality music tracks, and on the ShutterStock and Pond5 music data collections with respectively 25K and 365K instrument-only music tracks. All datasets consist of full-length music sampled at 32 kHz with metadata composed of a textual description and additional information such as the genre, BPM, and tags.",,,,,,,,,,,,Industry,Unverified,"We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at this https URL.",8/9/23 20:27
PaLM 2,Language,Language modelling,Google,Industry,"Andrew M. Dai, David R. So, Dmitry Lepikhin, Jonathan H. Clark, Maxim Krikun, Melvin Johnson, Nan Du, Rohan Anil, Siamak Shakeri, Xavier Garcia, Yanping Huang, Yi Tay, Yong Cheng, Yonghui Wu, Yuanzhong Xu, Yujing Zhang, Zachary Nado, Bryan Richter, Alex Polozov, Andrew Nystrom, Fangxiaoyu Feng, Hanzhao Lin, Jacob Austin, Jacob Devlin, Kefan Xiao, Orhan Firat, Parker Riley, Steven Zheng, Yuhuai Wu, Zhongtao Liu, Jiahui Yu, Guy Gur-Ari, Weikang Zhou, Sneha Kudugunta, Sunipa Dev, Frederick Liu, Gustavo Hernandez Abrego, Kelvin Xu, Abe Ittycheriah, Daniel Sohn, John Nham, Le Hou, Siyuan Qiao, Pidong Wang, Zirui Wang, Laurent El Shafey, Hyeontaek Lim, Marcello Maggioni, Michael Isard, Paul Barham, Qiao Zhang, Tao Wang, Yash Katariya, Aurko Roy, Benjamin Lee, Brennan Saeta, Ce Zheng, Hadi Hashemi, Junwhan Ahn, Rajkumar Samuel, Steven Hand, Zhifeng Chen, Kiran Vodrahalli, Aakanksha Chowdhery, Ethan Dyer, Emanuel Taropa, Vlad Feinberg, James Bradbury, Reiner Pope, Wei Li, YaGuang Li, Eric Chu, Jeffrey Hui, Joshua Howland, Vlad Fienber, Aroma Mahendru, Michele Catasta, Vedant Misra, Kevin Robinson, Maysam Moussalem, Sebastian Ruder, Erica Moreira, Eric Ni, Paige Bailey, Lucas Gonzalez, Alexandre Passos, Slav Petrov, Gaurav Mishra, Mark Omernick, Ambrose Slone, Andrea Hu, Colin Cherry, Denny Zhou, Jan Botha, John Wieting, Joshua Maynez, Kathleen Kenealy, Kevin Brooks, Linting Xue, Markus Freitag, Martin Polacek, Pengcheng Yin, Sebastian Gehrmann, Xuezhi Wang, Kathy Meier-Hellstern, Christopher A. Choquette-Choo, Daniel Smilkov, Emily Reif, Alicia Parrish, Alex Castro Ros, Clément Crepy, Dasha Valter, Jeremy Hurwitz, Katherine Lee, Mark Díaz, Marie Pellat, Matthew Jagielski, Renee Shelby, Shachi Dave",5/10/23,PaLM 2 Technical Report,https://ai.google/static/documents/palm2techreport.pdf,1.11E+02,SOTA Improvement,,3.40E+11,"Model Architecture: ""PaLM-2 is a new state-of-the-art language model. We have small, medium, and large variants that use stacked layers based on the Transformer architecture, with varying parameters depending on model size. Further details of model size and architecture are withheld from external publication.""
However, the parameter count was leaked to CNBC: https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html",7.34E+24,"Compute Requirements ""Not reported.""
However, it is suggested that C=6ND. Based on 340B parameters and 3.6*10^12 tokens, training compute would be around 7.3*10^24 FLOP.",,"""The PaLM 2 pre-training corpus is composed of a diverse set of sources: web documents, books, code, mathematics, and conversational data. The pre-training corpus is significantly larger than the corpus used to train PaLM (Chowdhery et al., 2022). PaLM 2 is trained on a dataset that includes a higher percentage of non-English data than previous large language models, which is beneficial for multilingual tasks"" (page 9)",2.70E+12,"""The pre-training corpus is significantly larger than the corpus used to train PaLM"" so greater than 6e+11. According to the leaked documents viewed by CNBC, the corpus was 4 trillion tokens or around 2.7*10^12 words.",,,,,,,,PaLM 2 was trained on TPU v4 according to the model card (pages 91-92),,Industry,,"We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM (Chowdhery et al., 2022). PaLM 2 is a Transformer-based model trained using a mixture of objectives similar to UL2 (Tay et al., 2023). Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities.",9/19/23 16:10
Perfusion,Drawing,Text-to-image,"NVIDIA,Tel Aviv University,Bar-Ilan University",Industry - Academia Collaboration (Industry leaning),"Yoad Tewel, Rinon Gal, Gal Chechik, Yuval Atzmon",5/2/23,Key-Locked Rank One Editing for Text-to-Image Personalization,https://arxiv.org/abs/2305.01644,9.00E+00,,,,,,,,,,,,,,,,,,,,Industry,Likely,"Text-to-image models (T2I) offer a new level of flexibility by allowing users to guide the creative process through natural language. However, personalizing these models to align with user-provided visual concepts remains a challenging problem. The task of T2I personalization poses multiple hard challenges, such as maintaining high visual fidelity while allowing creative control, combining multiple personalized concepts in a single image, and keeping a small model size. We present Perfusion, a T2I personalization method that addresses these challenges using dynamic rank-1 updates to the underlying T2I model. Perfusion avoids overfitting by introducing a new mechanism that ""locks"" new concepts' cross-attention Keys to their superordinate category. Additionally, we develop a gated rank-1 approach that enables us to control the influence of a learned concept during inference time and to combine multiple concepts. This allows runtime-efficient balancing of visual-fidelity and textual-alignment with a single 100KB trained model, which is five orders of magnitude smaller than the current state of the art. Moreover, it can span different operating points across the Pareto front without additional training. Finally, we show that Perfusion outperforms strong baselines in both qualitative and quantitative terms. Importantly, key-locking leads to novel results compared to traditional approaches, allowing to portray personalized object interactions in unprecedented ways, even in one-shot settings.",8/28/23 13:50
Vicuna-13B,Language,,Large Model Systems Organization,Academia,,3/30/23,Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality,https://lmsys.org/blog/2023-03-30-vicuna/,0.00E+00,Historical significance,,1.30E+10,,,Might be possible to estimate training compute from the training cost. Fine-tuning cost $300.,,"70K conversations from ShareGPT.com, a website where users can share their ChatGPT conversations.",,,,,,,,,259,"$300 in 2020, adjusted for inflation using BLS.gov inflation calculator",,Academia,Speculative,,9/20/23 21:22
Falcon-40B,Language,Language modelling,Technology Innovation Institute,Government,,3/15/23,Abu Dhabi-based Technology Innovation Institute Introduces Falcon LLM: Foundational Large Language Model (LLM) outperforms GPT-3 with 40 Billion Parameters,https://www.tii.ae/news/abu-dhabi-based-technology-innovation-institute-introduces-falcon-llm-foundational-large,0.00E+00,Historical significance,,4.00E+10,Model comes in 7B and 40B variants.,2.40E+23,C = 6ND = 6 * 40B * 1000B = 2.4e+23 FLOP (assuming one epoch),RefinedWeb,"Falcon-40B was trained on 1,000B tokens of RefinedWeb, a high-quality filtered and deduplicated web dataset which we enhanced with curated corpora. Significant components from our curated copora were inspired by The Pile (Gao et al., 2020).",7.50E+11,1000B tokens ~= 750B words,80000000000,80B FLOP per token,1440,"""Falcon-40B was trained on AWS SageMaker, on 384 A100 40GB GPUs in P4d instances.""
""Training started in December 2022 and took two months.""",NVIDIA A100,,,,,Academia,Confident,,9/20/23 21:22
GPT-4,Multimodal,Language modelling,OpenAI,Industry,OpenAI,3/15/23,GPT-4 Technical Report,https://arxiv.org/abs/2303.08774,0.00E+00,SOTA Improvement,"See the paper, p.1: ""On a suite of traditional NLP benchmarks, GPT-4 outperforms both previous large language models and most state-of-the-art systems (which often have benchmark-specific training or hand-engineering).""",,,2.10E+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",,,,,,,,,,Self-supervised learning,,,Yes,,,,9/19/23 16:11
Claude,Language,Language modelling,Anthropic,Industry,,3/14/23,Introducing Claude,https://www.anthropic.com/index/introducing-claude,0.00E+00,"Historical significance,SOTA improvement",,,,,,,,,,,,,,,Reinforcement learning,,,,,Unverified,"Claude is a next-generation AI assistant based on Anthropic’s research into training helpful, honest, and harmless AI systems. Accessible through chat interface and API in our developer console, Claude is capable of a wide variety of conversational and text processing tasks while maintaining a high degree of reliability and predictability.",9/20/23 21:22
LLaMA-65B,Language,Language modelling,Meta AI,Industry,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",2/24/23,LLaMA: Open and Efficient Foundation Language Models,https://arxiv.org/abs/2302.13971,9.49E+02,Historical significance,Widely-used foundation model that has been adapted for others such as Alpaca.,6.52E+10,"Model card, table 1: https://github.com/facebookresearch/llama/blob/53011c3d7946dadb8274a4c5c7586ab54edf792d/MODEL_CARD.md",5.50E+23,1.4e12 tokens * 6.52e10 parameters * 6 FLOP/token/parameter = 5.5e23 FLOP,"CCNet, GitHub, Wikipedia, books, arXiv, Stack Exchange","The model was trained using the following source of data: CCNet [67%], C4 [15%], GitHub [4.5%], Wikipedia [4.5%], Books [4.5%], ArXiv [2.5%], Stack Exchange[2%]. The Wikipedia and Books domains include data in the following languages: bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk. See the paper for more details about the training set and corresponding preprocessing.",1.05E+12,1.4 trillion tokens * 0.75 words/token = 1.05 trillion words,1.30E+11,per token,5.00E+02,"""When training a 65B-parameter model, our code processes around 380 tokens/sec/GPU on 2048 A100 GPU with 80GB of RAM. This means that training over our dataset containing 1.4T tokens takes approximately 21 days.""",NVIDIA A100,Supervised,1179384.75,"1023384 processor-hours on A100 GPUs. May 2023 cost rate is $1.36/GPU-hour on Azure ML cloud. https://azure.microsoft.com/en-us/pricing/details/machine-learning/ 
According to https://www.bls.gov/data/inflation_calculator.htm, $1.18 in May 2023 = $1.00 in January 2020.
$1391674 / 1.18 = $1179385 in 2020 USD.",,Industry,Likely,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",9/14/23 19:13
Gen-1,Video,Video generation,Runway,Industry,"Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, Anastasis Germanidis",2/6/23,Structure and Content-Guided Video Synthesis with Diffusion Models,https://arxiv.org/abs/2302.03011,2.30E+01,SOTA Improvement,,,,,,,,,,,,,,,,,,,,Unverified,,9/20/23 19:57
ALM 1.0,Language,Language modelling,BAAI,Academia,,11/28/22,ALM 1.0,https://github.com/FlagAI-Open/FlagAI/blob/master/examples/ALM/README.md,0.00E+00,SOTA Improvement,SOTA results on Arabic-language benchmark ALUE.,,,,,,,,,,,,,,,,,,,Speculative,,9/19/23 16:11
CICERO,Games,Diplomacy,Meta AI,Industry,,11/22/22,Human-level play in the game of Diplomacy by combining language models with strategic reasoning,https://www.science.org/doi/10.1126/science.ade9097,5.70E+01,,,,"""We took R2C2 (22) as our base model – a 2.7B parameter Transformer-based (23) encoder-decoder model pre-trained on text from the Internet using a BART de-noising objective (24).""",,,WebDiplomacy,,,"""We obtained a dataset of 125,261 games of Diplomacy played online at webDiplomacy.net. Of these, 40,408 games contained dialogue, with a total of 12,901,662 messages exchanged between players. Player accounts were de-identified and automated redaction of personally identifiable information (PII) was performed by webDiplomacy. We refer to this dataset hereafter as WebDiplomacy .""",,,,,,,,,,,,,9/19/23 16:39
AR-LDM,Multimodal,Text-to-image,"Alibaba,University of Waterloo,Vector Institute",Industry - Academia Collaboration (Industry leaning),"Xichen Pan, Pengda Qin, Yuhong Li, Hui Xue, Wenhu Chen",11/20/22,Synthesizing Coherent Story with Auto-Regressive Latent Diffusion Models,https://arxiv.org/abs/2211.10950,1.10E+01,,The first latent diffusion model for coherent visual story synthesizing.,1.50E+09,Table 1,5.10E+20,8 NVIDIA A100 GPUs for 8 days,,,,"PororoSV, FlintstonesSV and VIST. All storytelling datasets, sizes would be possible to look up.",,,194,8 NVIDIA A100 GPUs for 8 days,NVIDIA A100,,,,,,Confident,"Conditioned diffusion models have demonstrated state-of-the-art text-to-image synthesis capacity. Recently, most works focus on synthesizing independent images; While for real-world applications, it is common and necessary to generate a series of coherent images for story-stelling. In this work, we mainly focus on story visualization and continuation tasks and propose AR-LDM, a latent diffusion model auto-regressively conditioned on history captions and generated images. Moreover, AR-LDM can generalize to new characters through adaptation. To our best knowledge, this is the first work successfully leveraging diffusion models for coherent visual story synthesizing. Quantitative results show that AR-LDM achieves SoTA FID scores on PororoSV, FlintstonesSV, and the newly introduced challenging dataset VIST containing natural images. Large-scale human evaluations show that AR-LDM has superior performance in terms of quality, relevance, and consistency.",9/19/23 16:39
Galactica,Language,Language modelling,Meta AI,Industry,"Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic",11/15/22,Galactica: A Large Language Model for Science,https://galactica.org/static/paper.pdf,1.72E+02,,,1.20E+11,"""The largest 120B model we train runs on a single NVIDIA A100 node""",,,Galactica Corpus,,,"""Total dataset size = 106 billion tokens""",,,,,,Self-supervised learning,,,Yes,,,,9/19/23 16:38
AltCLIP,Multimodal,,BAAI,Academia,"Zhongzhi Chen, Guang Liu, Bo-Wen Zhang, Fulong Ye, Qinghong Yang, Ledell Wu",11/12/22,AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities,https://arxiv.org/abs/2211.06679,1.80E+01,,,,,,,,,,,,,,,,,,,,,Likely,"In this work, we present a conceptually simple and effective method to train a strong bilingual/multilingual multimodal representation model. Starting from the pre-trained multimodal representation model CLIP released by OpenAI, we altered its text encoder with a pre-trained multilingual text encoder XLM-R, and aligned both languages and image representations by a two-stage training schema consisting of teacher learning and contrastive learning. We validate our method through evaluations of a wide range of tasks. We set new state-of-the-art performances on a bunch of tasks including ImageNet-CN, Flicker30k-CN, COCO-CN and XTD. Further, we obtain very close performances with CLIP on almost all tasks, suggesting that one can simply alter the text encoder in CLIP for extended capabilities such as multilingual understanding. Our models and code are available at this https URL.",9/19/23 16:11
BLOOM,Language,Language model,"Hugging Face,BigScience",Research Collective,"Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",11/8/22,"BigScience Large Open-science Open-access Multilingual Language Model",https://huggingface.co/bigscience/bloom,0.00E+00,Historical significance,"Was the largest open-source model at the time. 1000+ researchers, many from important orgs such as Microsoft and NVIDIA.",1.76E+11,,1.80E+23,"https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32

384 A100 GPUs * 116 days","""TB scale multilingual dataset""","In total, 1.6 terabytes of pre-processed text was converted into 350 billion unique tokens as BLOOM's training datasets.
 arXiv:2210.15424",2.63E+11,,,,,,,Self-supervised learning,,,Yes,,,,9/19/23 16:36
Taiyi-Stable Diffusion,Drawing,Text-to-image,IDEA CCNL,,,10/31/22,,https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-1B-Chinese-v0.1,0.00E+00,Historical significance,"The first open-source, Chinese version of Stable Diffusion.",1.00E+09,,5.10E+22,"Fine-tuning: 32 NVIDIA A100 GPUs for 100 hours
32 * 312e12 * 30% * 100 * 60 * 60 = 1.078272e+21 FLOP

Base model: Stable Diffusion, 5e+22 FLOP",,,,,,,100,32 NVIDIA A100 GPUs for 100 hours,NVIDIA A100,,,,,,Likely,,9/19/23 16:39
Imagen Video,Vision,Video generation,Google Brain,Industry,"Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, Tim Salimans",10/5/22,Imagen Video: High Definition Video Generation with Diffusion Models,https://arxiv.org/abs/2210.02303,1.95E+02,,,1.16E+10,"Figure 6 summarizes the entire cascading pipeline of Imagen Video. In total, we have 1 frozen text
encoder, 1 base video diffusion model, 3 SSR (spatial super-resolution), and 3 TSR (temporal superresolution) models – for a total of 7 video diffusion models, with a total of 11.6B diffusion model
parameters",,,,,,"We train our models on a combination of an internal dataset consisting of 14 million video-text pairs
and 60 million image-text pairs, and the publicly available LAION-400M image-text dataset.",,,,,,,,,,,,,9/19/23 16:06
Phenaki,Vision,Video generation,"University College London,University of Michigan,Google Brain",Industry - Academia Collaboration (Industry leaning),"Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, Dumitru Erhan",10/5/22,Phenaki: Variable Length Video Generation From Open Domain Textual Description,https://arxiv.org/abs/2210.02399,8.80E+01,,,1.80E+09,"Unless specified otherwise, we train a 1.8B parameter Phenaki model on a corpus of ∼15M textvideo pairs at 8 FPS mixed with ∼50M text-images plus ∼400M pairs of LAION-400M [41] (more
details in Appendix B.3). The model used in the visualisations in this paper was trained for 1 million
steps at a batch size of 512, which took less than 5 days. In this setup 80% of the training data came
from the video dataset and each image dataset contributed 10%.",,,,"Unless specified otherwise, we train a 1.8B parameter Phenaki model on a corpus of ∼15M textvideo pairs at 8 FPS mixed with ∼50M text-images plus ∼400M pairs of LAION-400M [41] (more
details in Appendix B.3). The model used in the visualisations in this paper was trained for 1 million
steps at a batch size of 512, which took less than 5 days. In this setup 80% of the training data came
from the video dataset and each image dataset contributed 10%.",,"Unless specified otherwise, we train a 1.8B parameter Phenaki model on a corpus of ∼15M textvideo pairs at 8 FPS mixed with ∼50M text-images plus ∼400M pairs of LAION-400M [41] (more
details in Appendix B.3). The model used in the visualisations in this paper was trained for 1 million
steps at a batch size of 512, which took less than 5 days. In this setup 80% of the training data came
from the video dataset and each image dataset contributed 10%.",,,,,,Self-supervised learning,,,Yes,,,,9/19/23 16:12
Make-A-Video,Text-to-Video,,Meta AI,Industry,"Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, Yaniv Taigman",9/29/22,Make-A-Video: Text-to-Video Generation without Text-Video Data,https://arxiv.org/abs/2209.14792,2.40E+02,SOTA Improvement,,,,,,,,,,,,,,,Self-supervised learning,,,Yes,,,,9/19/23 16:36
Whisper,Speech,Audio Speech Recognition,OpenAI,Industry,"Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever",9/21/22,Robust Speech Recognition via Large-Scale Weak Supervision,https://cdn.openai.com/papers/whisper.pdf,5.04E+02,SOTA Improvement,,1.55E+09,Table 1,4.65E+22,"https://plotdigitizer.com/app

See figure 9",,,9.30E+09,"When scaled
to 680,000 hours of multilingual and multitask
supervision, the resulting models generalize well
to standard benchmarks and are often competitive
with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. 


13,680 words/h * 680,000h = 9302400000 words",,,,,,Self-supervised learning,,,Yes,,,"We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",9/19/23 16:39
GLM-130B,Language,,Tsinghua KEG,Academia,,8/4/22,GLM-130B: An open bilingual pre-trained model,https://keg.cs.tsinghua.edu.cn/glm-130b/posts/glm-130b/,6.00E+01,,,1.30E+11,Dense model,4.60E+22,"96 Nvidia A100 GPUs for 2 months

",,,,,,,,,,,,,,Industry,,,9/19/23 16:41
AlexaTM 20B,Language,Language modelling,Amazon,Industry,"Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, Chandana Satya Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv Verma, Gokhan Tur, Prem Natarajan",8/2/22,AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model,https://arxiv.org/abs/2208.01448,3.30E+01,SOTA Improvement,The Abstract of the paper reports SOTA improvement on multiple benchmarks.,1.98E+10,See Table 1 on p.3 of the paper,2.04E+23,"Training throughput is reported as 154 TFLOP/s - see p.5 of the paper.
""We relied on an internal and optimized version of DeepSpeed that we have since open-sourced (Chiu & Zheng, 2022) to obtain training throughput of up to 154 TFLOPS/GPU on 16 AWS p4d.24xlarge compute instances.""

Accelerator compute days are reported as 15,360 days - see Table 17 on p.18 of the paper.",mC4; Wikipedia,See Table 2 on p.3 of the paper.,,,,,2880,"See p.5 of the paper: ""We trained AlexaTM 20B for 120 days on 128 A100 GPUs...""",,,,,,Industry,,"In this work, we demonstrate that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more efficient few-shot learners than decoder-only models on various tasks. In particular, we train a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and show that it achieves state-of-the-art (SOTA) performance on 1-shot summarization tasks, outperforming a much larger 540B PaLM decoder model. AlexaTM 20B also achieves SOTA in 1-shot machine translation, especially for low-resource languages, across almost all language pairs supported by the model (Arabic, English, French, German, Hindi, Italian, Japanese, Marathi, Portuguese, Spanish, Tamil, and Telugu) on Flores-101 dataset. We also show in zero-shot setting, AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2 datasets and provides SOTA performance on multilingual tasks such as XNLI, XCOPA, Paws-X, and XWinograd. Overall, our results present a compelling case for seq2seq models as a powerful alternative to decoder-only models for Large-scale Language Model (LLM) training. ",9/19/23 16:36
NLLB,Language,Translation,Meta AI,Industry,"Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco (Paco) Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Jeff Wang",7/6/22,No Language Left Behind: Scaling Human-Centered Machine Translation,https://research.facebook.com/publications/no-language-left-behind/,1.90E+01,SOTA Improvement,Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art,5.45E+10,"Section 8.2.4: ""The model has a total of 54.5B parameters
and FLOPs similar to that of a 3.3B dense model""",1.75E+22,"Section 8.8:
"" To train NLLB-200, a cumulative
of 51968 GPU hours of computation was performed on hardware of type A100-SXM-80GB""
See also Table 48

Section 8.2.4 states they use FP16

NVIDIA Datasheet states 312TFLOPS for FP16
https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-nvidia-us-2188504-web.pdf

Assuming 0.3 utilization:

312e12*3600*51968*0.3

Also:
""Our final model is a Transformer
encoder-decoder model in which we replace the Feed Forward Network (FFN) layer in
every 4th Transformer block with a Sparsely Gated Mixture of Experts layer containing 128
experts. We use model dimension 2048, FFN dimension 8192, 16 attention heads, 24 encoder
layers and 24 decoder layers. We use Pre-LayerNorm (Xiong et al., 2020) as described in
Section 6.1.1. We share the embedding weights of the encoder input embedding, decoder
input embedding and decoder output embedding layers. We use an overall dropout of 0.3,
attention dropout 0.1 and EOM with peom=0.2. The model has a total of 54.5B parameters
and FLOPs similar to that of a 3.3B dense model.""",,,3.60E+11,"[WORDS]

Section 8.2.2: ""As we prepare to train on the final 202 language dataset comprising of over 18B sentence
pairs and 2440 language directions""

18B sentences * 20 words/sentence",,,,,,Self-supervised learning,39175.64,,Yes,Industry,,"Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today. However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system. Finally, we open source all contributions described in this work, accessible at https://github.com/facebookresearch/fairseq/tree/nllb.",9/20/23 20:48
Minerva (540B),Language,Quantitative Reasoning Problems,Google,Industry,"Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra",6/29/22,Solving Quantitative Reasoning Problems with Language Models,https://arxiv.org/abs/2206.14858,1.71E+02,SOTA Improvement,,5.40E+11,"""To further our understanding of the
impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer
language model, which we call Pathways Language Model (PaLM).""

Our approach is to start with the PaLM pretrained decoder-only transformer language models Chowdhery
et al. (2022), and further train (finetune) them on our mathematical dataset using an autoregressive objective.
Table 2 contains the main model and training hyperparameters.

See Table 2",2.74E+24,"See calculation from linked sheet

1024 TPUv4 for 29*24=696 hours is 8% of the training time of PaLM (6144 chips for 1200 hours). So total
compute is PaLM's compute * 1.08
","PaLM, finetuned on Arxiv",,6.14E+11,"""Our models were trained on a dataset of 38.5B tokens"" + PaLM",,,,,,Self-supervised learning,3267257.75,,Yes,Industry,,"Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.",9/19/23 16:41
GPT-SW3,Language,Language modelling,"AI Sweden,RISE",Academia,"Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman, Severine Verlinden, Joey Ohman, Fredrik Carlsson, Magnus Sahlgren",6/25/22,Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish,http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.376.pdf,6.00E+00,,,3.50E+09,,1.30E+21,"From section 4: ""Training was performed on GPU resources from the Berzelius Superpod, which is currently the fastest super
computer in Sweden, equipped with 60 Nvidia DGX
A100 servers, each of which consists of 8 Nvidia A100
GPUs with 320 GB Total GPU memory. Our training
process took 2.5 days utilizing 16 of the DGX A100
servers (in total 128 GPUs).""

2.5*24*60**2 * 128 * 1.56E+14 * 0.3 = 1.3e21",,Novel Swedish 100GB corpus from news articles.,1.67E+10,"100GB Swedish corpus, assume Swedish has similar 167M words per GB as German.
100*167e6 = 1.67e10",,,60,"""Our training process took 2.5 days utilizing 16 of the DGX A100 servers (in total 128 GPUs).""",NVIDIA A100,Self-supervised learning,,,Yes,,,"Large-scale generative language models such as the GPT series (Radford and Narasimhan, 2018; Radford et al., 2019; Brown et al., 2020) have enjoyed considerable attention in recent years. This has been partly due to their unprecedented ability to generate coherent text, but also for their capacity for zero-shot performance - without any training examples, on a wide range of different tasks. A prerequisite for building such models is access to both large amounts of high-quality text data and powerful computational resources. This has proven to be a limiting factor for the development of large-scale models for languages other than English. With the goal of promoting the development of largescale generative models for other languages, we here present our work on developing and evaluating GPTSW3, a 3.5 billion parameter autoregressive language model, trained on a newly collected 100 GB Swedish corpus. To the best of our knowledge, this is the largest generative model for Swedish to date, and probably one of the bigger non-English models at the moment. In this paper, we collect the lessons learned by developing and evaluating this model, including challenges with data collection, training procedures, and validation activities.",9/19/23 16:39
YaLM,Language,Language modelling,Yandex,Industry,,6/23/22,,https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6,,,,1.00E+11,,2.20E+23,"""It took us 65 days to train the model on a pool of 800 A100 graphics cards and 1.7 TB of online texts, books, and countless other sources.""",,,,,,,,,,,,,,Industry,,,5/29/23 20:51
Parti,Drawing,Text-to-image,Google Research,Industry,"Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, Yonghui Wu",6/22/22,Scaling Autoregressive Models for Content-Rich Text-to-Image Generation,https://arxiv.org/abs/2206.10789v1,3.33E+02,SOTA Improvement,"Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO",2.00E+10,"Abstract: ""we achieve consistent quality improvements
by scaling the encoder-decoder Transformer model up to 20B parameters""",3.96E+23,"Calculated from architecture. Does not take into account the encoding and decoding of text and images, only the transformer stack.

Table 1 shows for the 20B model
16 encoder layers
64 decoder layers
Dmodel = 4096
Dhidden = 16384
Num heads = 64

Just below table 1:
""We use a maximum length of text tokens of 128, and the length of image tokens are fixed to 1024""

I take the length of the sequence to be 100 for the encoder stack and 1024 for the decoder stack.

Section 3, Training: ""a total
of 450,000 steps and final ratio of 0.025. We use a global batch size of 8192 during training.""","LAION-400M, FIT400M, JFT-4B ",,4.80E+09,,,,,,,Self-supervised learning,486659.77,,Yes,Industry,,"We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https://parti.research.google/ for high-resolution images.",9/19/23 16:36
MetaLM,Multimodal,,Microsoft Research,Industry,"Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, Furu Wei",6/13/22,Language Models are General-Purpose Interfaces,https://arxiv.org/abs/2206.06336v1,4.10E+01,SOTA Improvement,"Abstract: ""Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.""",,,,,,,,,,,,,,Self-supervised learning,,,Yes,Industry,,,9/19/23 16:38
LiMoE,Multimodal,Image classification,Google,Industry,"Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, Neil Houlsby",6/6/22,Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts,https://arxiv.org/abs/2206.02770,1.50E+01,,"
",5.60E+09,"Section 1: ""We scale this up to a large 5.6B parameter LIMoE-H/14""",1.80E+22,"Section 3.2: ""The model contains 5.6B parameters in total, but only applies 675M parameters per token""

From Section A.3, ""batch size 21502 with resolution 288 and text sequence length16"". ""The model was trained for 700k steps pre-cooldown. There was one cooldown of length 125k steps
from the final step, and 3 of length 40k steps starting from step 650k"". Patch size 14 for images.

Assume C = 6*N*D. 
C = 6*675e6*21.5e3*1e6*(16+(288/14)**2)/2 = 1.8e22

This is broadly consistent with ViT-H/14's compute",,,7.60E+09,"Section 3: ""Training data. By default, all models are trained on paired image-text data used in [16], consisting of 3.6B images and alt-texts scraped from the web. For large LIMoE-H/14 experiment, we also co-train with JFT-4B [17]. """,,,,,,,,,,Industry,,,7/24/23 15:06
CogVideo,Multimodal,Video generation,"Tsinghua University,BAAI",Academia,"Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Jie Tang",5/29/22,CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers,https://arxiv.org/abs/2205.15868,9.10E+01,Historical significance,The world's largest and first opensource large-scale pre-trained text-to-video model.,9.00E+09,,,,,,,,,,,,,,,,,,Likely,"Large-scale pretrained transformers have created milestones in text (GPT-3) and text-to-image (DALL-E and CogView) generation. Its application to video generation is still facing many challenges: The potential huge computation cost makes the training from scratch unaffordable; The scarcity and weak relevance of text-video datasets hinder the model understanding complex movement semantics. In this work, we present 9B-parameter transformer CogVideo, trained by inheriting a pretrained text-to-image model, CogView2. We also propose multi-frame-rate hierarchical training strategy to better align text and video clips. As (probably) the first open-source large-scale pretrained text-to-video model, CogVideo outperforms all publicly available models at a large margin in machine and human evaluations.",9/19/23 16:36
Imagen,Vision,,Google Brain,Industry,"Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li",5/23/22,Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding,https://imagen.research.google/,1.45E+03,,,,,,,,,,,,,,,,Self-supervised learning,,,Yes,Industry,,,9/19/23 16:38
Gato,Multimodal,,DeepMind,Industry,"Scott Reed, Konrad Żołna, Emilio Parisotto, Sergio Gómez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Giménez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, Nando de Freitas",5/12/22,A Generalist Agent,https://www.deepmind.com/publications/a-generalist-agent,3.86E+02,Historical significance,,1.18E+09,"""This section focuses on in-simulation evaluation.
Figure 10 compares the full 1.18B parameter Gato"" p.10",5.44E+21,256 (16x16x) TPUv3 chips x 123e12 FLOPS/chip x 4 days x 86400 seconds/day * 0.5 utilization = 5.44e21 FLOPs,,,,,,,,,,,6781.08,,,Industry,,"Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.",9/19/23 16:37
UL2,Language,,"Google Research,Google Brain",Industry,"Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler",5/10/22,Unifying Language Learning Paradigms,https://arxiv.org/abs/2205.05131v1,1.90E+01,SOTA Improvement,"by scaling our model up to 20B parameters, we achieve SOTA
performance on 50 well-established supervised NLP tasks ",2.00E+10,Taken from Directory of LLMs,,,,,,,,,,,,,,,,Industry,,,8/11/23 19:08
ASE,Robotics,,"NVIDIA,UC Berkeley",Industry - Academia Collaboration,"Xue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine, Sanja Fidler",5/5/22,ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters,https://arxiv.org/abs/2205.01906,5.60E+01,,,,,6.10E+18,"Training was done using the Isaac Gym simulator on an NVIDIA V100 GPU. The model was trained on over 10 billion samples, which equates to 10 years of simulated experience time. Training took around 10 days on a single GPU.

14.13 TFLOP/s * 10 days * 86400 s/day * 0.50 utilization = 6.1e+18 FLOP",,"The model is trained on a dataset of 187 motion clips, about 30 minutes of human motion capture data, depicting locomotion and sword wielding motions.",,,,,240,Training took around 10 days on a single GPU.,NVIDIA Tesla V100 PCIe 16 GB,Reinforcement learning,,,,Industry,Likely,"The incredible feats of athleticism demonstrated by humans are made possible in part by a vast repertoire of general-purpose motor skills, acquired through years of practice and experience. These skills not only enable humans to perform complex tasks, but also provide powerful priors for guiding their behaviors when learning new tasks. This is in stark contrast to what is common practice in physics-based character animation, where control policies are most typically trained from scratch for each task. In this work, we present a large-scale data-driven framework for learning versatile and reusable skill embeddings for physically simulated characters. Our approach combines techniques from adversarial imitation learning and unsupervised reinforcement learning to develop skill embeddings that produce life-like behaviors, while also providing an easy to control representation for use on new downstream tasks. Our models can be trained using large datasets of unstructured motion clips, without requiring any task-specific annotation or segmentation of the motion data. By leveraging a massively parallel GPU-based simulator, we are able to train skill embeddings using over a decade of simulated experiences, enabling our model to learn a rich and versatile repertoire of skills. We show that a single pre-trained model can be effectively applied to perform a diverse set of new tasks. Our system also allows users to specify tasks through simple reward functions, and the skill embedding then enables the character to automatically synthesize complex and naturalistic strategies in order to achieve the task objectives.",8/10/23 14:26
Jurassic-X,Language,,AI21labs,Industry,"Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon Shashua, Moshe Tenenholtz",5/3/22,"MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning",https://www.ai21.com/blog/jurassic-x-crossing-the-neuro-symbolic-chasm-with-the-mrkl-system,2.30E+01,,,7.00E+09,,,,,,,,,,,,,,,,,Industry,,,9/22/23 23:29
OPT-175B,Language,Language modelling,Meta AI,Industry,"Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",5/2/22,OPT: Open Pre-trained Transformer Language Models,https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/,6.97E+02,,,1.75E+11,"""In line with Meta AI’s commitment to open science, we are sharing Open Pretrained Transformer (OPT-175B), a language model with 175 billion parameters trained on 
publicly available data sets""",4.30E+23,"https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/final_update.md

""As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute""",,,1.35E+11,"""The training data contains
180B tokens corresponding to 800 GB of data""

1 token ~ 0.75 words",,,,,,Self-supervised learning,1654082.5,,Yes,Industry,,"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3,1 while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models",9/19/23 16:40
Flamingo,Multimodal,Language modelling,DeepMind,Industry,"Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan",4/29/22,Flamingo: a Visual Language Model for Few-Shot Learning,https://arxiv.org/abs/2204.14198,7.56E+02,SOTA Improvement,"we demonstrate that a single Flamingo model can achieve a new state of the
art for few-shot learning",8.00E+10,"We obtain three models, Flamingo-3B, Flamingo-9B and Flamingo-80B",2.70E+23,"All training and evaluation
was performed on TPUv4 instances. The largest model containing 80 billion parameters is trained on
QUSV chips for 15 days and sharded across 16 devices.

All trained parameters and optimizer accumulators are stored
and updated in float32; all activations and gradients are computed in bfloat16 after downcasting
of parameters from float32 to bfloat16","MultiModal MassiveWeb, LTIP, VTP, ALIGN",,,"Flamingo was trained on a mixture of web-scraped datasets:
43M pages of text with interleaved images (MultiModal MassiveWeb dataset)
312M image-text pairs (LTIP dataset)
27M video-text pairs (VTP dataset)
1.8B image-alt text pairs (ALIGN dataset)

Training dataset size is at least 2.1 billion.",,,360,1536 TPU v4 chips for 15 days,Google TPU V4,Supervised,,,,Industry,Likely,"Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.",9/19/23 16:37
Sparse all-MLP,Language,,MetaAI,Industry - Academia Collaboration (Industry leaning),"Ping Yu, Mikel Artexte, Myle Ott, Sam Shleifer, Hongyu Gong, Ves Stoyanov, Xian Li",4/14/22,Efficient Language Modeling with Sparse all-MLP,https://arxiv.org/abs/2203.06850,0.00E+00,SOTA Improvement,"Abstract:
""Our model also outperforms
the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages.""",9.40E+09,"Table 2: ""In Section 4.4, we run our large model (9.41B parameters)""",6.08E+19,"112 hours on 32 V100 GPUs
assumed 0.33 util rate
32*112*60*60*0.3*1.57E+13
",RoBERTa dataset,,7.50E+10,100B tokens (Table 2) so 75B words.,,,112,,,Self-supervised learning,320,,Yes,Industry,,"All-MLP architectures have attracted increasing interest as an alternative to attention-based models. In NLP, recent work like gMLP shows that all-MLPs can match Transformers in language modeling, but still lag behind in downstream tasks. In this work, we analyze the limitations of MLPs in expressiveness, and propose sparsely activated MLPs with mixture-of-experts (MoEs) in both feature and input (token) dimensions. Such sparse all-MLPs significantly increase model capacity and expressiveness while keeping the compute constant. We address critical challenges in incorporating conditional computation with two routing strategies. The proposed sparse all-MLP improves language modeling perplexity and obtains up to 2× improvement in training efficiency compared to both Transformer-based MoEs (GShard, Switch Transformer, Base Layers and HASH Layers) as well as dense Transformers and all-MLPs. Finally, we evaluate its zero-shot in-context learning performance on six downstream tasks, and find that it surpasses Transformer-based MoEs and dense Transformers.",8/1/23 10:07
Stable Diffusion (LDM-KL-8-G),Drawing,Text-to-image,"Runway,Ludwig Maximilian University",Industry,"Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer",4/13/22,High-Resolution Image Synthesis with Latent Diffusion Models,https://arxiv.org/abs/2112.10752,2.61E+03,"Significant use,Highly cited",,1.45E+09,See Table 1,5.00E+22,"""I get 5e22 FLOP. 150k hours on A100 [1] gives 150*10^3 hours * 3600 seconds/hour * 3.12E+14 peak performance of A100 * 0.33 utilisation = 5e22  FLOP""

[1] https://twitter.com/EMostaque/status/1563870674111832066",LAION-400M,"Depends on the specific task; see sec 4

""we train a 1.45B parameter
KL-regularized LDM conditioned on language prompts on
LAION-400M""",4.00E+08,,,,,,,Self-supervised learning,,,Yes,,,"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at this https URL .",9/19/23 16:42
DALL·E 2,Drawing,Text-to-image,OpenAI,Industry,"Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen",4/6/22,Hierarchical Text-Conditional Image Generation with CLIP Latents,https://cdn.openai.com/papers/dall-e-2.pdf,2.13E+03,"SOTA Improvement,Highly cited",,3.50E+09,"""Our decoder architecture is the 3.5 billion parameter GLIDE model""",,Need to slowly work through Table 3...,"CLIP, DALL-E",,6.50E+08,"""When training the encoder, we sample from the CLIP [39] and DALL-E [40] datasets (approximately
650M images in total) with equal probability""",,,,,,Self-supervised learning,,,Yes,Industry,,,9/19/23 16:39
PaLM (540B),Language,Language modelling,Google Research,Industry,"Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev,, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta ,Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel",4/4/22,PaLM: Scaling Language Modeling with Pathways,https://arxiv.org/abs/2204.02311,2.28E+02,SOTA Improvement,"Demonstrates continued benefits of scaling, as well as discontinuous improvements in performance",5.40E+11,"""To further our understanding of the
impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer
language model, which we call Pathways Language Model (PaLM).""",2.53E+24,"See Table 20

https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf",,,5.85E+11,"""The PaLM pretraining dataset consists of a high-quality corpus of 780 billion tokens that represent a wide range of natural language use cases.""

1 token ~ 0.75 words",,,,,,Self-supervised learning,3232806.53,,Yes,Industry,,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",8/11/23 19:08
Chinchilla,Language,Language modelling,DeepMind,Industry,"Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals and Laurent Sifre",3/29/22,Training Compute-Optimal Large Language Models,https://arxiv.org/abs/2203.15556,1.47E+02,SOTA Improvement,"Proposes new scaling law, with good empirical results",7.00E+10,"""We test this hypothesis by training a predicted compute-optimal model, \chinchilla, that uses the same compute budget as \gopher but with 70B parameters and 4× more more data. \chinchilla uniformly and significantly outperforms \Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks.""",5.76E+23,"""Both Chinchilla and Gopher have been trained for the same number of FLOPs but differ in the
size of the model and the number of training tokens.""

We see the number of flops in table 3",,,1.05E+12,"Table 1 shows Chinchilla was training on 1.4 trillion tokens

1 token ~ 0.75 words",,,,,,Self-supervised learning,753491.58,,Yes,Industry,,"We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over \nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, \chinchilla, that uses the same compute budget as \gopher but with 70B parameters and 4× more more data. \chinchilla uniformly and significantly outperforms \Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that \chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, \chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over \gopher.",9/19/23 16:40
BaGuaLu,Multimodal,"Language modelling,Image classification","Tsinghua University,DAMO Academy Alibaba,Zhejiang Lab,BAAI",Industry - Academia Collaboration (Industry leaning),"Zixuan Ma, Jiaao He, Jiezhong Qiu, Huanqi Cao, Yuanwei Wang, Zhenbo Sun, Liyan Zheng, Haojie Wang, Shizhi Tang, Tianyu Zheng, Junyang Lin, Guanyu Feng, Zeqiang Huang, Jie Gao, Aohan Zeng, Jianwei Zhang, Runxin Zhong, Tianhui Shi, Sha Liu, Weimin Zheng, Jie Tang, Hongxia Yang, Xin Liu, Jidong Zhai, Wenguang Chen",3/28/22,BaGuaLu: Targeting Brain Scale Pretrained Models with over 37 Million Cores,https://dl.acm.org/doi/abs/10.1145/3503221.3508417,1.20E+01,,,1.74E+14,"Table 3, MoDa-174T has 173.9 trillion parameters",,"The 174T parameter system was not trained, the paper simply demonstrated they were able to train it for a few iterations.

Calculations below give some rough estimates of FLOP for full training.

From Table 5, sustained performance was 230 PFLOPS. Assuming they ran a 2-month training run, C=230e12*60**2*24*8=1.6e20 FLOP.

Using C=6ND with D=17.5B tokens and N=173.9T parameters, we get C=1.83e+25 FLOP. However, it's an MoE with 96e3 experts.

If we assume that scaling was perfect, then C=1.8e25/93e3=1.9e20 FLOP.

If we assume that they got similar utilisation to the Switch transformer, i.e. 0.10% of params active at a time, then C=1.8e25 * 0.001=1.8e22 FLOP.",M6-Corpus,"""The data are collected from different sources, including encyclopedias, ecommerce platforms, and other crawled web pages. The detailed statistics of the final processed dataset are reported in Table 4, where ""#Img"" refers to the number of distinct images, ""#Tok"" to the number of distinct tokens... after
the images transformed to features, the final product was a dataset of size 16 TB.""",1.32E+10,"17.5B tokens (in English, this is approximately 13.1B words, but the conversion may be different in Chinese) and 60.5M images.",,,,,,Self-supervised learning,,,Yes,Industry,,,8/10/23 13:27
Statement Curriculum Learning,Language,Automated theorem proving,OpenAI,Industry,"Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, Ilya Sutskever ",3/2/22,Formal Mathematics Statement Curriculum Learning,https://arxiv.org/abs/2202.01344,4.40E+01,,,,,,,"Common Crawl, WebMath","300 billion tokens from Common Crawl
72 billion tokens (220 GB) of code from WebMath
25000 theorems from mathlib
327 math problems from competitions and textbooks

The model was also trained on its own self-generated proofs",2.75E+11,"Table on p12 gives WebMath dataset size in GB of code. Uncompressed code probably has a similar number of tokens per gigabyte as natural language text, on the order of 3e8 tokens per GB.",,,,,,,,,,Industry,,,9/19/23 16:40
DeepNet,Language,Language modelling,Microsoft Research,Industry,"Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, Furu Wei",3/1/22,"DeepNet: Scaling Transformers to 1,000 Layers",https://arxiv.org/abs/2203.00555,7.60E+01,,,3.20E+09,"""Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly outperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points, which indicates a promising scaling direction""

EDIT 05/05/2022: The 12B model was presented in an earlier paper. This paper presents a 3.2B model",,,,,1.20E+10,""" The final data consists of 102 languages, 1932 directions, and
12B sentence pairs.""",,,,,,,,,,Industry,,,9/19/23 16:40
MuZero VP9,Other,Video compression,DeepMind,Industry,"Amol Mandhane, Anton Zhernov, Maribeth Rauh, Chenjie Gu, Miaosen Wang, Flora Xue, Wendy Shang, Derek Pang, Rene Claus, Ching-Han Chiang, Cheng Chen, Jingning Han, Angie Chen, Daniel J. Mankowitz, Jackson Broshear, Julian Schrittwieser, Thomas Hubert, Oriol Vinyals, Timothy Mann",2/14/22,MuZero with Self-competition for Rate Control in VP9 Video Compression,https://arxiv.org/abs/2202.06626,1.90E+01,,,,,,,,,,,,,,,,,,,,,,,9/19/23 16:40
LaMDA,Language,Language modelling,Google,Industry,"Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, Quoc Le",2/10/22,LaMDA: Language Models for Dialog Applications,https://arxiv.org/abs/2201.08239,5.72E+02,Historical significance,,1.37E+11,"""LaMDA is a family of Transformer-
based neural language models specialized for dialog, which have up to 137B parameters""",3.55E+23,"""The total FLOPS is 56.5% * 123 TFLOPS/s * 1024 chips * 57.7 days
= 3.55E+23""
From https://arxiv.org/pdf/2201.08239.pdf p.18
",Infiniset,"LaMDA's underlying dataset is called 'Infiniset', and besides the dialogue also involves common crawl, wikipedia, a mixture of english and non-english web documents, and data from programming-related sites (so LaMDA models can also dabble in code).",1.56E+12,"""and are pre-trained on 1.56T words of public dialog data and web text""",,,,,,Self-supervised learning,484957.2,,Yes,Industry,,"We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.",9/19/23 16:41
GPT-NeoX-20B,Language,,EleutherAI,Research Collective,"Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach",2/9/22,GPT-NeoX-20B: An Open-Source Autoregressive Language Model,https://arxiv.org/abs/2204.06745,4.50E+01,Historical significance,,2.00E+10,,9.32E+22,Trained for 3 months on 96 A100s (according to correspondence with author). Let's say 0.4 utilization rate.,The Pile,,1.77E+11,"""In aggregate, the Pile consists of over 825GiB of raw text data""

1 GB ~ 200M words",,,,,,Self-supervised learning,202407.46,,Yes,Industry,,,9/22/23 23:29
RETRO-7B,Language,,DeepMind,Industry,"Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae‡, Erich Elsen, Laurent Sifre",2/7/22,Improving language models by retrieving from trillions of tokens,https://arxiv.org/abs/2112.04426,5.90E+01,,,7.50E+09,"""Retro provides a constant gain for models ranging from 150M to 7B parameters, and Retro can be improved at evaluation time by increasing the database size and the number of retrieved neighbours. """,,,,,3.15E+11,"""we train for 419,430,400,000 training tokens"" ~= 315B words.",,,,,,Self-supervised learning,,,Yes,Industry,,,8/15/23 14:55
AlphaCode,Language,Code generation,DeepMind,Industry,The Alpha Code team,2/2/22,Competition-Level Code Generation with AlphaCode,https://arxiv.org/abs/2203.07814,7.50E+01,SOTA improvement,,,,4.05E+23,"Figure 7 shows a maximum compute budget of approx 2500 TPU-days, from reading the graph. The chips used were TPUv4s (Sec 4.1) which are approximately 2.7x faster than TPUv3s (https://venturebeat.com/2021/05/18/google-details-new-ai-accelerator-chips/). The Google LaMDA paper said that researchers achieved ""123 TFLOPS/sec with 56.5% FLOPS utilization"", so assume the TPUv4 gives 2.7 x 123 TFLOPS/sec = 332 TFLOPS/sec with the same utilization. This gives us 0.332 PFLOPS/sec x 0.565 x 2500 TPU*days = 470 PFLOPS/sec*days. We can probably (?) assume better utilization for the v4 chips, so this is a lower bound.
(from Edouard Harris, Ai tracker)",,,,Appendix part A has answers for pretraining.,,,,,,Self-supervised learning,,,Yes,Industry,,,9/20/23 19:57
InstructGPT,Language,,OpenAI,Industry,"Long Ouyang, Pamela Mishkin, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,John Schulman Amanda Askell, Fraser Kelton Peter Welinder, Luke Miller Maddie Simens Paul Christiano,Ryan Lowe,Chong Zhang Jacob Hilton, Sandhini Agarwal Katarina Slama Alex Ray, Jan Leike",1/27/22,Training language models to follow instructions with human feedback,https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf,1.49E+02,Historical significance,,,,,,,,1.31E+06,"Table 6 - describes **number of prompts**

26584 + 6623 = 33207

This is added to GPT-3 dataset size.",,,,,,,,,Yes,Industry,,,5/29/23 20:51
Primer,Language,,Google Brain,Industry,"DavidR.So, WojciechMan ́ke, HanxiaoLiu, ZihangDai, NoamShazeer, QuocV.Le",1/24/22,Primer: Searching for Efficient Transformers for Language Modeling,https://arxiv.org/abs/2109.08668,3.90E+01,,,1.90E+09,"""For instance, in a 1.9B parameter configuration similar to GPT-3 XL, Primer uses 1/3 of the training compute to achieve the same one-shot performance as Transformer""",7.10E+21,"From the email they claim to have use 72K TPUv4 hours for training

Thus: 
72000 h * 0.1 * 275e12 FLOP/s 3600s/h = 7.1e21 FLOP",C4,,1.73E+11,"In GB - TODO convert to words

""Dataset size: 806.92 GiB""
https://www.tensorflow.org/datasets/catalog/c4

This was the largest dataset that the authors used 
""These benefits are robust and hold across model sizes (20M
to 1.9B parameters), across compute scales (10 to 105
accelerator hours), across datasets (LM1B,
C4, PG19 [22])""

802.92 GiB ~ 866.42 GB
1 GB ~ 200M words",,,,,,,9690.72,,,Industry,,"Large Transformer models have been central to recent advances in natural language processing. The training and inference costs of these models, however, have grown rapidly and become prohibitively expensive. Here we aim to reduce the costs of Transformers by searching for a more efficient variant. Compared to previous approaches, our search is performed at a lower level, over the primitives that define a Transformer TensorFlow program. We identify an architecture, named Primer, that has a smaller training cost than the original Transformer and other variants for auto-regressive language modeling. Primer's improvements can be mostly attributed to two simple modifications: squaring ReLU activations and adding a depthwise convolution layer after each Q, K, and V projection in self-attention. Experiments show Primer's gains over Transformer increase as compute scale grows and follow a power law with respect to quality at optimal model sizes. We also verify empirically that Primer can be dropped into different codebases to significantly speed up training without additional tuning. For example, at a 500M parameter size, Primer improves the original T5 architecture on C4 auto-regressive language modeling, reducing the training cost by 4X. Furthermore, the reduced training cost means Primer needs much less compute to reach a target one-shot performance. For instance, in a 1.9B parameter configuration similar to GPT-3 XL, Primer uses 1/3 of the training compute to achieve the same one-shot performance as Transformer. We open source our models and several comparisons in T5 to help with reproducibility.",8/11/23 19:07
data2vec (language),Language,,MetaAI,Industry,"Alexei Baevski,  Wei-Ning Hsu,  Qiantong Xu , Arun Babu,  Jiatao Gu,  Michael Auli",1/20/22,"Data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language/,1.80E+02,SOTA Improvement,"""Experiments on the major benchmarks of speech recognition, image classification, and natural lan guage understanding demonstrate a new state of the art or competitive performance to predominant approaches""",7.05E+08,"Section 4: ""We experiment with two model sizes: data2vec Base and
data2vec Large, containing either L = 12 or L = 24 Trans-
former blocks with H = 768 or H = 1024 hidden dimen-
sion (with 4 × H feed-forward inner-dimension)""
",,,"Books Corpus, English Wikipedia",,3.30E+09,"Section 5.3: ""we
adopt the same training setup as BERT (Devlin et al., 2019)
by pre-training on the Books Corpus (Zhu et al., 2015) and
English Wikipedia data over 1M updates and a batch size
of 256 sequences.""",,,,,,,,,Yes,Industry,,,5/29/23 20:51
data2vec (speech),Speech,,MetaAI,Industry,"Alexei Baevski,  Wei-Ning Hsu,  Qiantong Xu , Arun Babu,  Jiatao Gu,  Michael Auli",1/20/22,"Data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language/,1.80E+02,SOTA Improvement,"""Experiments on the major benchmarks of speech recognition, image classification, and natural lan guage understanding demonstrate a new state of the art or competitive performance to predominant approaches""",7.05E+08,"Section 4: ""We experiment with two model sizes: data2vec Base and
data2vec Large, containing either L = 12 or L = 24 Trans-
former blocks with H = 768 or H = 1024 hidden dimen-
sion (with 4 × H feed-forward inner-dimension)""
",,,LS-960,,1.31E+07,"Section 5.2:
""we pre-train data2vec on the 960
hours of speech audio data from Librispeech (LS-960)""

13,680 words per hour",,,,,,,,,Yes,Industry,,,5/29/23 20:51
data2vec (vision),Vision,,MetaAI,Industry,"Alexei Baevski,  Wei-Ning Hsu,  Qiantong Xu , Arun Babu,  Jiatao Gu,  Michael Auli",1/20/22,"Data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language/,1.80E+02,SOTA Improvement,"""Experiments on the major benchmarks of speech recognition, image classification, and natural lan guage understanding demonstrate a new state of the art or competitive performance to predominant approaches""",7.05E+08,"Section 4: ""We experiment with two model sizes: data2vec Base and
data2vec Large, containing either L = 12 or L = 24 Trans-
former blocks with H = 768 or H = 1024 hidden dimen-
sion (with 4 × H feed-forward inner-dimension)""
",,,ImageNet,,1.28E+06,"Section 5.1: 
""we pretrain data2vec on the images of the ImageNet-1K training
set""",,,,,,,,,Yes,Industry,,,5/29/23 20:51
ERNIE-ViLG,Multimodal,Vision-language generation,Baidu,Industry,"Han Zhang, Weichong Yin, Yewei Fang, Lanxin Li, Boqiang Duan, Zhihua Wu, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",12/31/21,ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation,https://arxiv.org/abs/2112.15283,1.20E+01,,,1.00E+10,"To explore the landscape of large-scale pre-training for bidirectional text-image generation,
we pre-train a 10-billion parameter model on a large-scale dataset of 145 million high-quality Chinese image-text pairs.",,,,,1.45E+08,"To explore the landscape of large-scale pre-training for bidirectional text-image generation,
we pre-train a 10-billion parameter model on a large-scale dataset of 145 million high-quality Chinese image-text pairs.",,,,,,,,,Yes,,,,5/29/23 20:51
ERNIE 3.0 Titan,Language,,Baidu,Industry,"Shuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng",12/23/21,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,https://arxiv.org/abs/2112.12731,1.10E+01,,,2.60E+11,source: https://twitter.com/BaiduResearch/status/1468633977242243078?t=6q4zuLNdTSc4GUBe9OM5Aw&s=19,3.14E+23,This model requires 2.1TB for parameter and optimizer states storage and 3.14E11 TeraFLOPS for training 300 billion tokens., ERNIE 3.0 Corpus,,6.68E+11,"""To ensure the success of the pre-training of ERNIE 3.0 Titan, we utilize the ERNIE 3.0 Corpus [ 2 ], a large-scale, wide-variety, and high-quality Chinese text corpora amounting to 4TB""

Assuming 167M words per GB",,,,,,,,,,Industry,,,7/24/23 6:50
GLIDE,Drawing,,OpenAI,Industry,"Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam Pamela Mishkin Bob McGrew IlyaSutskever MarkChen",12/20/21,GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models,https://arxiv.org/abs/2112.10741,2.38E+02,,,3.50E+09,"""Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking""",,,,,2.50E+08,"Section 4:
""We train our model on the same dataset as DALL-E (Ramesh
et al., 2021)""

This paper used 250M image-text pairs
https://arxiv.org/pdf/2102.12092.pdf",,,,,,,,,,Industry,,,5/29/23 20:51
XGLM,Language,,Meta AI,Industry,"Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li",12/20/21,Few-shot Learning with Multilingual Language Models,https://arxiv.org/abs/2112.10668,1.20E+01,SOTA improvement,"Our largest model (XGLM7.5B) sets a new
state of the art performance for few-shot learning in
more than 20 representative languages (including
medium- and low-resource languages) for the tasks
of commonsense reasoning, natural language infer-
ence and machine translation.",7.50E+09,,,,Subset of CC100-XL,,1.74E+09,,,,,,,Self-supervised learning,,,Yes,Industry,,,9/20/23 19:58
Gopher (280B),Language,Language modelling,DeepMind,Industry,"Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu and Geoffrey Irving",12/8/21,"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",https://deepmind.com/blog/article/language-modelling-at-scale,1.76E+02,,,2.80E+11,"""In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales — from models with tens of millions of parameters up to a 280 billion parameter model called Gopher.""",6.31E+23,"See table A24
6.31E+08 Train PFLOPs",,,2.25E+11,"""We train all models for 300 billion tokens with a 2048 token context window, using the Adam (Kingma
and Ba, 2014) optimiser.""

1 token ~ 0.75 words",,,,,,Self-supervised learning,891638.8,,Yes,Industry,,"We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25× fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",9/14/23 20:02
Player of Games,Games,,DeepMind,Industry,"Martin Schmid, Matej Moravcik, Neil Burch, Rudolf Kadlec, Josh Davidson, Kevin Waugh, Nolan Bard, Finbarr Timbers, Marc Lanctot, Zach Holland, Elnaz Davoodi, Alden Christianson, Michael Bowling",12/6/21,Player of Games,https://arxiv.org/abs/2112.03178,9.00E+00,SOTA Improvement,"Player of Games reaches strong performance in chess and Go, beats the strongest openly available agent in heads-up no-limit Texas hold'em poker (Slumbot), and defeats the state-of-the-art agent in Scotland Yard",,,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
NÜWA,Multimodal,,"Microsoft Research,Peking University",Industry,"Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan",11/24/21,NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion,https://arxiv.org/abs/2111.12417,3.60E+01,SOTA Improvement,"NÜWA achieves state-of-the-art results on text-to-image generation, text-to-video generation, video prediction, etc",8.70E+08,Section 4.1,4.84E+21,"From AI Tracker:
""Compute cost: End of Sec 4.1: ""We pre-train on 64 A100 GPUs for two weeks"". Info sheet from NVIDIA (https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet.pdf) gives single precision TensorFloat 32 performance of 156 TFLOPs/s. So we get 64 x 14 x 156 = 140,000 TFLOPs/s x days.""

Multiply by seconds/day and 30% utilization","Conceptual Captions, Moments in Time, VATEX",,,"we first pre-train N  ̈UWA on three
datasets: Conceptual Captions [22] for text-to-image (T2I)
generation, which includes 2.9M text-image pairs, Mo-
ments in Time [26] for video prediction (V2V), which in-
cludes 727K videos, and VATEX dataset [43] for text-to-
video (T2V) generation, which includes 241K text-video
pairs.",,,,,,,10446.84,,Yes,Industry,,"This paper presents a unified multimodal pre-trained model called NÜWA that can generate new or manipulate existing visual data (i.e., images and videos) for various visual synthesis tasks. To cover language, image, and video at the same time for different scenarios, a 3D transformer encoder-decoder framework is designed, which can not only deal with videos as 3D data but also adapt to texts and images as 1D and 2D data, respectively. A 3D Nearby Attention (3DNA) mechanism is also proposed to consider the nature of the visual data and reduce the computational complexity. We evaluate NÜWA on 8 downstream tasks. Compared to several strong baselines, NÜWA achieves state-of-the-art results on text-to-image generation, text-to-video generation, video prediction, etc. Furthermore, it also shows surprisingly good zero-shot capabilities on text-guided image and video manipulation tasks. Project repo is this https URL.",6/8/23 0:39
#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!
Japanese dialog transformers,Language,,NTT Communication Science Laboratories,Industry,"Hiroaki Sugiyama, Masahiro Mizukami, Tsunehiro Arimoto, Hiromi Narimatsu, Yuya Chiba, Hideharu Nakajima, Toyomi Meguro",11/9/21,Empirical Analysis of Training Strategies of Transformer-based Japanese Chit-chat Systems,https://arxiv.org/abs/2109.05217,3.10E+01,,,1.60E+09,"""We examined the improvement in model size in detail by considering four model sizes: 0.35B, 0.7B, 1.1B, and 1.6B parameters""",,,,,2.10E+09,"[Pairs of text]

""We obtained 2.1 billion (521 GB) pairs by this method. The average number of utterances in the input context was 2.913, and the average number of characters was 62.3 for the input context and 20.3 for the target utterance""",,,,,,,,,,Industry,,,5/29/23 20:51
EfficientZero,Games,,"Tsinghua University, UC Berkeley, Shanghai Qi Zhi institute",Academia,"Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, Yang Gao",10/30/21,Mastering Atari Games with Limited Data,https://arxiv.org/abs/2111.00210,4.00E+01,,,,,,"""Our implementation is computationally friendly. To train an Atari agent for 100k steps, it only needs 4 GPUs to train 7 hours.""",,,,,,,,,,,,,,Academia,,,5/29/23 20:51
Cloob,Multimodal,,"Ellis Unit Linz and LIT AI Lab, Johannes Kepler University, IARIA Vienna, HERE Technologies",Industry - Academia Collaboration (Academia Leaning),"Andreas Fürst ∗Elisabeth Rumetshofer ∗Johannes Lehner,Viet Tran,Fei Tang, Hubert Ramsauer, David Kreil, Michael Kopp, Günter Klambauer, Angela Bitto-Nemling, Sepp Hochreiter",10/21/21,CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP,https://arxiv.org/abs/2110.11316,2.00E+01,,,,,,,,,1.50E+07,"[Image-text pairs]
""To be comparable to the CLIP results, we use the same subset of 15 million samples from the YFCC100M dataset""",,,,,,,,,,Industry,,,5/29/23 20:51
PAGnol-XL,Language,Language modelling,"LightOn,LPENS,INRIA",Industry - Academia Collaboration,"Julien Launay, E.L. Tommasone, Baptiste Pannier, François Boniface, Amélie Chatelain, Alessandro Cappelli, Iacopo Poli, Djamé Seddah",10/16/21,PAGnol: An Extra-Large French Generative Model,https://arxiv.org/abs/2110.08554,3.00E+00,,,1.50E+09,,2.59E+20,"They report their compute directly.

From section 8: ""About 62k GPU-hours on the Jean Zay HPC Cluster."" Jean Zay uses both A100 and V100 GPUs, and maybe other stuff as well?

Note they explicitly call out V100 in their Appendix A.

https://www.hpcwire.com/2021/11/17/frances-jean-zay-supercomputer-gets-ai-boost-from-hpe-nvidia/",CCNet,They mostly use CCNet but also OSCAR for a comparison.,2.40E+10,Section 4.1: 32G tokens => 32e9*0.75 = 24e9 words,,,,,NVIDIA Tesla V100 SMX2,Self-supervised learning,,,Yes,,,"Access to large pre-trained models of varied architectures, in many different languages, is central to the democratization of NLP. We introduce PAGnol, a collection of French GPT models. Using scaling laws, we efficiently train PAGnol-XL (1.5B parameters) with the same computational budget as CamemBERT, a model 13 times smaller. PAGnol-XL is the largest model trained to date for the French language. We plan to train increasingly large and performing versions of PAGnol, exploring the capabilities of French extreme-scale models. For this first release, we focus on the pretraining and scaling calculations underlining PAGnol. We fit a scaling law for compute for the French language, and compare it with its English counterpart. We find the pre-training dataset significantly conditions the quality of the outputs, with common datasets such as OSCAR leading to low-quality offensive text. We evaluate our models on discriminative and generative tasks in French, comparing to other state-of-the-art French and multilingual models, and reaching the state of the art in the abstract summarization task. Our research was conducted on the public GENCI Jean Zay supercomputer, and our models up to the Large are made publicly available.",9/20/23 19:56
T0-XXL,Language,Language modelling,"Hugging Face,Brown University",Industry - Academia collaboration,"Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao,  Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng-Xin Yong, Harshit Pandey, Michael McKenna, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, Alexander M. Rush",10/15/21,Multitask Prompted Training Enables Zero-Shot Task Generalization,https://arxiv.org/abs/2110.08207,6.62E+02,,,1.10E+10,"""Unless specified otherwise, we use the XXL version which
has 11B parameters.""",1.79E+22,"From section B.1: ""These training runs corresponded to about 270 total hours of training on a v3-512 Cloud TPU device."" (512 cores for 270 hours)",,,,"Multitask - 12 tasks, 62 datasets. See fig 2 for details. 

This is going to be a nightmare to figure out! TODO figure out the sizes of each of these 62 datasets!

All datasets from here: https://arxiv.org/pdf/2109.02846.pdf",,,,,,,,,,Industry,,"Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at this https URL and all prompts are available at this https URL.",9/19/23 16:09
#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!
Yuan 1.0,Language,Language modelling,Inspur,Industry,"Shaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang, Chong Shen, Hongli Liu, Feng Li, Hong Zhu, Jiangang Luo, Liang Xu, Xuanwei Zhang, Jun Liu",10/12/21,Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning,https://arxiv.org/abs/2110.04725,1.20E+01,,,2.45E+11,"""With this method, Yuan 1.0, the current largest singleton language model with 245B parameters, achieves excellent performance on thousands GPUs during training, and the state-of-the-art results on NLP tasks.""",4.10E+23,source: https://www.aitracker.org/,,,8.35E+11,"""Yuan 1.0 was trained on a new Chinese dataset of 5TB
high-quality text that was built on 850TB raw data from Internet.""

1 GB ~ 167M words",,,,,,Self-supervised learning,606364.75,,Yes,Industry,,"Recent work like GPT-3 has demonstrated excellent performance of Zero-Shot and Few-Shot learning on many natural language processing (NLP) tasks by scaling up model size, dataset size and the amount of computation. However, training a model like GPT-3 requires huge amount of computational resources which makes it challengeable to researchers. In this work, we propose a method that incorporates large-scale distributed training performance into model architecture design. With this method, Yuan 1.0, the current largest singleton language model with 245B parameters, achieves excellent performance on thousands GPUs during training, and the state-of-the-art results on NLP tasks. A data processing method is designed to efficiently filter massive amount of raw data. The current largest high-quality Chinese corpus with 5TB high quality texts is built based on this method. In addition, a calibration and label expansion method is proposed to improve the Zero-Shot and Few-Shot performance, and steady improvement is observed on the accuracy of various tasks. Yuan 1.0 presents strong capacity of natural language generation, and the generated articles are difficult to distinguish from the human-written ones.",9/15/23 19:11
Megatron-Turing NLG 530B,Language,,"Microsoft,NVIDIA",Industry,"Ali Alvi, Paresh Kharya",10/11/21,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World’s Largest and Most Powerful Generative Language Model",https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/,2.81E+02,SOTA improvement,"The 105-layer, transformer-based MT-NLG improved upon the prior state-of-the-art models in zero-, one-, and few-shot settings",5.30E+11,,1.17E+24,https://www.lesswrong.com/posts/bGuMrzhJdENCo8BxX/nvidia-and-microsoft-releases-530b-parameter-transformer?commentId=HSJSNspKp94tFcSCx,"Common Crawl, The Pile"," In addition to Common Crawl data, we leveraged a number of other previously generated datasets. From The Pile, we selected Books3, OpenWebText2, Stack Exchange, PubMed Abstracts,
Wikipedia, Gutenberg (PG-19), BookCorpus2, NIH ExPorter, and Pile-CC datasets. We also included the
CC-Stories and RealNews datasets used to train Megatron",2.03E+11,"""Our training dataset consists of 339 billion tokens and we
trained MT-NLG on 270 billions tokens by blending the 15 training datasets as described above. We also set aside 2% of our data for validation.""

1 token ~ 0.75 words",,,,,,Self-supervised learning,3046994.09,,Yes,Industry,,"Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.",8/15/23 14:40
M6-10T,Multimodal,,Alibaba,Industry,"Junyang Lin, An Yang, Jinze Bai, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Yong Li, Wei Lin, Jingren Zhou, Hongxia Yang",10/8/21,M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining,https://arxiv.org/abs/2110.03888,1.70E+01,,,1.00E+13,"""We demonstrate a practice of pretraining unprecedented 10-trillion-parameter model, an order of magnitude larger than the state-of-the-art, on solely 512 GPUs within 10 days""",5.53E+21,"512 GPUs in 10 days - using NVIDIA V100 GPUs

Using the NVIDIA V100 Specifications this works out to be: 
0.30 * 125E12 * 512 * 10 * 86400 = 1.66E22

(Assuming 30% utilisation, and 125 TFLOPS)",BookCorpus; English Wikipedia,,8.00E+09,"""We conduct experiments for pretraining and finetuning to analyze model competence in upstream and
downstream tasks. Following the classical data setup for pretraining and finetuning, we pretrain the model on BookCorpus [52] and English Wikipedia [9], which are corpora with around 16GB of plain
texts.""

I used http://extraconversion.com/data-storage/gigabits/gigabits-to-words.html for the conversion to number of words",,,,512 GPUs * 10 days * 24 h/day,,Self-supervised learning,20073.49,,Yes,Industry,,"Recent expeditious developments in deep learning algorithms, distributed training, and even hardware design for large models have enabled training extreme-scale models, say GPT-3 and Switch Transformer possessing hundreds of billions or even trillions of parameters. However, under limited resources, extreme-scale model training that requires enormous amounts of computes and memory footprint suffers from frustratingly low efficiency in model convergence. In this paper, we propose a simple training strategy called ""Pseudo-to-Real"" for high-memory-footprint-required large models. Pseudo-to-Real is compatible with large models with architecture of sequential layers. We demonstrate a practice of pretraining unprecedented 10-trillion-parameter model, an order of magnitude larger than the state-of-the-art, on solely 512 GPUs within 10 days. Besides demonstrating the application of Pseudo-to-Real, we also provide a technique, Granular CPU offloading, to manage CPU memory for training large model and maintain high GPU utilities. Fast training of extreme-scale models on a decent amount of resources can bring much smaller carbon footprint and contribute to greener AI.",7/28/23 2:09
PLATO-XL,Language,Language modelling,Baidu,Industry,"Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhihua Wu, Zhen Guo, Hua Lu, Xinxian Huang, Xin Tian, Xinchao Xu, Yingzhan Lin, Zheng-Yu Niu",9/20/21,PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation,https://arxiv.org/abs/2109.09519,4.20E+01,SOTA Improvement,,1.10E+10,,,,,,,,,,,,NVIDIA Tesla V100 DGXS 32 GB,,,,,Industry,Likely,"To explore the limit of dialogue generation pre-training, we present the models of PLATO-XL with up to 11 billion parameters, trained on both Chinese and English social media conversations. To train such large models, we adopt the architecture of unified transformer with high computation and parameter efficiency. In addition, we carry out multi-party aware pre-training to better distinguish the characteristic information in social media conversations. With such designs, PLATO-XL successfully achieves superior performances as compared to other approaches in both Chinese and English chitchat. We further explore the capacity of PLATO-XL on other conversational tasks, such as knowledge grounded dialogue and task-oriented conversation. The experimental results indicate that PLATO-XL obtains state-of-the-art results across multiple conversational tasks, verifying its potential as a foundation model of conversational AI.",9/19/23 16:41
DLRM-2022,Recommendation,,Facebook,Industry,"D Mudigere, Y Hao, J Huang, A Tulloch",9/15/21,Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models,https://arxiv.org/abs/2104.05158,1.00E+01,,,3.00E+12,"Figure 1

https://arxiv.org/abs/2104.05158",1.10E+21,"Figure 1

https://arxiv.org/abs/2104.05158",,,,,,,,,"NVIDIA Tesla V100 DGXS 32 GB,NVIDIA A100",,2394.07,,,Industry,,"Deep learning recommendation models (DLRMs) are used across many business-critical services at Facebook and are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper we discuss the SW/HW co-designed solution for high-performance distributed training of large-scale DLRMs. We introduce a high-performance scalable software stack based on PyTorch and pair it with the new evolution of Zion platform, namely ZionEX. We demonstrate the capability to train very large DLRMs with up to 12 Trillion parameters and show that we can attain 40X speedup in terms of time to solution over previous systems. We achieve this by (i) designing the ZionEX platform with dedicated scale-out network, provisioned with high bandwidth, optimal topology and efficient transport (ii) implementing an optimized PyTorch-based training stack supporting both model and data parallelism (iii) developing sharding algorithms capable of hierarchical partitioning of the embedding tables along row, column dimensions and load balancing them across multiple workers; (iv) adding high-performance core operators while retaining flexibility to support optimizers with fully deterministic updates (v) leveraging reduced precision communications, multi-level memory hierarchy (HBM+DDR+SSD) and pipelining. Furthermore, we develop and briefly comment on distributed data ingestion and other supporting services that are required for the robust and efficient end-to-end training in production environments.",6/21/23 18:42
HyperClova,Language,,"NAVER,Search Solutions, Inc.",Industry,"Boseop Kim, HyoungSeok Kim, Sang-Woo Lee, Gichang Lee, Donghyun Kwak, Dong Hyeon Jeon, Sunghyun Park, Sungju Kim, Seonhoon Kim, Dongpil Seo, Heungsub Lee, Minyoung Jeong, Sungjae Lee, Minsub Kim, Suk Hyun Ko, Seokhun Kim, Taeyong Park, Jinuk Kim, Soyoung Kang, Na-Hyeon Ryu, Kang Min Yoo, Minsuk Chang, Soobin Suh, Sookyo In, Jinseong Park, Kyungduk Kim, Hiun Kim, Jisu Jeong, Yong Goo Yeo, Donghoon Ham, Dongju Park, Min Young Lee, Jaewook Kang, Inho Kang, Jung-Woo Ha, Woomyoung Park, Nako Sung",9/10/21,What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers,https://arxiv.org/abs/2109.04650,6.30E+01,,,8.20E+10,"""We introduce a Korean in-context large-scale LM with 82B parameters, i.e., HyperCLOVA. This is the first discovery on near
100B-scale non-English LM.""",1.48E+23,"""For experiments in Section 4, the model trained with 150B is used for fair comparison, because not all models are finished training at the same iteration. However, experiments in Section 5.2 use the model trained with 300B tokens, as HyperCLOVA Studio provided the 39B and 82B models trained with 300B tokens.""

82e9 connections * 2 FLOP/connection * 300e9 tokens * 3 backward pass = 1.476e23 FLOP

Calculation using GPU time corroborates this:
- ""Our model is based on megatron-LM (Shoeybi et al., 2019) and trained on the NVIDIA Superpod, which includes 128 strongly clustered DGX servers with 1,024 A100 GPUs.""
- ""It takes 13.4 days to train a model with 82B parameters with 150B tokens."" Assume 300B tokens takes twice as long, 26.8 days.
- Assume the default of 30% utilization rate for large language models.

1024 A100 GPUs * 312e12 FLOP/second * 0.3 utilization * 26.8 days * 24 * 60 * 60 seconds/day = 2.219e+23 FLOP",,,1.90E+11,"""We introduce HyperCLOVA, a large-scale
Korean in-context learning-based LM with
nearly 100B parameters, by constructing a
large Korean-centric corpus of 560B tokens.""

Based on tokenizing the Hyperclova article itself using OpenAI's tiktoken BPE tokenizer (https://github.com/openai/tiktoken), there are 3285 tokens for 1069 words - about 3 tokens per word.

This work uses a special tokenizer, but based on Figure 5 in Appendix E, the number of tokens seems similar between different tokenization methods.

Based on that, 5.6e11 Korean tokens ~= 1.9e11 words",,,,,,Self-supervised learning,103802.31,,,Industry,,,8/15/23 14:46
MEB,Search,,Microsoft Bing,Industry,"W Liu, Z Wang, X Liu, N Zeng, Y Liu, FE Alsaadi",9/4/21,Make Every feature Binary: A 135B parameter sparse neural network for massively improved search relevance,https://www.microsoft.com/en-us/research/blog/make-every-feature-binary-a-135b-parameter-sparse-neural-network-for-massively-improved-search-relevance/,2.60E+01,Significant use,"""MEB is running in production for 100 percent of Bing searches, in all regions and languages.""",1.35E+11,See paper title,,,,,,"""MEB uses three years of search logs from Bing as training data."" TODO convert",,,,,,,,,,Industry,,,5/29/23 20:51
FLAN,Language,Language modelling,Google Research,Industry,"Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le",9/3/21,Finetuned Language Models Are Zero-Shot Learners,https://arxiv.org/abs/2109.01652,2.40E+02,SOTA Improvement,"Abstract: 
""FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate.""",1.37E+11,"Abstract:
""We take a 137B parameter pretrained language model and instruction tune it on
over 60 NLP datasets verbalized via natural language instruction templates. We
evaluate this instruction-tuned model, which we call FLAN, on unseen task types.""

Many models seem to be using the same 137B base transformer model?",4.90E+22,"From section 2.4: ""60 hours on a TPUv3 with 128 cores."" I assume that ""128 cores"" = 128 TPUv3s. Which took less than 2% of total time (see environmental considerations section)",,"Abstract: ""We take a 137B parameter pretrained language model and instruction tune it on
over 60 NLP datasets""",1.87E+12,"""Model architecture and pretraining. In our experiments, we use LaMDA-PT, a dense left-to-right,
decoder-only transformer language model of 137B parameters (Thoppilan et al., 2022). This model
is pretrained on a collection of web documents (including those with computer code), dialog data,
and Wikipedia, tokenized into 2.49T BPE tokens with a 32k vocabulary using the SentencePiece
library (Kudo & Richardson, 2018). Around 10% of the pretraining data was non-English. Note that
LaMDA-PT only has language model pretraining (c.f. LaMDA, which was finetuned for dialog).""

2.49e12 tokens ~= 1.87e12 words",,,,,,Self-supervised learning,,,,Industry,,"This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning—finetuning language models on a collection of datasets described via instructions—substantially improves zeroshot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",9/20/23 19:57
XLMR-XXL,Language,,Facebook AI Research,Industry,"Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau",8/17/21,Larger-Scale Transformers for Multilingual Masked Language Modeling,https://arxiv.org/abs/2105.00572,1.80E+01,SOTA Improvement,"Abstract:
""Our model also outperforms
the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages.""",1.07E+10,"Section 2.1:
"" ...XLM-RXXL (L= 48, H = 4096, A = 32, 10.7B params)""",,,CC100,,1.25E+11,"""We pretrain the models on the CC100 dataset, which corresponds to 167B tokens in 100 languages.""

1 token ~ 0.7 words",,,,,,,,,,Industry,,,6/8/23 0:39
Jurassic-1-Jumbo,Language,,AI21 Labs,Industry,"Opher Lieber, Or Sharir, Barak Lenz, Yoav Shoham",8/11/21,Jurassic-1: Technical Details and Evaluation,https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf,5.50E+01,,,1.78E+11,"""Jurassic-1 models come in two sizes, where the Jumbo version, at 178B parameters, is the largest and most sophisticated language model ever released for general use by developers.""",3.70E+23,see here https://docs.google.com/document/d/1B8x6XYcmB1u6Tmq3VcbAtj5bzhDaj2TcIPyK6Wpupx4/edit,,,2.25E+11,"""Our model was trained with the conventional self-supervised auto-regressive training objective on 300B tokens drawn from publicly available resources""

1 token ~ 0.75 words",,,,,,,805277.01,,Yes,Industry,,"Jurassic-1 is a pair of auto-regressive language models recently released by AI21 Labs, consisting of J1-Jumbo, a 178B-parameter model, and J1-Large, a 7B-parameter model. We describe their architecture and training, and evaluate their performance relative to GPT-3. The evaluation is in terms of perplexity, as well as zero-shot and few-shot learning. To that end, we developed a zeroshot and few-shot test suite, which we made publicly available (https://github.com/ai21labs/ lm-evaluation) as a shared resource for the evaluation of mega language models.",5/29/23 20:51
Zidong Taichu,Multimodal,,Chinese Academy of Sciences,,,8/11/21,Zidong Ancestral multi-modal large model,https://gitee.com/zidongtaichu/multi-modal-models,0.00E+00,Historical significance,"The world’s first image, language, and audio trimodal pre-trained model.",1.00E+11,,,,,,,,,,,,,,,,,,Likely,,9/19/23 16:41
SEER,Vision,,"Facebook AI Research,Inria",Industry - Academia Collaboration (Industry leaning),"Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, Piotr Bojanowski",7/29/21,Self-supervised Pretraining of Visual Features in the Wild,https://arxiv.org/abs/2103.01988,1.38E+02,,"Applies self-supervised learning to outside ImageNet and unbounded datasets, with good results",1.30E+09,"From abstract:
"" Our final SElf-supERvised (SEER) model, a RegNetY with 1.3B parameters...""",4.42E+21,"Numbers from section 3.2

512 GPUs * 0.1 * 8days * 24h/day * 3600s/h * 125 TFLOP/s",,"Section 3.3:
""For our billion scale pretraining, we consider a dataloader that directly samples random, public, and non-EU images from Instagram""

Note the dataset is not static - it is refreshed every 90 days",1.00E+09,"""Overall, we train
on 1B images for a total of 122K iterations.""",,,,,NVIDIA Tesla V100 DGXS 32 GB,,16058.8,,Yes,Industry,,"Recently, self-supervised learning methods like MoCo, SimCLR, BYOL and SwAV have reduced the gap with supervised methods. These results have been achieved in a control environment, that is the highly curated ImageNet dataset. However, the premise of self-supervised learning is that it can learn from any random image and from any unbounded dataset. In this work, we explore if self-supervision lives to its expectation by training large models on random, uncurated images with no supervision. Our final SElf-supERvised (SEER) model, a RegNetY with 1.3B parameters trained on 1B random images with 512 GPUs achieves 84.2% top-1 accuracy, surpassing the best self-supervised pretrained model by 1% and confirming that self-supervised learning works in a real world setting. Interestingly, we also observe that self-supervised models are good few-shot learners achieving 77.9% top-1 with access to only 10% of ImageNet. Code: this https URL",6/7/23 18:30
GOAT,Games,Open ended play,DeepMind,Industry,Open- Ended Learning Team,7/27/21,Open-Ended Learning Leads to Generally Capable Agents,https://deepmind.com/blog/article/generally-capable-agents-emerge-from-open-ended-play,6.40E+01,,,3.50E+06,estimate described here: https://docs.google.com/document/d/1S9xZyCeITDOs-P1W_-liNW0WgVN-OLsSudVrPXMaLqw/edit?usp=sharing,7.80E+22,"[Final calculation]
(8 TPUs)(4.20e14 FLOP/s)(0.1 utilisation rate)(32 agents)(7.3e6 s/agent) = 7.8e22 FLOPs

==========================
NOTES BELOW

[Hardware]
- ""Each agent is trained using 8 TPUv3s and consumes approximately 50,000 agent steps (observations) per second.""
- TPUv3 (half precision): 4.2e14 FLOP/s
- Number of TPUs: 8
- Utilisation rate: 0.1

[Timesteps]
- Figure 16 shows steps per generation and agent. In total there are 1.5e10 + 4.0e10 + 2.5e10 + 1.1e11 + 2e11 = 3.9e11 steps per agent.
- 3.9e11 / 5e4 = 8e6 s → ~93 days
- 100 million steps is equivalent to 30 minutes of wall-clock time in our setup. (pg 29, fig 27)
- 1e8 steps → 0.5h
- 3.9e11 steps → 1950h → 7.0e6 s → ~82 days
- Both of these seem like overestimates, because:
“Finally, on the largest timescale (days), generational training iteratively improves population performance by bootstrapping off previous generations, whilst also iteratively updating the validation normalised percentile metric itself.” (pg 16)
- Suggests that the above is an overestimate of the number of days needed, else they would have said (months) or (weeks)?
- Final choice (guesstimate): 85 days = 7.3e6 s

[Population size]
- 8 agents? (pg 21) → this is describing the case where they’re not using PBT, so ignore this number
- The original PBT paper uses 32 agents for one task https://arxiv.org/pdf/1711.09846.pdf (in general it uses between 10 and 80)
- (Guesstimate) Average population size: 32",XLand,,3.90E+11,Figure 16 shows steps per generation and agent. In total there are 1.5e10 + 4.0e10 + 2.5e10 + 1.1e11 + 2e11 = 3.9e11 steps per agent.,,,,,,,122418.97,,Yes,Industry,,"In this work we create agents that can perform well beyond a single, individual task, that exhibit much wider generalisation of behaviour to a massive, rich space of challenges. We define a universe of tasks within an environment domain and demonstrate the ability to train agents that are generally capable across this vast space and beyond. The environment is natively multi-agent, spanning the continuum of competitive, cooperative, and independent games, which are situated within procedurally generated physical 3D worlds. The resulting space is exceptionally diverse in terms of the challenges posed to agents, and as such, even measuring the learning progress of an agent is an open research problem. We propose an iterative notion of improvement between successive generations of agents, rather than seeking to maximise a singular objective, allowing us to quantify progress despite tasks being incomparable in terms of achievable rewards. We show that through constructing an open-ended learning process, which dynamically changes the training task distributions and training objectives such that the agent never stops learning, we achieve consistent learning of new behaviours. The resulting agent is able to score reward in every one of our humanly solvable evaluation levels, with behaviour generalising to many held-out points in the universe of tasks. Examples of this zero-shot generalisation include good performance on Hide and Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks we characterise the behaviour of our agent, and find interesting emergent heuristic behaviours such as trial-and-error experimentation, simple tool use, option switching, and cooperation. Finally, we demonstrate that the general capabilities of this agent could unlock larger scale transfer of behaviour through cheap finetuning.",5/29/23 20:51
HuBERT,Language,,Facebook AI Research,Industry,"Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed",7/27/21,HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units,https://arxiv.org/abs/2106.07447,4.58E+02,SOTA Improvement,"Abstract: 
"" the
HuBERT model either matches or improves upon the state-ofthe-art wav2vec 2.0 performance on the Librispeech (960h) and
Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and
960h fine-tuning subsets.""",1.00E+09,"From abstract:
""Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets""",5.54E+21,"GPU NOT SPECIFIED - for the sake of argument I assume something on the order of 1 TFLOP/s

Numbers from Section IV part C
0.1 * (960h * 32GPUs + 60000h * 256 GPUs) * 3600s/h * 1 TFLOP/s/GPU",LibriSpeech,,8.21E+08,"""When the HuBERT model is pre-trained on either the standard Librispeech 960h [24] or the Libri-Light 60k hours [25], it either matches or improves upon the state-of-theart wav2vec 2.0 [6] performance on all fine-tuning subsets of 10mins, 1h, 10h, 100h, and 960h.""

1h ~ 13,680 words
13,680 * 60,000 = 820800000",,,,,,,8632.11,,Yes,Industry,,"Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.",5/29/23 20:51
Codex,Language,Code autocompletion,Open AI,Industry,"Mark Chen , Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,  Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,  Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba ",7/7/21,Evaluating Large Language Models Trained on Code,https://openai.com/blog/openai-codex/,3.01E+02,Significant use,,1.20E+10,"""With just a single sample, a 12B parameter Codex solves 28.8% of these problems, and a 300M parameter Codex solves 13.2% of these problems""",,"""The original training of GPT-3-12B consumed hundreds of petaflop/sdays of compute, while fine-tuning it to create Codex-12B
consumed a similar amount of compute.""
",,,3.18E+10,"""Our training dataset was collected in May 2020 from 54 million public software repositories hosted on GitHub, containing 179 GB of unique Python files under 1 MB. We filtered out files which were likely auto-generated, had average line
length greater than 100, had maximum line length greater
than 1000, or contained a small percentage of alphanumeric
characters. After filtering, our final dataset totaled 159 GB.""

1 GB ~ 200M words",,,,,,,,,Yes,Industry,,,5/29/23 20:51
ERNIE 3.0,Language,,Baidu Inc.,Industry,"Y Sun, S Wang, S Feng, S Ding, C Pang",7/5/21,ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,http://research.baidu.com/Blog/index-view?id=160,1.00E+02,,,1.00E+10,We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph.,2.25E+22,"Section 3.3.3: 
""""The model is trained for
a total of 375 billion tokens""

Total compute approximated as 6*N*D",,,6.68E+11,"""To ensure the success of the pre-training of ERNIE 3.0, we construct a large-scale, wide-variety and high-quality Chinese text corpora amounting to 4TB storage size in 11 different categories.""

1 GB ~ 167M chinese words",,,,,,,3.83,,Yes,Industry,,,6/8/23 0:39
EfficientNetV2,Vision,Image Classification,Google,Industry,"Mingxing Tan, Quoc V. Le",6/23/21,EfficientNetV2: Smaller Models and Faster Training,EfficientNetV2: Smaller Models and Faster Training,1.39E+03,"SOTA Improvement,Highly cited","""EfficientNetV2 achieves 87.3% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while 
 training 5x-11x faster using the same computing resources.""",2.08E+08,"Table 7, page 7",6.22E+18,"Table 7, page 7: 45 hours on 32 TPUv3 cores.
4e12*32*60*60*45*0.3 = 6.2e18 FLOP",ImageNet21k,,1.42E+07,,,,45,Table 7,Google TPU V3,Supervised,,,,Industry,Likely,"This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller.
Our training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose to adaptively adjust regularization (e.g., dropout and data augmentation) as well, such that we can achieve both fast training and good accuracy.
With progressive learning, our EfficientNetV2 significantly outperforms previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our EfficientNetV2 achieves 87.3% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while training 5x-11x faster using the same computing resources. Code will be available at this https URL.",9/20/23 19:58
ALIGN,Multimodal,Representation Learning,Google Research,Industry,"ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,HieuPham,QuocV.Le,YunhsuanSung, Zhen Li, and Tom Duerig",6/11/21,Scaling up visual and vision-language representation learning with noisy text supervision,https://arxiv.org/abs/2102.05918,6.41E+02,,,8.20E+08,"From author communication

 480M (image tower) + 340 M (text tower)",2.15E+23,"From author communication

14.82K TPUv3 core-days

Precision: bfloat16

Estimation

TPUv3 at float16: 4.20E+14 

0.1 * 4.20E+14 FLOP/s * 14.82k TPU-days * 24h/day * 3600s/h
= 2.15E+23

",,,1.60E+09,"Dataset contains 1.8B image-text pairs, then some duplicates are removed.",,,,,,Self-supervised learning,357760.33,,Yes,Industry,,"Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.",8/11/23 18:01
Denoising Diffusion Probabilistic Models (LSUN Bedroom),Drawing,,UC Berkeley,Academia,"Jonathan Ho, Ajay Jain, Pieter Abbeel",6/11/21,Denoising Diffusion Probabilistic Models,https://arxiv.org/abs/2006.11239,9.18E+02,SOTA Improvement,"Novel approach to image synthesis that yields SOTA results on datasets like CIFAR-10

Abstract: 
""On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. """,2.56E+08,"Appendix B: 
"" Our CIFAR10 model has 35.7 million parameters, and our LSUN and
CelebA-HQ models have 114 million parameters. We also trained a larger variant of the LSUN Bedroom model with approximately 256 million parameters by increasing filter count.""",9.50E+19,"Numbers in Appendix B
10.6h for the CIFAR model (batch size 128, 21 step/s)
2.2 step/s for the LSUN model, 1.15M steps so 702.8 hours

1.25E14 FLOP/s * 702.8h * 3600s/h * 0.3 = 9.5e19",LSUN Bedroom,,3.03E+06,"""We trained on CelebA-HQ for 0.5M steps, LSUN Bedroom for 2.4M steps, LSUN Cat for 1.8M steps, and LSUN Church for 1.2M steps.""

""The CelebA-HQ dataset is a high-quality version of CelebA that consists of 30,000 images at 1024×1024 resolution.""
https://paperswithcode.com/dataset/celeba-hq

LSUN bedroom has 3,033,042 examples. LSUN cat has 1,657,266 examples. LSUN church has 126,227 examples.
https://www.tensorflow.org/datasets/catalog/lsun
",,,,,Google TPU V3,,2.6,,,Academia,,,8/1/23 10:19
DeBERTa,Language,,Microsoft,Industry,"Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen",6/10/21,DeBERTa: Decoding-enhanced BERT with Disentangled Attention,https://arxiv.org/abs/2006.03654,6.61E+02,,,1.50E+09,"""...we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters""

Other versions are smaller and use a smaller pre-training dataset. These are distinguished in the paper (e.g. DeBERTa1.5B is the version of DeBERTa with 1.5 billion parameters).",6.00E+21,"From section 5.1.1: ""We use 6 DGX-2 machines (96 V100 GPUs) to train the models. A single model trained with 2K batch size and 1M steps takes about 20 days."" 

This specifically refers to the largest models referred to in the paper, and smaller models are described elsewhere, but I'm assuming the large models are what we care about here. 

Apparently there are multiple types of GPUs referred to as V100s. I'm guessing these are NVIDIA Tesla SMX2s.",,,1.56E+10,""" DeBERTa is pretrained on 78G training data""

1GB ~ 200M words",,,,,,,,,Yes,Industry,,"Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).",5/29/23 20:51
ViT-G/14,Vision,,Google Brain,Industry,"X Zhai, A Kolesnikov, N Houlsby, L Beyer",6/8/21,Scaling Vision Transformers,https://arxiv.org/abs/2106.04560,3.02E+02,,,1.80E+09,source: https://lair.lighton.ai/akronomicon/,3.40E+21,source: https://lair.lighton.ai/akronomicon/,JFT-3B,,3.00E+09,"""For this study, we use the proprietary JFT-3B dataset, a larger version of the JFT-300M dataset used
in many previous works on large-scale computer vision models [31, 18, 11]. This dataset consists of
nearly 3 billion images, annotated with a class-hierarchy of around 30k labels via a semi-automatic
pipeline""",,,,,,Self-supervised learning,5541.84,,Yes?,Industry,,"Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10 examples per class.",8/11/23 18:02
Wu Dao 2.0,Multimodal,,BAAI,Non-profit,A Tarantola,5/31/21,China's gigantic multi-modal AI is no one-trick pony,https://www.engadget.com/chinas-gigantic-multi-modal-ai-is-no-one-trick-pony-211414388.html,0.00E+00,,,1.75E+12,"""It's been trained on 1.75 trillion parameters""",,,WuDao Corpora,"WuDao Corpora, as of version 2.0, was a large dataset constructed for training Wu Dao 2.0. It contains 3 terabytes of text scraped from web data, 90 terabytes of graphical data (incorporating 630 million text/image pairs), and 181 gigabytes of Chinese dialogue (incorporating 1.4 billion dialogue rounds).
https://en.wikipedia.org/wiki/Wu_Dao#WuDao_Corpora",,,,,,,,,,,,Industry,,,9/19/23 16:07
CogView,Drawing,Text-to-image,"Tsinghua University, DAMO academy Alibaba",Industry - Academia Collaboration (Academia leaning),"M Ding, Z Yang, W Hong, W Zheng, C Zhou",5/26/21,CogView: Mastering Text-to-Image Generation via Transformers,https://arxiv.org/abs/2105.13290,1.41E+02,,,4.00E+09,"""We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem.""",2.68E+22,source: https://lair.lighton.ai/akronomicon/,,,3.00E+07,"""We collected about 30 million text-image pairs from multiple channels, and built a 2.5TB new
dataset (after tokenization, the size becomes about 250GB).""",,,,,,,44452.39,,Yes,Industry,,"Text-to-Image generation in the general domain has long been an open problem, which requires both a powerful generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E.",5/29/23 20:51
Transformer local-attention (NesT-B),Vision,,"Google Cloud,Google Research",Industry,"Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, Sercan Arık, Tomas Pfister",5/26/21,"Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding",https://arxiv.org/abs/2105.12723v4,5.73E+03,Highly cited,,9.01E+07,"Table A2, NesT-B is the largest size.",2.41E+19,"17.9 GFLOPS per forward pass
300 epochs
1.28M training examples
3.5 f_to_b pass ratio
(From Imagenet paper-data, Besiroglu et al., forthcoming) ",Imagenet-1k,,1.28E+06,,,,,,,Self-supervised learning,39.51,,Yes,Industry,,"Hierarchical structures are popular in recent vision transformers, however, they require sophisticated designs and massive datasets to work well. In this paper, we explore the idea of nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical way. We find that the block aggregation function plays a critical role in enabling cross-block non-local information communication. This observation leads us to design a simplified architecture that requires minor code changes upon the original vision transformer. The benefits of the proposed judiciously-selected design are threefold: (1) NesT converges faster and requires much less training data to achieve good generalization on both ImageNet and small datasets like CIFAR; (2) when extending our key ideas to image generation, NesT leads to a strong decoder that is 8× faster than previous transformer-based generators; and (3) we show that decoupling the feature learning and abstraction processes via this nested hierarchy in our design enables constructing a novel method (named GradCAT) for visually interpreting the learned model. Source code is available this https URL.",8/11/23 16:56
ConSERT,Language,Language modelling,,Academia,"Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang, Wei Wu, Weiran Xu",5/25/21,ConSERT: A contrastive framework for self-supervised sentence representation transfer,https://arxiv.org/abs/2105.11741,3.35E+02,SOTA Improvement,Trains an effective BERT model on small sample sizes and achieves an 8% improvement over previous SOTA on STA datasets.,3.45E+08,,2.80E+20,"Fine-tuning was done using a single Nvidia V100 GPU for a few minutes -> 1.0E+15 to 5.0E+15 (2 to 10 min)

Foundation model is BeRT with 2.8e+20 FLOP.

So total compute is 2.8e+20.",,,,,,,0.1,,NVIDIA Tesla V100S PCIe 32 GB,,,,,,Likely,"Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though BERT-based pre-trained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised Sentence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way. By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks. Experiments on STS datasets demonstrate that ConSERT achieves an 8\% relative improvement over the previous state-of-the-art, even comparable to the supervised SBERT-NLI. And when further incorporating NLI supervision, we achieve new state-of-the-art performance on STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.",9/19/23 16:07
ProtT5-XXL,Other,Proteins,"Technical University of Munich,Med AI Technology,NVIDIA,Oak Ridge National Laboratory,Google",Industry - Academia Collaboration,"A Elnaggar, M Heinzinger, C Dallago, G Rihawi",5/4/21,ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Learning,https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3,3.96E+02,,,1.10E+10,source: https://lair.lighton.ai/akronomicon/,7.37E+22,source: https://lair.lighton.ai/akronomicon/,UniRef; BDF,,3.93E+11,"""Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids.""",,,,,,Self-supervised learning,123918.36,,Yes,Industry,,"Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores. Dimensionality reduction revealed that the raw protein LM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks. The first was a per-residue prediction of protein secondary structure (3-state accuracy Q3=81%-87%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81%) and membrane vs. water-soluble (2-state accuracy Q2=91%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the grammar of the language of life. To facilitate future work, we released our models at https://github.com/agemagician/ProtTrans.",8/10/23 15:28
GPT-J-6B,Language,,,Research collective,Aran Komatsuzaki,5/1/21,GPT-J-6B: 6B JAX-Based Transformer,https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/,0.00E+00,,,6.05E+09,source: model details table in GitHub,1.50E+22,source: zero shot evaluation table in GitHub,,,1.60E+11,"""The model was trained on 400B tokens from The Pile dataset with 800GB text.""

1 GB ~ 200M words",,,,,,Self-supervised learning,25176.8,,Yes,Industry,,,9/19/23 16:07
PanGu-α,Language,,Huawei Noah's Ark Lab,Industry,"Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi LiaoZhiwei WangXin JiangZhenzhang YangKaisheng WangXiaoda ZhangChen LiZiyan GongYifan YaoXinjing HuangJun WangJianfeng YuQi GuoYue YuYan ZhangJin WangHengtao TaoDasen YanZexuan YiFang PengFangqing JiangHan ZhangLingfeng DengYehong ZhangZhe LinChao ZhangShaojie ZhangMingyue GuoShanzhi GuGaojun FanYaowei WangXuefeng JinQun LiuYonghong Tian",4/25/21,PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation,https://arxiv.org/abs/2104.12369,6.50E+01,,,2.07E+11,"Table in https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha

Note: Directory of LLMs (https://docs.google.com/spreadsheets/d/1gc6yse74XCwBx028HV_cvdxwXkmXejVjkO-Mz2uwE0k/edit#gid=0)
gives a slightly lower estimate, not sure about source",5.83E+22,source: https://lair.lighton.ai/akronomicon/,,Custom dataset,2.00E+11,"""The composition of our corpus and the processing steps adopted to each data source is shown in Table 3.2.
Based on the new corpus, we construct two training datasets with 100GB and 1TB text data for our medium (2.6B and 13B) and large (200B) models, respectively""

1 TB = 1000 GB
1 GB ~ 200M words",,,,,,Self-supervised learning,97802.06,,Yes,Industry,,"Large-scale Pretrained Language Models (PLMs) have become the new paradigm for Natural Language Processing (NLP). PLMs with hundreds of billions parameters such as GPT-3 have demonstrated strong performances on natural language understanding and generation with \textit{few-shot in-context} learning. In this work, we present our practice on training large-scale autoregressive language models named PanGu-α, with up to 200 billion parameters. PanGu-α is developed under the MindSpore and trained on a cluster of 2048 Ascend 910 AI processors. The training parallelism strategy is implemented based on MindSpore Auto-parallel, which composes five parallelism dimensions to scale the training task to 2048 processors efficiently, including data parallelism, op-level model parallelism, pipeline model parallelism, optimizer model parallelism and rematerialization. To enhance the generalization ability of PanGu-α, we collect 1.1TB high-quality Chinese data from a wide range of domains to pretrain the model. We empirically test the generation ability of PanGu-α in various scenarios including text summarization, question answering, dialogue generation, etc. Moreover, we investigate the effect of model scales on the few-shot performances across a broad range of Chinese NLP tasks. The experimental results demonstrate the superior capabilities of PanGu-α in performing various tasks under few-shot or zero-shot settings.",9/21/23 3:12
PLUG,Language,,Alibaba Group,Industry,,4/19/21,,https://mp.weixin.qq.com/s/DAQomIkDa52Sef-ruyH5qg,0.00E+00,SOTA Improvement,Was a SOTA in CLUE 1.0 https://www.cluebenchmarks.com/classification10.html,2.70E+10,,3.60E+22,128 Nvidia A100 for 35 days,,,,,,,,,,,,,,,,,9/19/23 16:07
DLRM-12T,Recommendation,Recommender system,"Meta AI,Carnegie Mellon University",Industry - Academia Collaboration (Industry leaning),"Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Zhihao Jia, Andrew Tulloch, Srinivas Sridharan, Xing Liu, Mustafa Ozdal, Jade Nie, Jongsoo Park, Liang Luo, Jie Amy Yang, Leon Gao, Dmytro Ivchenko, Aarti Basant, Yuxi Hu, Jiyan Yang, Ehsan K. Ardestani, Xiaodong Wang, Rakesh Komuravelli, Ching-Hsiang Chu, Serhat Yilmaz, Huayu Li, Jiyuan Qian, Zhuobo Feng, Yinbin Ma, Junjie Yang, Ellie Wen, Hong Li, Lin Yang, Chonglin Sun, Whitney Zhao, Dimitry Melts, Krishna Dhulipala, KR Kishore, Tyler Graf, Assaf Eisenman, Kiran Kumar Matam, Adi Gangidi, Guoqiang Jerry Chen, Manoj Krishnan, Avinash Nayak, Krishnakumar Nair, Bharath Muthiah, Mahmoud khorashadi, Pallab Bhattacharya, Petr Lapukhov, Maxim Naumov, Ajit Mathews, Lin Qiao, Mikhail Smelyanskiy, Bill Jia, Vijay Rao",4/12/21,Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models,https://arxiv.org/abs/2104.05158,3.10E+01,,,1.20E+13,They instantiated a 12T-parameter model to show that their hardware setup can train it despite the huge memory requirements.,,No training details provided.,,No training details provided.,,No training details provided.,,No inference details provided.,,No training details provided.,NVIDIA A100,,,No training details provided.,,,Confident,"Deep learning recommendation models (DLRMs) are used across many business-critical services at Facebook and are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper we discuss the SW/HW co-designed solution for high-performance distributed training of large-scale DLRMs. We introduce a high-performance scalable software stack based on PyTorch and pair it with the new evolution of Zion platform, namely ZionEX. We demonstrate the capability to train very large DLRMs with up to 12 Trillion parameters and show that we can attain 40X speedup in terms of time to solution over previous systems. We achieve this by (i) designing the ZionEX platform with dedicated scale-out network, provisioned with high bandwidth, optimal topology and efficient transport (ii) implementing an optimized PyTorch-based training stack supporting both model and data parallelism (iii) developing sharding algorithms capable of hierarchical partitioning of the embedding tables along row, column dimensions and load balancing them across multiple workers; (iv) adding high-performance core operators while retaining flexibility to support optimizers with fully deterministic updates (v) leveraging reduced precision communications, multi-level memory hierarchy (HBM+DDR+SSD) and pipelining. Furthermore, we develop and briefly comment on distributed data ingestion and other supporting services that are required for the robust and efficient end-to-end training in production environments.",7/28/23 15:09
Megatron-LM (1T),Language,Text autocompletion,"Microsoft Research,NVIDIA,Stanford University",Industry - Academia Collaboration (Industry leaning),"Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, Matei Zaharia",4/9/21,Efficient Large-Scale Language Model Training on GPU Clusters,https://arxiv.org/abs/2104.04473,2.17E+02,Historical significance,Improved SOTA efficiency at distributed training.,1.00E+12,"[NOTE: They didn't train the model fully end-to-end, probably just to obtain enough information to gauge the ability to do model parallelisation]

""Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of 52% of theoretical peak.""",,"NOTE: They didn't train the model fully end-to-end, probably just to obtain enough information to gauge the ability to do model parallelisation.

We calculate below the FLOP required for a full training, but we do not populate it in the Training Compute column.

“For the 1 trillion parameter model, we assume that 450 billion tokens are needed for end-to-end training. With 3072 A100 GPUs, we can achieve a per-GPU throughput of 163 teraFLOP/s, and end-to-end training time of 84 days. We believe these training times (using a reasonable number of GPUs) are practical.”

Table 1 gives a utilisation rate of 52%

Plugging this into the calculator: https://epochai.org/blog/estimating-training-compute
84 days, 3072 GPUs, NVIDIA A100, FP16, 52% utilisation rate --> 3.6e24 FLOP",,Dataset information not provided.,,,,,2016,84 days,NVIDIA A100,Self-supervised learning,,,Yes,Industry,Likely,"Large language models have led to state-of-the-art accuracies across a range of tasks. However, training these models efficiently is challenging for two reasons: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required to train these models can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to fundamental scaling issues at thousands of GPUs, e.g., due to expensive cross-node communication or devices spending significant time waiting on other devices to make progress.
In this paper, we show how different types of parallelism methods (tensor, pipeline, and data parallelism) can be composed to scale to thousands of GPUs and models with trillions of parameters. We survey techniques for pipeline parallelism and propose a novel interleaved pipeline parallelism schedule that can improve throughput by 10+% with memory footprint comparable to existing approaches. We quantitatively study the trade-offs between tensor, pipeline, and data parallelism, and provide intuition as to how to configure distributed training of a large model. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of 52% of theoretical peak.",9/13/23 15:53
GPT-Neo,Language,,EleutherAI,Research collective,,3/21/21,GPT-Neo,https://www.eleuther.ai/projects/gpt-neo/,0.00E+00,,,2.70E+09,"source: https://www.eleuther.ai/projects/gpt-neo/

Note: Directory of LLMs (https://docs.google.com/spreadsheets/d/1gc6yse74XCwBx028HV_cvdxwXkmXejVjkO-Mz2uwE0k/edit#gid=0) gives a somewhat lower estimate (2e9)",7.90E+21,source: https://www.aitracker.org/,The Pile,,8.86E+11,"""In aggregate, the Pile consists of over 825GiB of raw text data""

(see GPT-NeoX)",,,,,,Self-supervised learning,13685.99,,Yes,Industry,,,9/19/23 16:07
Generative BST,Language,,Facebook AI Research,Industry,"Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston",3/5/21,Recipes for building an open-domain chatbot,https://arxiv.org/abs/2004.13637,5.03E+02,SOTA Improvement,"Abstract:
""Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.""",9.40E+09,"Abstract:
""We build variants of these recipes with 90M, 2.7B and 9.4B parameter models""",,"Unclear - no mention of GPUs used, or training time, and the architecture is terribly complicated",,"Section 6:
Pushshfit.io Reddit, ConvAI 2, Wizard of Wikipedia",,,,,,,,,,,,Industry,,,6/8/23 0:39
M6-T,Multimodal,,Alibaba Group,Industry,"An Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jiamang Wang, Yong Li, Di Zhang, Wei Lin, Lin Qu, Jingren Zhou, Hongxia Yang",3/5/21,M6-T: Exploring Sparse Expert Models and Beyond,https://arxiv.org/abs/2105.15082,7.60E+01,SOTA Improvement,"Improves on hardware SOTA for similar problems

Abstract: 
""We push the model
scale to over 1 trillion parameters and implement it on solely 480 NVIDIA V100-32GB GPUs, in comparison with the recent SOTAs [11; 6] on 2048 TPU cores.""",1.00E+12,"Section 4, pg 8:
""Due to limited computational resources, we attempt to figure out solutions to implement a 1-trillion-parameter model on solely 480 NVIDIA V100-32GB GPUs.""",5.50E+21,Estimate taken from https://www.governance.ai/research-paper/recent-trends-chinas-llm-landscape,M6-Corpus,,1.90E+12,Images,,,,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,,,Yes,Industry,Likely,"Mixture-of-Experts (MoE) models can achieve promising results with outrageous large amount of parameters but constant computation cost, and thus it has become a trend in model scaling. Still it is a mystery how MoE layers bring quality gains by leveraging the parameters with sparse activation. In this work, we investigate several key factors in sparse expert models. We observe that load imbalance may not be a significant problem affecting model quality, contrary to the perspectives of recent studies, while the number of sparsely activated experts k and expert capacity C in top-k routing can significantly make a difference in this context. Furthermore, we take a step forward to propose a simple method called expert prototyping that splits experts into different prototypes and applies k top-1 routing. This strategy improves the model quality but maintains constant computational costs, and our further exploration on extremely large-scale models reflects that it is more effective in training larger models. We push the model scale to over 1 trillion parameters and implement it on solely 480 NVIDIA V100-32GB GPUs, in comparison with the recent SOTAs on 2048 TPU cores. The proposed giant model achieves substantial speedup in convergence over the same-size baseline.",8/15/23 15:16
M6-100B,Multimodal,,"Tsinghua University, Alibaba Group",Industry - Academia Collaboration (Industry leaning),"J Lin, R Men, A Yang, C Zhou, M Ding, Y Zhang",3/1/21,M6: A Chinese Multimodal Pretrainer,https://arxiv.org/abs/2103.00823,7.60E+01,,,1.00E+11,"""We scale the
model size up to 10 billion and 100 billion parameters, and build
the largest pretrained model in Chinese.""",,,,,1.90E+12,"""1.9TB images and 292GB texts""

TODO: figure out what to do for multimodal pretraining datasets",,,,,,,,,,Industry,,,5/29/23 20:51
M6-10B,Multimodal,,"Tsinghua University, Alibaba Group",Industry - Academia Collaboration (Industry leaning),"J Lin, R Men, A Yang, C Zhou, M Ding, Y Zhang",3/1/21,M6: A Chinese Multimodal Pretrainer,https://arxiv.org/abs/2103.00823,7.60E+01,,,1.00E+10,"""We scale the
model size up to 10 billion and 100 billion parameters, and build
the largest pretrained model in Chinese.""",,"""We implement M6-100B with around 100 billion parameters
on 128 Nvidia A100s and the speed of pretraining achieves 1440
samples/s (for samples of the sequence length of 272).""

Their response to our email doesn't say enough to tell us what the compute is for this paper, but allows us to determine the compute for the follow-up paper with the M6-10T model (but we knew this already)",,,1.90E+12,"""1.9TB images and 292GB texts""

TODO: figure out what to do for multimodal pretraining datasets",,,,,,,,,,Industry,,,5/29/23 20:51
Meta Pseudo Labels,Vision,Image Classification,Google Brain,Industry,"Hieu Pham, Zihang Dai, Qizhe Xie, Minh-Thang Luong, and Quoc V. Le",3/1/21,Meta pseudo labels,https://arxiv.org/abs/2003.10580,3.93E+02,SOTA Improvement,,4.80E+08,"Table 4
 480M",4.79E+22,"From communication with author:

22671 TPU days on specific hardware.

Which hardware did you use and in which configuration?
2048 cores of TPU v3.

Precision: Mixed. bfloat16 for activations, float32 for weights and optimizer slots.

2048 TPUv3 cores means 1024 TPUv3 chips, and the spec is 123e12 FLOP/second per chip with bfloat16 precision (Source: https://cloud.google.com/tpu/docs/system-architecture-tpu-vm)

So the compute estimate is:
1024 chips * 123e12 FLOP/second * 0.4 utilization * 11 days * 24 * 60 * 60 = 4.788191232e+22 FLOP",ImageNet,,1.30E+08,"Section 4
Datasets. For this experiment, we use the entire ImageNet
training set as labeled data, and use the JFT dataset as unlabeled data. The JFT dataset has 300 million images, and
then is filtered down to 130 million images by Noisy Student
using confidence thresholds and up-sampling [77]. We use
the same 130 million images as Noisy Student",,,,,,Self-supervised learning,369462.82,,Yes,Industry,,"We present Meta Pseudo Labels, a semi-supervised learning method that achieves a new state-of-the-art top-1 accuracy of 90.2% on ImageNet, which is 1.6% better than the existing state-of-the-art. Like Pseudo Labels, Meta Pseudo Labels has a teacher network to generate pseudo labels on unlabeled data to teach a student network. However, unlike Pseudo Labels where the teacher is fixed, the teacher in Meta Pseudo Labels is constantly adapted by the feedback of the student's performance on the labeled dataset. As a result, the teacher generates better pseudo labels to teach the student. Our code will be available at this https URL.",8/11/23 17:27
Wu Dao - Wen Hui,Multimodal,,BAAI,Non-profit,,3/1/21,China's GPT-3? BAAI Introduces Superscale Intelligence Model 'Wu Dao 1.0',https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70,0.00E+00,,,1.13E+10,"""Wu Dao — Wen Hui has reached 11.3 billion parameters, and through simple fine-tuning can generate poetry, make videos, draw pictures, retrieve text, perform complex reasoning, etc.""

https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70",1.16E+20,"64 Nvidia V100 GPUs for 2.5 days

64 GPUs * 2.8e13 FLOP/s /GPU * 2.5*24*60*60s* 0.3 [utilization rate]

",,,,,,,,,,Self-supervised learning,,,Yes,Industry,,,9/19/23 16:07
Wu Dao - Wen Lan,Multimodal,,BAAI,Non-profit,,3/1/21,China's GPT-3? BAAI Introduces Superscale Intelligence Model 'Wu Dao 1.0',https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70,0.00E+00,,,1.00E+09,"""Currently, the model has 1 billion parameters and is trained on 50 million graphic pairs collected from open sources.""

https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70",7.20E+21,"128 Nvidia A100 GPUs for 7 days

128 GPUs * 3.1e14 FLOP/s /GPU * 7*24*60*60s* 0.3 [utilization rate]

",,,,,,,,,,Self-supervised learning,,,Yes,Industry,,,9/19/23 16:07
Wu Dao - Wen Su,Other,Proteins,BAAI,Non-profit,,3/1/21,China's GPT-3? BAAI Introduces Superscale Intelligence Model 'Wu Dao 1.0',https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70,0.00E+00,,,,,,,,,,,,,,,,Self-supervised learning,,,Yes,Industry,,,9/19/23 16:07
Rational DQN Average,Games,Atari Games,TU Darmstadt,Academia,"Q Delfosse, P Schramowski, A Molina",2/18/21,Recurrent Rational Networks,https://openreview.net/forum?id=gnRmI8TatHV,3.00E+00,SOTA improvement,,1.68E+06,See figure 7,,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
Switch,Language,Text autocompletion,Google,Industry,"William Fedus, Barret Zoph, Noam Shazeer",1/11/21,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,https://arxiv.org/abs/2101.03961,5.11E+02,,,1.60E+12,"Combining expert, model and data parallelism, we design two large Switch Transformer models, one
with 395 billion and 1.6 trillion parameters",8.22E+22,"Table 4
https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf",,,4.32E+11,"""In our protocol we pre-train with 220 (1,048,576) tokens
per batch for 550k steps amounting to 576B total tokens.""

1 token ~ 0.75 words",,,,,,Self-supervised learning,149825.6,,Yes,Industry,,"In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the ""Colossal Clean Crawled Corpus"" and achieve a 4x speedup over the T5-XXL model.
",8/10/23 15:22
Wu Dao - Wen Yuan,Language,,BAAI,Industry - Academia Collaboration (Academia Leaning),,1/11/21,"Tencent: Facing cognition, Zhiyuan Research Institute and several units released a super-large-scale new pre-training model ""Enlightenment·Wenhui""",https://web.archive.org/web/20230409001523/https://mp.weixin.qq.com/s/BUQWZ5EdR19i40GuFofpBg,0.00E+00,,,2.60E+09,"""It has 2.6 billion parameters and is capable of performing cognitive activities such as memorization, comprehension, retrieval, numerical calculation, multi-language, etc.""

https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70",6.50E+20,"64 Nvidia V100 GPUs for two weeks

64 GPUs * 2.8e13 FLOP/s /GPU * 14*24*60*60s * 0.3 [utilization rate]

",,,,,,,,,,Self-supervised learning,,,Yes,Industry,,,9/19/23 16:07
BigSSL,Speech,Audio speech recognition,"Google, Apple",Industry,"Yu Zhang,  Daniel S. Park, Wei Han,James Qin, Anmol Gulati, Joel Shor, Aren Jansen, Yuanzhong Xu, Yanping Huang, Shibo Wang, Zongwei Zhou, Bo Li, Min Ma, William Chan, Jiahui Yu, Yongqiang Wang, Liangliang Cao, Khe Chai Sim, Bhuvana Ramabhadran, Tara N. Sainath, Françoise Beaufays, Zhifeng Chen, Quoc V. Le, Chung-Cheng Chiu, Ruoming Pang and Yonghui Wu",1/10/21,BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition,https://arxiv.org/abs/2109.13226,4.10E+01,,,8.00E+09,"""... we study the utility of large models, with the parameter count ranging from 600M to 8B...""",,,,,4.26E+10,"Sum all values in Table VII, and add 34k for English VAD, and 926k for English Youtube = 3116k hours

Note this involves significant self-training: ""Noisy student training (NST) [23], [41] is a self-training
method where a teacher model generates pseudo-labels for a
large unlabeled dataset, which is in turn used to train a student
model with augmentation.""

1 hour ~ 13,680 words
13680 * 3116000 = 42626880000",,,,,,,,,,Industry,,,5/29/23 20:51
CLIP (ResNet-50),Multimodal,Zero-shot image classification,Open AI,Industry,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever",1/5/21,Learning Transferable Visual Models From Natural Language Supervision,https://arxiv.org/abs/2103.00020,2.92E+03,SOTA Improvement,,8.86E+07,"Image encoder
~ResNet-50 (from paper)
25.6M params

Text encoder
~Transformer (from paper)
63M params",,,,Custom image-text pairs from the internet,4.00E+08,,7.00E+06,Figure 10 https://arxiv.org/pdf/2103.00020.pdf,,,,,,,,Industry,,,9/21/23 3:11
CLIP (ViT L/14@336px),Multimodal,Zero-shot image classification,Open AI,Industry,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever",1/5/21,Learning Transferable Visual Models From Natural Language Supervision,https://arxiv.org/abs/2103.00020,2.92E+03,SOTA Improvement,,3.70E+08,"Image encoder
Vision Transformer
Table 1 in https://arxiv.org/pdf/2010.11929.pdf
Authors fine-tuned ViT L/14 at additional 336px resolution, hence the @336 (See ViT)
307M params

Text encoder
~Transformer (from paper)
63M params",1.05E+22,https://docs.google.com/document/d/156miAJkFN9DDX06C3s03UDsretCtymCKiGDddLBCgQE/edit?usp=sharing,,Custom image-text pairs from the internet,4.00E+08,,1.10E+08,Figure 10 https://arxiv.org/pdf/2103.00020.pdf,,"""In the end, our best performing CLIP model trains on 256 GPUs for 2 weeks which is similar to existing large scale image models.""",,Self-supervised learning,40146.99,"https://www.kdnuggets.com/2021/03/beginners-guide-clip-model.html
",Yes,Industry,,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",9/21/23 3:11
DALL-E,Drawing,Text-to-image,OpenAI,Industry,"Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever",1/5/21,Zero-Shot Text-to-Image Generation,https://openai.com/blog/dall-e/,9.82E+02,Significant use,,1.20E+10,DALL·E is a 12-billion parameter version of GPT-3 trained to generate images from text descriptions,4.70E+22,source: https://lair.lighton.ai/akronomicon/,,,2.50E+08,"""To scale up to 12-billion parameters, we created a dataset of a similar scale to JFT-300M (Sun et al., 2017) by collecting
250 million text-images pairs from the internet. """,,,,,,,171537.13,,Yes,Industry,,"Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",6/8/23 0:39
AraGPT2-Mega,Language,,American University of Beirut,Academia,"W Antoun, F Baly, H Hajj",12/31/20,AraGPT2: Pre-Trained Transformer for Arabic Language Generation,https://arxiv.org/abs/2012.15520,2.40E+01,,,1.50E+09,source: https://lair.lighton.ai/akronomicon/,2.00E+21,"source: https://github.com/lightonai/akronomicon/blob/10adaca9c74afa7d11f196947e410d248f25abe9/akrodb/American%20University%20of%20Beirut/AraGPT2-Mega.json

Akronomicon uses units of petaflop/s-days. 20 petaflop/s-days ~= 2e21 FLOP.

Our own validation of this estimate is below.

For the Mega model: 9 days on a TPUv3-128, bfloat16 precision  (from author communication)

A TPUv3-128 has 128 cores (you can infer this from footnote 9 on p.4 of the paper - 128 * 16GB = 2TB). TPUv3 has 2 cores per chip. So 64 chips.

TPUv3 FLOP/s: 1.23E+14

Utilization: use default value of 30% for Language domain (https://epochai.org/blog/estimating-training-compute)

64 chips * 30% * 1.23E+14 FLOP/s * 9 days * 24h/day * 3600s/h
~= 2e21 FLOP",,,8.80E+09,"""The total dataset size is 77GB with 8.8B words [word count was done after preprocessing, where a white
space is inserted before and after punctuations, brackets, numbers... which increased the total word count]""",,,,,,,3685.43,,Yes,Academia,,"Recently, pre-trained transformer-based architectures have proven to be very efficient at language modeling and understanding, given that they are trained on a large enough corpus. Applications in language generation for Arabic are still lagging in comparison to other NLP advances primarily due to the lack of advanced Arabic language generation models. In this paper, we develop the first advanced Arabic language generation model, AraGPT2, trained from scratch on a large Arabic corpus of internet text and news articles. Our largest model, AraGPT2-mega, has 1.46 billion parameters, which makes it the largest Arabic language model available. The Mega model was evaluated and showed success on different tasks including synthetic news generation, and zero-shot question answering. For text generation, our best model achieves a perplexity of 29.8 on held-out Wikipedia articles. A study conducted with human evaluators showed the significant success of AraGPT2-mega in generating news articles that are difficult to distinguish from articles written by humans. We thus develop and release an automatic discriminator model with a 98% percent accuracy in detecting model-generated text. The models are also publicly available, hoping to encourage new research directions and applications for Arabic NLP.",5/29/23 20:51
VQGAN + CLIP,Drawing,Text-to-image,Heidelberg University,Academia,"Patrick Esser, Robin Rombach, Björn Ommer",12/17/20,Taming Transformers for High-Resolution Image Synthesis,https://arxiv.org/abs/2012.09841,4.94E+02,SOTA Improvement,,,,,,,,,I'm confused - I guess they pretrained on several different datasets? I think the model is also able to do zero-shot learning,,,,,,,,,,Academia,,,9/20/23 21:26
CPM-Large,Language,,"Tsinghua University, BAAI",Academia,"Z Zhang, X Han, H Zhou, P Ke, Y Gu, D Ye, Y Qin, Y Su",12/1/20,CPM: A Large-scale Generative Chinese Pre-trained Language Model,https://arxiv.org/abs/2012.00413,4.90E+01,,,2.60E+09,"""To the best of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained language mode""",1.80E+21,source: https://lair.lighton.ai/akronomicon/,,,1.67E+10,"""language model, with 2.6 billion parameters and 100GB Chinese training data.""

We use the conversion factor 1GB ~ 167M words",,,,,,,6569.51,"https://towardsdatascience.com/the-future-of-ai-is-decentralized-848d4931a29a#:~:text=Training%20GPT%2D3%20reportedly%20cost,a%20single%20training%20run%C2%B9.",Yes,Academia,,"Pre-trained Language Models (PLMs) have proven to be beneficial for various downstream NLP tasks. Recently, GPT-3, with 175 billion parameters and 570GB training data, drew a lot of attention due to the capacity of few-shot (even zero-shot) learning. However, applying GPT-3 to address Chinese NLP tasks is still challenging, as the training corpus of GPT-3 is primarily English, and the parameters are not publicly available. In this technical report, we release the Chinese Pre-trained Language Model (CPM) with generative pre-training on large-scale Chinese training data. To the best of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained language model, which could facilitate several downstream Chinese NLP tasks, such as conversation, essay generation, cloze test, and language understanding. Extensive experiments demonstrate that CPM achieves strong performance on many NLP tasks in the settings of few-shot (even zero-shot) learning. The code and parameters are available at this https URL.",5/29/23 20:51
AlphaFold2,Other,Protein folding prediction,DeepMind,Industry,"John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Kathryn Tunyasuvunakool, Olaf Ronneberger, Russ Bates, Augustin Žídek, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Anna Potapenko, Andrew J Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Martin Steinegger, Michalina Pacholska, David Silver, Oriol Vinyals, Andrew W Senior, Koray Kavukcuoglu, Pushmeet Kohli, Demis Hassabis.",11/30/20,High Accuracy Protein Structure Prediction Using Deep Learning,https://www.nature.com/articles/s41586-021-03819-2,2.40E+01,"Important context, significant use",,,,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
KEPLER,Language,Relation Extraction,"Tsinghua University,Princeton,Mila- Quebec AI,University de Montreal,HEC,CIFAR",Academia,"Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan Liu, Juanzi Li, and Jian Tang.",11/23/20,KEPLER: A Unified Model for Knowledge Embedding and Pre- trained Language Representation.,https://arxiv.org/abs/1911.06136,2.56E+02,,,1.10E+08,,1.24E+20,"From author communication

""About 128 GPU-days using Nvidia V100 (16GB). ""

precision: float16

V100 GPU for float16: 28000000000000 (2.8E+13)

0.4 * 28TFLOP/s * 128 GPU-days * 24h/day * 3600s/h
= 1.24E+20


","Wikipedia, BookCorpus","From author communication

    For the language modeling objective, we use Wikipedia+BookCorpus datasets (about 13GB).    For the knowledge embedding objective, we use Wikidata5m (about 1GB).",3.30E+09,"For BookCorpus + English Wikipedia: 800M + 2500M

For Wikidata5M: 20614279
See table 1. Contains ""entities"", ""relations"", and ""triplets""",,"From author communication

It depends on the length of the input sequences. The inference computation of KEPLER is the same as RoBERTa (base) and you may estimate it with this.",,,,Self-supervised learning,437.97,,Yes,Academia,,"Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagE Representation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we encode textual entity descriptions with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KEPLER achieves state-of-the-art performances on various NLP tasks, and also works remarkably well as an inductive KE model on KG link prediction. Furthermore, for pre-training and evaluating KEPLER, we construct Wikidata5M, a large-scale KG dataset with aligned entity descriptions, and benchmark state-of-the-art KE methods on it. It shall serve as a new KE benchmark and facilitate the research on large KG, inductive KE, and KG with text. The source code can be obtained from this https URL.",9/21/23 3:11
SimCLRv2,,,Google Brain,Industry,"Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton",10/26/20,Big self- supervised models are strong semi-supervised learners.,https://arxiv.org/abs/2006.10029,4.57E+02,,,7.95E+08,"From author communication

We trained different model sizes (from 24M to 795M), and they're summarized in Table 1 of the paper (https://arxiv.org/pdf/2006.10029.pdf).",,,,,1.28E+06,"[Double check this, uncertain if this is right]

""Following the semi-supervised learning setting in [30, 19, 1], we evaluate the proposed method
on ImageNet ILSVRC-2012 [21]. While all ∼1.28 million images are available, only a randomly
sub-sampled 1% (12811) or 10% (128116) of images are associated with labels""",,,,,,,,,,Industry,,,8/11/23 17:29
ViT-Base/32,Vision,Image representation,Google Brain,Industry,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby",10/22/20,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://arxiv.org/abs/2010.11929,7.29E+02,Highly cited,,8.60E+07,Table 1 https://arxiv.org/pdf/2010.11929.pdf,,,,,,,,,,,,,,,,Industry,,,9/20/23 21:27
ViT-Huge/14,Vision,Image representation,Google Brain,Industry,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby",10/22/20,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://arxiv.org/abs/2010.11929,7.29E+02,Highly cited,,6.32E+08,Table 1 https://arxiv.org/pdf/2010.11929.pdf,,,,,,,,,,,,,,,,Industry,,,9/20/23 21:27
wave2vec 2.0 LARGE,Speech,Speech completion,Facebook,Industry,"Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli",10/22/20,wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations,https://arxiv.org/abs/2006.11477,4.10E+02,SOTA Improvement,"Arguably an ""important"" paper? 

Abstract: 
""We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler.""",3.17E+08,"Section 5.1:
""We consider two model sizes: BASE (95m parameters) and LARGE (317m parameters)
",1.90E+21,"From surveying the authors:

We trained the base model on 64 V100 GPUs for 400k updates. This takes about 3 days to complete. The large model is trained on 128 V100 GPUs for 1 million updates, and this takes about 7 days to complete.

V100 GPU peak: 125TFLOP/s (https://www.nvidia.com/en-gb/data-center/tesla-v100/)
Assume 40% utilization based on default for non-Language domain (https://epochai.org/blog/estimating-training-compute)

64 GPUs * 40% * 125TFLOP/s * 7 days * 24h/day * 3600s/h
~= 1.9E+21 FLOP",LibriSpeech,,7.28E+08,"pg 4, section 4.1

""As unlabeled data we consider the Librispeech corpus [40] without transcriptions containing 960 hours of audio (LS-960) or the audio data from LibriVox (LV-60k). For the latter we follow the preprocessing of [27] resulting in 53.2k hours of audio.""

53.2k h * 13,680 words/h = 727776000 words",,,,,NVIDIA Tesla V100 DGXS 32 GB,,1569.38,,Yes,Industry,,"We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.",6/7/23 18:29
ViT-H/14,Vision,Image representation,Google Brain,Industry,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby",9/28/20,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://openreview.net/forum?id=YicbFdNTTy,1.91E+03,Highly cited,,,,1.28E+22,"Table 5
They also report TPUv3 days, which aligns with the number on table 5
(From Imagenet paper-data, Besiroglu et al., forthcoming) ",Imagenet-1k,,1.28E+06,,,,,,,,25757.45,,,Industry,,"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",8/11/23 18:00
ERNIE-GEN (large),Language,Language Generation,Baidu,Industry,"Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",8/6/20,ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation,https://arxiv.org/abs/2001.11314,9.20E+01,,,3.40E+08,"We train a base model ERNIEGENBASE (L=12, H=768, A=12, Total Parameters=110M)1
and a large model ERNIE-GENLARGE (L=24, H=1024,
A=16, Total Parameters=340M) with parameters initialized
by BERTBASE and BERTLARGE respectively",,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
EfficientDet,Vision,Object detection,Google Brain,Industry,"Mingxing Tan, Ruoming Pang, Quoc V. Le",7/27/20,EfficientDet: Scalable and Efficient Object Detection,https://openaccess.thecvf.com/content_CVPR_2020/html/Tan_EfficientDet_Scalable_and_Efficient_Object_Detection_CVPR_2020_paper.html,7.06E+02,,,5.20E+07,"""In particular, with single-model and single-scale, our EfficientDetD7 achieves state-of-the-art 52.2 AP on COCO test-dev with 52M parameters and 325B FLOPs""",,,,,,,3.25E+11,"""In particular, with single-model and single-scale, our EfficientDetD7 achieves state-of-the-art 52.2 AP on COCO test-dev with 52M parameters and 325B FLOPs""",,,,,,,,Industry,,,8/11/23 17:59
Hopfield Networks (2020),Other,,"Johannes Kepler University Linz,Institute of Advanced Research in Artificial Intelligence,University of Oslo",Academia,"Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlović, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, Günter Klambauer, Johannes Brandstetter, Sepp Hochreiter",7/16/20,Hopfield Networks is All You Need,https://arxiv.org/abs/2008.02217,1.82E+02,,,,,,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
DLRM-2021,Recommendation,,Facebook AI ,Industry,"D Mudigere, Y Hao, J Huang, A Tulloch",7/1/20,"High- performance, Distributed Training of Large scale Deep Learning Recommendation Models",https://www.arxiv-vanity.com/papers/2104.05158/,1.00E+01,,,1.00E+12,"Figure 1

https://arxiv.org/abs/2104.05158",3.00E+20,"Figure 1

https://arxiv.org/abs/2104.05158",,,,,,,,,,,1094.92,"https://bdtechtalks.com/2020/02/03/google-meena-chatbot-ai-language-model/
",,Industry,,"Deep learning recommendation models (DLRMs) are used across many business-critical services at Facebookand are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper we discuss the SW/HW co-designed solution for high-performance distributed training of large-scale DLRMs. We introduce a high-performance scalable software stack based on PyTorch and pair it with the new evolution of Zion platform, namely ZionEX. We demonstrate the capability to train very large DLRMs with up to 12 Trillion parameters and show that we can attain 40 × speedup in terms of time to solution over previous systems. We achieve this by (i) designing the ZionEX platform with dedicated scale-out network, provisioned with high bandwidth, optimal topology and efficient transport (ii) implementing an optimized PyTorch-based training stack supporting both model and data parallelism (iii) developing sharding algorithms capable of hierarchical partitioning of the embedding tables along row, column dimensions and load balancing them across multiple workers; (iv) adding high-performance core operators while retaining flexibility to support optimizers with fully deterministic updates (v) leveraging reduced precision communications, multi-level memory hierarchy (HBM+DDR+SSD) and pipelining. Furthermore, we develop and briefly comment on distributed data ingestion and other supporting services that are required for the robust and efficient end-to-end training in production environments.",5/29/23 20:51
GShard (600B),Language,Translation,Google,Industry,"Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen",6/30/20,GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,https://arxiv.org/abs/2006.16668,2.95E+02,,,6.00E+11,"""The 600B parameters model that achieved the best translation quality was trained with 2048 TPU v3 cores for 4 days, a total cost of 22 TPU v3 core-years.""",1.33E+22,"https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
Table 4",,,2.60E+11,"""We focus on improving the translation quality (measured in terms of BLEU score [48]) from all 100 languages to English. This resulted in approximately 13 billion training examples to be used for model training""

Each example is a sentence pair. Assuming 20 words per sentence, that is 13*20 billion words.",,,,,,Self-supervised learning,27609.81,,Yes,Industry,,"Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.",8/10/23 15:22
GShard (dense),Language,Translation,Google,Industry,"Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen",6/30/20,GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,https://arxiv.org/abs/2006.16668,2.95E+02,,,2.30E+09,"""Our best quality dense single Transformer model (2.3B parameters) achieving ∆BLEU
of 6.1, was trained with GPipe [15] on 2048 TPU v3 cores for 6 weeks or total of 235.5 TPU v3
core-years.""",2.60E+22,"Estimated in the blogpost below

https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening",,,2.60E+11,"""We focus on improving the translation quality (measured in terms of BLEU score [48]) from all 100 languages to English. This resulted in approximately 13 billion training examples to be used for model training""

Each example is a sentence pair. Assuming 20 words per sentence, that is 13*20 billion words.",,,,,,Self-supervised learning,55219.61,,Yes,Industry,,"Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.",8/10/23 15:22
iGPT-L,Drawing,Image completion,Open AI,Industry,"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",6/17/20,Generative Pretraining from Pixels,https://openai.com/blog/image-gpt/,6.89E+02,,,1.36E+09,source: https://openai.com/blog/image-gpt/#rfref53,8.91E+21,"We have that ""iGPT-L was trained for roughly 2500 V100-days"" [1]

I assume this is the NVIDIA Tesla V100 GPU. In the specifications, the NVIDIA Tesla V100 has 7 to 8.2 TFLOPS of peak double precision performance and 14 to 16.4 TFLOPS of peak single precision performance and 112 to 130 TFLOPS of peak tensor performance [2].

I suppose the one that makes sense using if peak tensor performance, for ~125 TFLOPS peak tensor performance more or less.
Following OpenAIs AI and compute we apply a 0.33 utitilization factor [3].

In total we get 2500 V100-days * (24*60*60) seconds/day * 125 TFLOPS * 0.33 = 8.91e+21 FLOPS = 89.1 PF-days.

[1] https://openai.com/blog/image-gpt/
[2] https://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdf
[3] https://openai.com/blog/ai-and-compute/",ILSVRC 2012,,9.60E+06,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""

https://image-net.org/challenges/LSVRC/2012/

""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""
",,,,,NVIDIA Tesla V100 DGXS 32 GB,,32482.56,,Yes,Industry,,"Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",6/7/23 18:18
iGPT-XL,Drawing,Image completion,Open AI,Industry,"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",6/17/20,Generative Pretraining from Pixels,https://openai.com/blog/image-gpt/,6.89E+02,,,6.80E+09,source: https://openai.com/blog/image-gpt/#rfref53,3.30E+22,"Taken from here
https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening",ILSVRC 2012,,9.60E+06,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""

https://image-net.org/challenges/LSVRC/2012/

""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""
",,,,,NVIDIA Tesla V100 DGXS 32 GB,,120440.96,,Yes,Industry,,"Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",6/7/23 18:18
SqueezeBERT,Language,Text autocompletion,Berkeley,Academia,"Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer",6/10/20,SqueezeBERT: What can computer vision teach NLP about efficient neural networks?,https://arxiv.org/abs/2006.11316,6.20E+01,,,5.11E+07,Rados,,,,,,,7.42E+09,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,,Academia,,,5/29/23 20:51
GPT-3 175B (davinci),Language,Text autocompletion,OpenAI,Industry,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",5/28/20,Language models are Few-Shot Learners,https://arxiv.org/abs/2005.14165,1.53E+03,Highly cited,,1.75E+11,"we train GPT-3, an autoregressive language model with 175 billion parameters",3.14E+23,"Table D.1
https://arxiv.org/abs/2005.14165",CommonCrawl; WebText2; Books1; Books2; Wikipedia,Table 2.2 (other datasets also used),3.74E+11,"From table 2.2, we determine that there are 410 + 19 + 12 + 55 + 3 = 499 billion tokens. 

We multiply this by 0.75 to give 374B words. 

3.74e11

========================
[Anson: I think the calculation below doesn't look at all the data, the CommonCrawl data only constitutes 60% of the data. Multiplying by 5/3 gives 4.75e11]

""The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens. ""

Converted to words using 
http://extraconversion.com/data-storage/gigabits/gigabits-to-words.html

2.85e11",7.40E+14,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,1131415.12,,Yes,Industry,,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",9/14/23 20:38
Once for All,Vision,,"MIT-IBM Watson AI Lab,Massachusetts Institute of Technology",Industry,"Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han",4/29/20,Once for all: Train one network and specialize it for efficient deployment.,https://arxiv.org/abs/1908.09791,7.33E+02,,,7.70E+06,,1.78E+21,"4.2k V100-hours (table 1)
0.33 utilization rate
",ImageNet,,,,,,,,,,6569.51,,,Industry,,"We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices. Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing CO2 emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks (>1019) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and CO2 emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting (<600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and 50 pre-trained models (for many devices & many latency constraints) are released at this https URL.",9/12/23 22:28
#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!
Go-explore,Games,Atari,"Uber AI, OpenAI",Industry,"A Ecoffet, J Huizinga, J Lehman, KO Stanley, J Clune",4/27/20,"First return, then explore",https://arxiv.org/abs/2004.12919,1.79E+02,,,,,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
CURL,Games,Atari Games,UC Berkeley,Academia,"A Srinivas, M Laskin, P Abbeel",4/8/20,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,https://arxiv.org/abs/2004.04136v4,2.90E+02,SOTA improvement,,9.07E+05,,,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
MobileBERT,Language,Text autocompletion,"Carnegie Mellon University,Google Brain",Industry - Academia Collaboration (Industry leaning),"Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, Denny Zhou",4/6/20,MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices,https://arxiv.org/abs/2004.02984,3.92E+02,,,2.53E+07,Rados,,,,,,,5.36E+09,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,,Industry,,,8/11/23 16:55
Agent57,Games,Atari,DeepMind,Industry,"AP Badia, B Piot, S Kapturowski",3/30/20,Agent57: Outperforming the Atari Human Benchmark,https://arxiv.org/abs/2003.13350,3.45E+02,,,,,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
MetNet,Other,Weather prediction,Google,Industry,"Casper Kaae Sønderby, Lasse Espeholt, Jonathan Heek, Mostafa Dehghani, Avital Oliver, Tim Salimans, Shreya Agrawal, Jason Hickey, Nal Kalchbrenner",3/24/20,MetNet: A Neural Weather Model for Precipitation Forecasting,https://arxiv.org/abs/2003.12140,1.35E+02,,,,,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
ELECTRA,Language,Text autocompletion,"Stanford University,Google",Industry - Academia Collaboration,"Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning",3/23/20,ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators,https://arxiv.org/abs/2003.10555v1,2.90E+03,Highly cited,,3.35E+08,Rados,2.00E+20,From Table 1: 4d on 16 TPUv3s,,,,,7.90E+10,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,Self-supervised learning,,,Yes,Industry,,"Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.",9/19/23 16:10
ProGen,Other,Protein generation,"Salesforce research,Stanford University",Industry - Academia Collaboration,"Ali Madani, Bryan McCann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael R. Eguchi,  View ORCID ProfilePo-Ssu Huang, Richard Socher",3/13/20,ProGen: Language Modeling for Protein Generation,https://www.biorxiv.org/content/10.1101/2020.03.07.982272v2,1.31E+02,,,1.20E+09,"""We train a 1.2B-parameter language model, ProGen, on ∼280M protein sequences""",3.70E+20,"Our model was implemented in TensorFlow (Abadi et al.,
2016) and trained with a global batch size of 64 distributed
across 256 cores of a Cloud TPU v3 Pod for 1M iterations. Training took approximately two weeks using Adagrad (Duchi et al., 2011)

4.00E+12*256*60**2*24*14*0.3 = 3.7e20",,,,,,,,,,Self-supervised learning,623.75,,Yes,Industry,,"Generative modeling for protein engineering is key to solving fundamental problems in synthetic biology, medicine, and material science. We pose protein engineering as an unsupervised sequence generation problem in order to leverage the exponentially growing set of proteins that lack costly, structural annotations. We train a 1.2B-parameter language model, ProGen, on ∼280M protein sequences conditioned on taxonomic and keyword tags such as molecular function and cellular component. This provides ProGen with an unprecedented range of evolutionary sequence diversity and allows it to generate with fine-grained control as demonstrated by metrics based on primary sequence similarity, secondary structure accuracy, and conformational energy.",9/6/23 18:13
SimCLR,Drawing,Image completion,Google Brain,Industry,"Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton",2/13/20,A Simple Framework for Contrastive Learning of Visual Representations,https://arxiv.org/abs/2002.05709,2.16E+03,Highly cited,,3.75E+08,source: https://openai.com/blog/image-gpt/,,,,,,,,,,,Google TPU V3,,,,,Industry,,,8/11/23 17:58
Turing NLG,Language,Text autocompletion,Microsoft,Industry,C Rosset,2/13/20,Turing-NLG: A 17-billion-parameter language model by Microsoft,https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/,1.14E+02,,,1.70E+10,source: https://lair.lighton.ai/akronomicon/,1.57E+22,source: https://lair.lighton.ai/akronomicon/,,,3.48E+10,"Authors say they pretrain on the same data as for Megatron-LM. 

From Megatron-LM paper: https://arxiv.org/pdf/1909.08053.pdf

""The resulting aggregate
corpus contains 174 GB of deduplicated text.""

174GB * 2e8words/GB = 3.48e10 words",3.60E+13,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,58395.62,,Yes,Industry,,,5/29/23 20:51
ALBERT-xxlarge,Language,,"Toyota Technological Institute at Chicago,Google",Industry - Academia Collaboration,"Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut",2/9/20,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.,https://arxiv.org/abs/1909.11942,2.18E+03,Highly cited,,2.35E+08,,2.39E+21,"32 hours of training
512 TPU V3s
0.33 utilization rate
",,,3.30E+09,"Pretraining same as for BERT - Wikipedia and BookCorpus

""For the pre-training corpus we
use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)""",2.50E+12,Source: https://github.com/amirgholami/ai_and_memory_wall,,,,Self-supervised learning,5924.43,,Yes,Industry,,"Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL.",8/10/23 15:26
Perceiver IO,Multimodal,,DeepMind,Industry,"Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hénaff,
Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, João Carreira",2/8/20,Perceiver IO: A General Architecture for Structured Inputs & Outputs,https://arxiv.org/abs/2107.14795,1.43E+02,,,,,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
Theseus 6/768,Language,Text autocompletion,"UC San Diego,Beihang University,Microsoft",Industry - Academia Collaboration,"Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou",2/7/20,BERT-of-Theseus: Compressing BERT by Progressive Module Replacing,https://arxiv.org/abs/2002.02925,1.23E+02,,,6.60E+07,"Rados, also specified in Table 1 in the paper",,,,,,,1.13E+10,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,,Industry,,,6/8/23 0:39
Meena,Language,Text autocompletion,Google Brain,Industry,"Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",1/28/20,Towards a Human-like Open-Domain Chatbot,https://arxiv.org/abs/2001.09977,6.15E+02,,,,,,,,,,,,,,,,Self-supervised learning,263099.94,,Yes,Industry,,"We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated.",8/11/23 17:40
AlphaFold,Other,Protein folding prediction,DeepMind,Industry,"Andrew W. Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Žídek, Alexander W. R. Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David T. Jones, David Silver, Koray Kavukcuoglu, Demis Hassabis",1/15/20,Improved protein structure prediction using potentials from deep learning,https://www.nature.com/articles/s41586-019-1923-7,8.40E+02,SOTA improvement,"""On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game.""",4.48E+07,"p.13 of https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf

“7 × 4 Blocks with 256 channels, cycling through dilations 1, 2, 4, 8”
“48 × 4 Blocks with 128 channels, cycling through dilations 1, 2, 4, 8”

Dilations don't change the number of parameters in each filter

Unclear what the ""projection"" layers are - just count convolution layer parameters.

Approximation: 7 * 4 * 256 * 3 * 3 * 256 + 48 * 4 * 128 * 3 * 3 * 128 = 44826624
",1.00E+20,"Estimated in the blogpost below

https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening",,,,Multiple tasks! Different units,,,,,,Self-supervised learning,241.59,,Yes,Industry,Speculative,"Protein structure prediction can be used to determine the three-dimensional shape of a protein from its amino acid sequence1. This problem is of fundamental importance as the structure of a protein largely determines its function2; however, protein structures can be difficult to determine experimentally. Considerable progress has recently been made by leveraging genetic information. It is possible to infer which amino acid residues are in contact by analysing covariation in homologous sequences, which aids in the prediction of protein structures3. Here we show that we can train a neural network to make accurate predictions of the distances between pairs of residues, which convey more information about the structure than contact predictions. Using this information, we construct a potential of mean force4 that can accurately describe the shape of a protein. We find that the resulting potential can be optimized by a simple gradient descent algorithm to generate structures without complex sampling procedures. The resulting system, named AlphaFold, achieves high accuracy, even for sequences with fewer homologous sequences. In the recent Critical Assessment of Protein Structure Prediction5 (CASP13)—a blind assessment of the state of the field—AlphaFold created high-accuracy structures (with template modelling (TM) scores6 of 0.7 or higher) for 24 out of 43 free modelling domains, whereas the next best method, which used sampling and contact information, achieved such accuracy for only 14 out of 43 domains. AlphaFold represents a considerable advance in protein-structure prediction. We expect this increased accuracy to enable insights into the function and malfunction of proteins, especially in cases for which no structures for homologous proteins have been experimentally determined7.",8/15/23 16:39
Big Transfer (BiT-L),Vision,Image classification,Google Brain,Industry,"A Kolesnikov, L Beyer, X Zhai, J Puigcerver, J Yung",12/24/19,Large scale learning of general visual representations for transfer,https://arxiv.org/abs/1912.11370,8.30E+01,,,9.28E+08,,,,,,,,,,,,,,,,,Industry,,,8/11/23 17:51
OpenAI Five,Games,Dota 2,OpenAI,Industry,"J Raiman, S Zhang, F Wolski",12/13/19,Dota 2 with Large Scale Deep Reinforcement Learning,https://arxiv.org/abs/1912.06680,4.54E+02,SOTA improvement,"""On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game.""",1.59E+08,"""We define a policy (π) as a function from the history of observations to a probability distribution
over actions, which we parameterize as a recurrent neural network with approximately 159 million
parameters (θ)."" pg. 3 of paper

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",6.70E+22,"""770±50 PFlops/s·days of compute"" for the model that played against world champions. They did a single training run that took 10 months.

While the model was playing against world champions, they continued training for a few days, so that the resulting model used even more training compute: 820±50 PFlops/s·days.

Finally, they also trained a Rerun model with 150±5 PFlops/s·days of compute.

Source: Dota 2 with Large Scale Deep Reinforcement Learning
https://arxiv.org/abs/1912.06680",,,4.54E+11,"""Although the Dota 2 engine runs at 30 frames per second, OpenAI Five only acts on every 4th
frame which we call a timestep""
--> 7.5 timesteps/s

""OpenAI Five is a single training run that ran from June 30th, 2018 to April 22nd, 2019. "" --> 296 days

296 * 24*3600 * 7.5 = 1.92e8

This number seems a little low? The DQN paper had 1e7 timesteps. Might be to do with sample efficiency?

EDIT 14/06/2022
Multiple copies of OpenAI Five were trained in parallel, so the total training time is much higher than 296 days.
Table 1 shows 220,000 GPU iterations, each iteration has a batch size of between 1M and 3M timesteps (Table 2), so the total number of episodes is on the order of 2e11",,,,,,,166042.11,,Yes,Industry,,"On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.
",6/8/23 0:39
OpenAI Five Rerun,Games,Dota 2,OpenAI,Industry,"Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung,
Przemysław “Psyho"" Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Józefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique Pondé de Oliveira Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, Susan Zhang",12/13/19,Dota 2 with Large Scale Deep Reinforcement Learning,https://cdn.openai.com/dota-2.pdf,1.00E+03,SOTA improvement,"""On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game.""",1.59E+08,"""We define a policy (π) as a function from the history of observations to a probability distribution
over actions, which we parameterize as a recurrent neural network with approximately 159 million
parameters (θ)."" pg. 3 of paper

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",1.30E+22,"THIS CALCULATION IS FOR RERUN

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",,,5.31E+10,"54k iterations (Fig 7)
with a batch size of 983040 (Table 2)",,,,,,,32217.13,,Yes,Industry,,"On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.",5/29/23 20:51
StarGAN v2,Drawing,,"NAVER,Yonsei University,Swiss Federal Institute of Technology",Industry - Academia Collaboration (Industry leaning),"Yunjey Choi, Youngjung Uh, Jaejun Yoo, Jung-Woo Ha",12/4/19,StarGAN v2: Diverse Image Synthesis for Multiple Domains,https://arxiv.org/abs/1912.01865,8.32E+02,,,,,,,,,,,,,,,,,,,,Industry,,,8/15/23 14:25
Photo-Geometric Autoencoder,Vision,,University of Oxford,Academia,"Shangzhe Wu, Christian Rupprecht, Andrea Vedaldi
",11/25/19,Unsupervised Learning of Probably Symmetric Deformable 3D Objects From Images in the Wild,https://arxiv.org/abs/1911.11130,1.96E+02,,,,,,,,,,,,,,,,,,,,Academia,,,8/30/23 22:41
MuZero,Games,Atari Games,DeepMind,Industry,"J Schrittwieser, I Antonoglou, T Hubert, K Simonyan",11/19/19,Mastering Atari Go Chess and Shogi by Planning with a Learned Model,https://arxiv.org/abs/1911.08265v2,4.12E+02,SOTA improvement,,3.69E+07,"Both the representation and dynamics function use the same architecture asAlphaZero, but with 16 instead of20 residual blocks [15]. We use 3x3 kernels and 256 hidden planes for each convolution.

Previous downsampling:
•  1 convolution with stride 2 and 128 output planes, output resolution 48x48.•  2 residual blocks with 128 planes•  1 convolution with stride 2 and 256 output planes, output resolution 24x24.•  3 residual blocks with 256 planes.•  Average pooling with stride 2, output resolution 12x12.•  3 residual blocks with 256 planes.•  Average pooling with stride 2, output resolution 6x6.",4.80E+19,"third-generation Google Cloud TPU
(For each board game, we used 16 TPUs for training and 1000 TPUs for self-play)
For each game in Atari, we used 8 TPUs for training and 32 TPUs for self-play
Training for 12 hours (for Atari)
Data from Parameter, Compute and Data Trends in Machine Learning
Google v3 TPU: 1.23E+14 FLOP/s (although with the caveat that it might be not applicable)
Utilization rate 
In LaMDA: Language Models for Dialog Applications, they report for TPU V3: 56.5%
Calculations for Atari:
12 hours → 43200 seconds
(8 TPUs for training) * (1.23*10^14 FLOP/s) * (43.2 *10^3 s) * (0.565 utilization rate) = 2.4017472 * 10^19 FLOP
Training time missing for boardgames
Assumption also 12 hours 
Also: 2.4017472 * 10^19 FLOP
Total cost ≈ 4.8 * 10^19 FLOP",,,2.00E+10,"Table 1
https://arxiv.org/pdf/1911.08265.pdf",,,,,,,121.18,,Yes,Industry,,"Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.",5/29/23 20:51
MoCo,Drawing,Image completion,Facebook AI,Industry,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xe, Ross Girshick",11/13/19,Momentum Contrast for Unsupervised Visual Representation Learning,https://arxiv.org/abs/1911.05722,1.72E+03,Highly cited,,3.75E+08,https://openai.com/blog/image-gpt/#rfref53,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
Noisy Student (L2),Vision,Image classification,"Carnegie Mellon University,Google",Industry - Academia Collaboration (Industry leaning),"Q Xie, MT Luong, E Hovy, QV Lee",11/11/19,Self-training with Noisy Student improves ImageNet classification,https://paperswithcode.com/paper/self-training-with-noisy-student-improves/review/,5.76E+02,,,4.80E+08,,8.49E+20,"""Our largest model, EfficientNet-L2, needs to be trained for 6 days on a Cloud TPU v3 Pod, which has 2048 cores, if the unlabeled batch size is 14x the labeled batch size""

2048*4.00E+12*60**2*24*4*0.3 = 8.5e20","ImageNet, JFT",,8.10E+07,"""Due to duplications, there are only 81M unique images among these 130M images.""",1.04E+12,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",144,"""Xie et al. (2020) required 33 TPUv3 core-years to train their Noisy Student EfficientNet-L2""

https://arxiv.org/abs/2103.00020",,,,,,Industry,,,8/15/23 15:00
AlphaStar,Games,StarCraft,DeepMind,Industry,"Oriol Vinyals,Igor Babuschkin,Wojciech M. Czarnecki,Michaël Mathieu,Andrew Dudzik,Junyoung Chung,David H. Choi,Richard Powell,Timo Ewalds,Petko Georgiev,Junhyuk Oh,Dan Horgan,Manuel Kroiss,Ivo Danihelka,Aja Huang,Laurent Sifre,Trevor Cai,John P. Agapiou,Max Jaderberg,Alexander S. Vezhnevets,Rémi Leblond,Tobias Pohlen,Valentin Dalibard,David Budden,Yury Sulsky,James Molloy,Tom L. Paine,Caglar Gulcehre,Ziyu Wang,Tobias Pfaff,Yuhuai Wu,Roman Ring,Dani Yogatama,Dario Wünsch,Katrina McKinney,Oliver Smith,Tom Schaul,Timothy Lillicrap,Koray Kavukcuoglu,Demis Hassabis,Chris Apps,David Silver",10/30/19,Grandmaster level in StarCraft II using multi-agent reinforcement learning,https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning,1.04E+03,Highly cited,,1.39E+08,"AlphaStar has 139 million weights, but only 55 million weights are required during inference.",2.02E+23,"Estimated in the blogpost below

https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening",,,,"Multiple data types. First supervised learning, then other stuff",,,,,,,512765.27,,,Industry,,"Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using generalpurpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.",6/8/23 0:39
BART-large,Language,,Facebook AI,Industry,"Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer",10/29/19,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",https://arxiv.org/abs/1910.13461,1.01E+03,Highly cited,,4.06E+08,"""In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.""

I counted the parameters in the huggingface model
https://huggingface.co/facebook/bart-large/tree/main

from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(""facebook/bart-large"")
model = AutoModel.from_pretrained(""facebook/bart-large"")
sum(p.numel() for p in model.parameters() if p.requires_grad)",,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
T5-11B,Language,Text autocompletion,Google,Industry,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",10/23/19,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://arxiv.org/abs/1910.10683,1.54E+03,Highly cited,,1.10E+10,The full 11-billion parameter model,4.05E+22,"https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
Table 4",C4,,1.50E+11,"""This produces a collection of text that is not only
orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also
comprises reasonably clean and natural English text. We dub this data set the “Colossal
Clean Crawled Corpus” (or C4 for short) and release it as part of TensorFlow Datasets""

750GB * 200M word/GB = 1.5e11",,,,,Google TPU V3,Self-supervised learning,105686.2,,Yes,Industry,,"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",9/21/23 3:01
T5-3B,Language,Text autocompletion,Google,Industry,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",10/23/19,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://arxiv.org/abs/1910.10683,1.54E+03,Highly cited,,3.00E+09,source: https://lair.lighton.ai/akronomicon/,1.04E+22,source: https://lair.lighton.ai/akronomicon/,C4,,1.50E+11,"""This produces a collection of text that is not only
orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also
comprises reasonably clean and natural English text. We dub this data set the “Colossal
Clean Crawled Corpus” (or C4 for short) and release it as part of TensorFlow Datasets""

750GB * 200M word/GB = 1.5e11",,,,,Google TPU V3,Self-supervised learning,25777.12,,Yes,Industry,,"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",9/21/23 3:01
Rubik's cube,Robotics,,Open AI,Industry,"Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, Lei Zhang
",10/15/19,Solving Rubik’s Cube with a Robot Hand,https://arxiv.org/abs/1910.07113,5.18E+02,,,2.78E+07,"Table 13 on pg. 44 of the Cube paper, saved in ""RL papers"" folder. Sum of all the trainable parameters (dominated by the value and policy networks).

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",8.54E+20,source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389,,,6.24E+07,""" The cumulative amount of experience over that period used for training on the
Rubik’s cube is roughly 13 thousand years, which is on the same order of magnitude as the 40 thousand years used by
OpenAI Five""

13/40 * 1.92e8 = 6.24e7",,,,,NVIDIA Tesla V100 DGXS 32 GB,,3102.27,,,Industry,,"We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR) and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems. Videos summarizing our results are available: this https URL",6/7/23 18:10
AlphaX-1,Vision,Neural architecture search for computer vision,Brown and Facebook AI Research,Industry - Academia Collaboration (Academia leaning),"Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, Rodrigo Fonseca1",10/2/19,AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search,https://arxiv.org/abs/1903.11059,7.20E+01,,,5.79E+08,,7.60E+18,,ImageNet,,,,,,,,NVIDIA Geforce GTX1080 Ti,,24.1,,,Industry,,,6/7/23 18:08
DistilBERT,Language,Text autocompletion,HuggingFace,Industry,"Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf",10/2/19,"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",https://arxiv.org/abs/1910.01108,8.95E+02,,,6.60E+07,Table 3,1.24E+19,"Section 3: DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours.

1.6e13*8*60**2*90*0.3 = 1.2e19",,"Section 3: We train DistilBERT on the same corpus as the original BERT model: a concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015].",,,,,,,,,,,,Industry,,,8/15/23 12:32
ALBERT,Language,,"Toyota Technological Institute at Chicago,Google Research",Industry - Academia Collaboration,"Z Lan, M Chen, S Goodman, K Gimpel",9/26/19,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,https://arxiv.org/abs/1909.11942,1.66E+03,Highly cited,,1.80E+07,Section 3.2 of paper,,,,,3.30E+09,"Pretraining same as for BERT - Wikipedia and BookCorpus

""For the pre-training corpus we
use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)""",2.25E+10,"Rados dataset  (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,,Industry,,,8/11/23 17:34
Hide and Seek,Games,Hide and Seek,OpenAI,Industry,"B Baker, I Kanitscheider, T Markov, Y Wu",9/17/19,Emergent Tool Use From Multi-Agent Autocurricula,https://openai.com/blog/emergent-tool-use/,5.00E+02,,,1.60E+06,"""The default model, which uses a batch size of 64,000 and 1.6 million parameters,..."" pg. 7 of the Hide and Seek paper, stored in ""RL papers""

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",3.04E+17,source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389,,,3.17E+10,"""The default model, which uses a batch size of 64,000 and 1.6 million parameters, requires 132.3 million episodes (31.7 billion frames) over 34 hours of training to reach stage 4 of the skill progression, i.e. ramp defense.""",,,,,,,0.8,,,Industry,,,5/29/23 20:51
Megatron-BERT,Language,,NVIDIA,Industry,"M Shoeybi, M Patwary, R Puri, P LeGresley",9/17/19,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,https://arxiv.org/abs/1909.08053,5.57E+02,,,3.90E+09,Source: https://lair.lighton.ai/akronomicon/,6.90E+22,"A source: https://lair.lighton.ai/akronomicon/ claims 5.7e22

Param-based calculation:
6ND = 6*3.9e9*2e6*1024*1024 = 4.8e22 FLOP

Time-based calculation:
The 8.3B GPT-like arch took 2.1 days per epoch on 512 GPUs, batch size 512. An epoch was 68.5k iterations.
BERT: batch size 1024, 2e6 iterations total.
So we should expect 4B => 1.0 days per epoch (69e3*512 examples)
=> 2e6*1024/(69e3*512) = 58 days training
The GPUs were V100 39e12 FLOP/s with 30% util.
C=58*39e12*24*60*60*512=9.9e22 FLOP

The param and time calculations seem more trustworthy. Geometric mean is 6.9e22 FLOP",,,3.48E+10,"""The resulting aggregate corpus contains 174 GB of deduplicated text.""",,,,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,208034.39,,Yes,Industry,,"Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",8/8/23 17:42
Megatron-LM (8.3B),Language,,NVIDIA,Industry,"M Shoeybi, M Patwary, R Puri, P LeGresley",9/17/19,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,https://arxiv.org/abs/1909.08053,5.57E+02,,,8.30E+09,"Source: https://lair.lighton.ai/akronomicon/

Archived source: https://web.archive.org/web/20211220142906/https://lair.lighton.ai/akronomicon/
",9.10E+21,Source: https://lair.lighton.ai/akronomicon/,,,3.48E+10,"""The resulting aggregate
corpus contains 174 GB of deduplicated text.""",1.80E+13,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,33212.51,,Yes,Industry,,"Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",9/14/23 19:26
ObjectNet,Vision,Object recognition,Massachusetts Institute of Technology,Academia,"Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfre- und, Josh Tenenbaum, and Boris Katz",9/6/19,Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models,https://papers.nips.cc/paper/2019/file/97af07a14cacba681feacf3012730892-Paper.pdf,2.39E+03,Highly cited,,3.80E+07,,1.94E+19,"3-5 days of training (say, 4.5), 50 teraFLOP/second at 50% utilization rate (reported) = 1.94E19",Internal data,,5.00E+04,"In total, 95,824 images were collected from 5,982 workers out of which 50,000 images were retained
after validation and included in the dataset",,,,,,,50.79,,,Academia,,"We collect a large real-world test set, ObjectNet, for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. Most scientific experiments have controls, confounds which are removed from the data, to ensure that subjects cannot perform a task by exploiting trivial correlations in the data. Historically, large machine learning and computer vision datasets have lacked such controls. This has resulted in models that must be fine-tuned for new datasets and perform better on datasets than in real-world applications. When tested on ObjectNet, object detectors show a 40-45% drop in performance, with respect to their performance on other benchmarks, due to the controls for biases. Controls make ObjectNet robust to fine-tuning showing only small performance increases. We develop a highly automated platform that enables gathering datasets with controls by crowdsourcing image capturing and annotation. ObjectNet is the same size as the ImageNet test set (50,000 images), and by design does not come paired with a training set in order to encourage generalization. The dataset is both easier than ImageNet – objects are largely centered and unoccluded – and harder, due to the controls. Although we focus on object recognition here, data with controls can be gathered at scale using automated tools throughout machine learning to generate datasets that exercise models in new ways thus providing valuable feedback to researchers. This work opens up new avenues for research in generalizable, robust, and more human-like computer vision and in creating datasets where results are predictive of real-world performance.",9/12/23 22:28
Pluribus,Games,Poker,Facebook AI Research,Industry - Academia collaboration,"Noam Brown, Tuomas Sandholm",7/11/19,Superhuman AI for multiplayer poker,https://www.science.org/cms/asset/910714a7-ee2a-486e-9970-42fb893b08d9/pap.pdf,5.75E+02,,,,,6.60E+16,"Trained in 8 days on a 64 core CPU
https://ai.facebook.com/blog/pluribus-first-ai-to-beat-pros-in-6-player-poker/

""We trained the blueprint strategy for Pluribus in eight days on a 64-core server and required less than 512 GB of RAM. No GPUs were used. At typical cloud computing instance rates, it would cost less than $150 to train.""

Guess: trained on i7 Intel CPU, approx 5e9 FLOP/s for each core.

 https://epochai.org/blog/estimating-training-compute
8 days, 64 cores, 5e9 FLOP/s, 30% utilization",,,,,,,,,,,,,,,,,7/25/23 16:44
BigBiGAN,Drawing,Image completion,Google,Industry,"Spyros Gidaris, Praveer Singh, Nikos Komodakis",7/4/19,Large Scale Adversarial Representation Learning,https://arxiv.org/abs/1907.02544,4.03E+02,,,8.60E+07,https://openai.com/blog/image-gpt/#rfref53,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
RoBERTa Large,Language,,Facebook,Industry,"Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov",7/1/19,RoBERTa: A Robustly Optimized BERT Pretraining Approach,https://arxiv.org/abs/1907.11692,1.51E+03,Highly cited,,3.55E+08,,4.15E+21,"Section 5: We pretrain our model using 1024 V100 GPUs for approximately one day.

Note this is the base pretraining comparable to BERT, 100k steps. Subsequently they do more: ""increasing the number of pretraining steps
from 100K to 300K, and then further to 500K"".

So assume 5x the 1024 V100 GPUs for 1d estimate. Mixed precision.

C=5*1024*3.13E+13*60**2*24*0.3 = 4.2e21",,,3.20E+10,160GB*200M words/GB = 3.2e10 words,,"Authors of KEPLER say their model has the same inference compute as RoBERTa, so if we calculate this we may use it for KEPLER, too

""    It depends on the length of the input sequences. The inference computation of KEPLER is the same as RoBERTa (base) and you may estimate it with this.""",,,,,,,,Industry,,,8/15/23 12:31
Walking Minotaur robot,Robotics,,"UC Berkeley,Google Brain",Industry - Academia Collaboration (Industry leaning),"Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, Sergey Levine",6/19/19,Learning to Walk via Deep Reinforcement Learning,https://arxiv.org/abs/1812.11103,3.77E+02,SOTA Improvement,,,,,,,,,,,,,,,Reinforcement learning,,,,Industry,Speculative,"Deep reinforcement learning (deep RL) holds the promise of automating the acquisition of complex controllers that can map sensory inputs directly to low-level actions. In the domain of robotic locomotion, deep RL could enable learning locomotion skills with minimal engineering and without an explicit model of the robot dynamics. Unfortunately, applying deep RL to real-world robotic tasks is exceptionally difficult, primarily due to poor sample complexity and sensitivity to hyperparameters. While hyperparameters can be easily tuned in simulated domains, tuning may be prohibitively expensive on physical systems, such as legged robots, that can be damaged through extensive trial-and-error learning. In this paper, we propose a sample-efficient deep RL algorithm based on maximum entropy RL that requires minimal per-task tuning and only a modest number of trials to learn neural network policies. We apply this method to learning walking gaits on a real-world Minitaur robot. Our method can acquire a stable gait from scratch directly in the real world in about two hours, without relying on any model or simulation, and the resulting policy is robust to moderate variations in the environment. We further show that our algorithm achieves state-of-the-art performance on simulated benchmarks with a single set of hyperparameters. Videos of training and the learned policy can be found on the project website.",8/11/23 16:50
FixRes ResNeXt-101 WSL,Vision,Image classification,Facebook AI,Industry,"H Touvron, A Vedaldi, M Douze, H Jégou",6/14/19,Fixing the train-test resolution discrepancy,https://arxiv.org/abs/1906.06423,4.05E+02,,,8.29E+08,,,,,,9.40E+08,"""Conversely, when training a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images at resolution 224x224 and further optimizing for test resolution 320x320, we obtain a test top-1 accuracy of 86.4% (top-5: 98.0%) (single-crop)""",,,,,,,,"https://medium.com/swlh/deepmind-achieved-starcraft-ii-grandmaster-level-but-at-what-cost-32891dd990e4#:~:text=According%20to%20the%20analysis%20by,Source%3A%20DeepMind.",,Industry,,,5/29/23 20:51
AMDIM,Drawing,Image completion,Microsoft Research,Industry,"Philip Bachman, R Devon Hjelm, William Buchwalter",6/3/19,Learning Representations by Maximizing Mutual Information Across Views,https://arxiv.org/abs/1906.00910,4.86E+02,,,6.26E+08,source: https://openai.com/blog/image-gpt/#rfref13e,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
XLM,Language,,Facebook,Industry,"G Lample, A Conneau",6/1/19,Cross-lingual Language Model Pretraining,https://arxiv.org/abs/1901.07291,6.78E+02,,,6.65E+08,,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
XLNet,Language,,"Carnegie Mellon University,Google Brain",Industry - Academia Collaboration,"Z Yang, Z Dai, Y Yang, J Carbonell",6/1/19,XLNet: Generalized Autoregressive Pretraining for Language Understanding,https://arxiv.org/abs/1906.08237,3.06E+03,Highly cited,,3.60E+08,,,,,,,,,,,,,,,,,Industry,,,8/11/23 16:49
DLRM-2020,Recommendation,,Facebook AI,Industry,"M Naumov, D Mudigere, HJM Shi, J Huang",5/31/19,Deep Learning Recommendation Model for Personalization and Recommendation Systems,https://arxiv.org/abs/1906.00091,3.45E+02,,,1.00E+11,"Figure 1

https://arxiv.org/abs/2104.05158",4.00E+18,"Figure 1

https://arxiv.org/abs/2104.05158",,,,,,,,,,,14.6,,,Industry,,,5/29/23 20:51
#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!
FTW,Games,Capture the flag,DeepMind,Industry,"M Jaderberg, WM Czarnecki, I Dunning, L Marris",5/31/19,Human-level performance in 3D multiplayer games with population-based reinforcement learning,https://deepmind.com/research/publications/capture-the-flag,4.25E+02,,,1.26E+08,"Architecture described in figure S11 of the supplement

The architecture includes modules for visual embedding, reward prediction, recurrent processing, policy, baseline and pixel control.

Input is 84x84x3 pixels as seen in figure S10 of the supplement

""We elected to use a resolution of 84x84 pixels as in previous related work in this environment. Each pixel is represented by a triple of three bytes""

Visual embedding (84x84x3 -> 256)
32*(8*8*3+1)+64*(4*4*32+1)+64*(3*3*64+1)+64*(3*3*64+1) + (84/(S^4)*84/(S^4)*64+1)*256
Note there is no information about the stride S used in the convolutions; we assume S = 1

Reward prediction (256 -> 3)
(256+1)*128 + (128+1)*3

Recurrent processing (n-> 512)
VU1 (256 -> 512)
4*(799+2*32)*((512+(32*2) + 3*32 + 5*2 + 3)+(799+2*32)+1) + 2*(256+1)*256

VU2 (512 -> 512)
4*(512+2*32)*((512+(32*2) + 3*32 + 5*2 + 3)+(512+2*32)+1) + 2*(256+1)*256

LSTMs usually have 4*(n*m+n*n+n) parameters, where n=input size and m=output size.

This DNS + LSTM takes as input the concatenation of the previous layer of size n and R read vectors of size W=32; and outputs m units plus an interface vector of size (W*R) + 3*W + 5*R + 3, for a total of about 4*(n+R*W)*((m+(W*R) + 3*W + 5*R + 3)+(n+R*32)+1) parameters

I assume R=2 since that seems implied by the previous paper (?)

The first VU has as input the visual embedding (size 256), the previous action (size 540) and the previous reward (size 3), for a total size of 256+540+3 = 799. The output is size 512.

The second VU has input size 512 and output size 512

The DNC memory architecture is described in https://www.nature.com/articles/nature20101.epdf

Policy (512 -> 5x3x3x3x2x2)
6*(512+1)*256 + (256+1)*5 + 3*(256+1)*3 + 2*(256+1)*2

Baseline
(512+1)*256 + (256+1)*1

Pixel control
(512+1)*32*7*7 + 32*(9*9+1) + 5*(4*4+1) + 3*2*(4*4+1) + 2*2*(4*4+1) + 1*(4*4+1)
""we trained independent pixel control policies for each of the six action groups""",7.26E+21,"We assume that most operations happen in the visual embedding.

2* 84^2*84^2 * 32 * 3 / 1^2 = 9.5 *10^9
new image size: 76 x 76 x 32
ignore ReLU/additions becaue probably very little influence 
2 * 76^2 * 76^2 * 10* 64 = 4 *10^10
new image size: 72 x 72 x 64
2 * 72^2 *72^2 * 64 * 64 * 3=  6.6 * 10^11
new image size: 69 x 69 x 64
2 * 69^2 *69^2 * 64 * 64 * 3=  5.5 * 10^11
new image size: 66 x 66 x 64
Linear layer: 2* ( 66*66*64)*256 = 1.4*10^8
Total aprox: 1.21e+12 FLOP/forward pass

",,,,,1.21E+12,,,,,,21045.02,,Yes,Industry,,,5/29/23 20:51
Grover-Mega,Language,,University of Washington,Industry - Academia Collaboration (Academia leaning),"R Zellers, A Holtzman, H Rashkin, Y Bisk",5/29/19,Defending Against Neural Fake News,https://arxiv.org/abs/1905.12616,5.43E+02,,,1.50E+09,,,,,,,,,,,,,,,,,Industry,,,6/8/23 0:39
MnasNet-A1 + SSDLite,Vision,Performing image classification and object detection on mobile devices,Google,Industry,"Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V. Le",5/29/19,MnasNet: Platform-Aware Neural Architecture Search for Mobile,https://arxiv.org/abs/1807.11626,1.43E+03,Highly cited,,4.90E+06,,1.50E+21,"4.5 days of training
64 TPUv2 devices, which have a peak performance of 180 TFLOPS
Assume 1/3 utilization rate
0.33*4.5*8.64*10^4*64*1.8*10^14 ~= 1.5×10^21",MS COCO,,1.18E+05,,,,,,Google TPU V3,,4331,,,Industry,,"Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8x faster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3x faster than NASNet [36] with 1.2% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at this https URL",6/7/23 17:11
MnasNet-A3,Vision,Performing image classification and object detection on mobile devices,Google,Industry,"Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V. Le",5/29/19,MnasNet: Platform-Aware Neural Architecture Search for Mobile,https://arxiv.org/abs/1807.11626,1.43E+03,Highly cited,,5.20E+06,From https://arxiv.org/pdf/1807.11626.pdf,1.50E+21,"4.5 days of training
64 TPUv2 devices, which have a peak performance of 180 TFLOPS
Assume 1/3 utilization rate
0.33*4.5*8.64*10^4*64*1.8*10^14 ~= 1.5×10^21",ImageNet,,1.28E+06,"""In this paper, we directly perform our architecture search on the ImageNet training set but with fewer training steps (5 epochs). As a common practice, we reserve randomly selected 50K images from the training set as the fixed validation set. """,,,,,Google TPU V3,,4331,,,Industry,,"Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8x faster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3x faster than NASNet [36] with 1.2% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at this https URL",6/7/23 17:11
EfficientNet-L2,Vision,Image classification,Google,Industry,"M Tan, Q Le",5/28/19,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/abs/1905.11946,3.19E+03,Highly cited,,4.80E+08,,,,,,,,3.90E+08,"table 2: using efficientnet_b0
",,,,,,,,Industry,,,9/20/23 19:58
CPC v2,Drawing,Image completion,"DeepMind, Berkeley",Industry - Academia Collaboration (Industry leaning),,5/22/19,Data-Efficient Image Recognition with Contrastive Predictive Coding,https://arxiv.org/abs/1905.09272,4.91E+02,,,3.03E+08,source: https://openai.com/blog/image-gpt/#rfref25d,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
ResNet-50 Billion-scale,Vision,Image classification,Facebook AI,Industry,"I. Zeki Yalniz, Hervé Jégou, Kan Chen, Manohar Paluri, Dhruv Mahajan",5/2/19,Billion-scale semi-supervised learning for image classification,https://arxiv.org/abs/1905.00546,3.97E+02,Highly cited,,2.50E+07,25M parameters vanilla ResNet50,,,,,,,,,,,,,,,,Industry,,,9/20/23 19:55
ResNeXt-101 Billion-scale,Vision,Image classification,Facebook AI,Industry,"IZ Yalniz, H Jégou, K Chen, M Paluri",5/2/19,Billion-scale semi-supervised learning for image classification,https://arxiv.org/abs/1905.00546,3.19E+02,,,1.93E+08,,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
DANet,Vision,Semantic segmentation,Chinese Academy of Sciences,Industry - Academia Collaboration (Academia leaning),"Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, Hanqing Lu",4/21/19,Dual Attention Network for Scene Segmentation,https://openaccess.thecvf.com/content_CVPR_2019/html/Fu_Dual_Attention_Network_for_Scene_Segmentation_CVPR_2019_paper.html,1.99E+03,,,,,,,,,,,,,,,,,,,,Industry,,,8/15/23 15:29
SpecAugment,Language,Speech recognition,Google Brain,Industry," Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, Quoc V. Le",4/18/19,SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,https://arxiv.org/abs/1904.08779,1.41E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,8/15/23 15:29
Cross-lingual alignment,,,"Tel Aviv University,Massachusetts Institute of Technology",Academia,"Tal Schuster, Ori Ram, Regina Barzilay, and Amir Globerson.",4/4/19,"Cross-lingual alignment of contextual word embeddings, with applications to zero- shot dependency parsing.",https://arxiv.org/abs/1902.09492,1.67E+02,,,,,2.56E+18,"From author communication:

Precision: float32

Hardware: 4 GPU  NVIDIA 1080Ti

NVIDIA 1080Ti: 1.06E+13

Compute: 7 GPU-days

0.4 * 1.06E+13 FLOP/s * 7 days * 24h/day * 3600s/h
= 2.56E+18",,,,,3.66E+12,"From author communication:

Precision: float32

Hardware: 4 GPU  NVIDIA 1080Ti

NVIDIA 1080Ti: 1.06E+13

Compute (Estimate): 0.00001 GPU Days


0.4 * 1.06E+13 FLOP/s * 0.00001 days * 24h/day * 3600s/h
= 3.66E+12



",,,,,7.83,,,Academia,,,9/12/23 22:28
KataGo,Games,Go,Jane Street,Industry,David J. Wu,2/27/19,Accelerating Self-Play Learning in Go,https://arxiv.org/abs/1902.10565,7.00E+01,SOTA Improvement,Better than ELF OpenGo while using 1/50th the compute.,2.50E+06,https://arxiv.org/abs/2210.00849 gives parameter count for AlphaZero in Fig 1b.,2.32E+19,"""[KataGo] surpasses the strength of ELF OpenGo after training on about 27 V100 GPUs for 19 days""
14.13 teraFLOP/s * 19 days = 2.32e+19 FLOP",,"Self-play: ""In total, KataGo’s main run lasted for 19 days using a maximum of 28 V100 GPUs at any time (averaging 26-27) and generated about 241 million training samples across 4.2 million games.""",2.41E+08,241 million training samples across 4.2 million games,,,456,27 processors for 19 days,NVIDIA Tesla V100 DGXS 16 GB,Self-supervised learning,,,,Industry,Speculative,"By introducing several improvements to the AlphaZero process and architecture, we greatly accelerate self-play learning in Go, achieving a 50x reduction in computation over comparable methods. Like AlphaZero and replications such as ELF OpenGo and Leela Zero, our bot KataGo only learns from neural-net-guided Monte Carlo tree search self-play. But whereas AlphaZero required thousands of TPUs over several days and ELF required thousands of GPUs over two weeks, KataGo surpasses ELF's final model after only 19 days on fewer than 30 GPUs. Much of the speedup involves non-domain-specific improvements that might directly transfer to other problems. Further gains from domain-specific techniques reveal the remaining efficiency gap between the best methods and purely general methods such as AlphaZero. Our work is a step towards making learning in state spaces as large as Go possible without large-scale computational resources.",7/21/23 16:22
ProxylessNAS,Vision,,Massachusetts Institute of Technology,Academia,"Han Cai, Ligeng Zhu, and Song Han",2/23/19,ProxylessNAS: Direct neural architecture search on target task and hardware,https://arxiv.org/abs/1812.00332,9.96E+02,,,,,3.71E+19,"For their searched Imagenet models, they used 200 GPU hours on a V100 GPU.

At FP32, a V100 GPU has a peak performance of 1.56E+14 FLOPS.

Utilization rate of 0.33.",ImageNet,,1.28E+06,,2.63E+11,"5.1 Miliseconds on a V100 GPU
",,,,,135.04,,,Academia,,"Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 104 GPU hours) makes it difficult to \emph{directly} search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize~\emph{proxy} tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present \emph{ProxylessNAS} that can \emph{directly} learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08\% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6× fewer parameters. On ImageNet, our model achieves 3.1\% better top-1 accuracy than MobileNetV2, while being 1.2× faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.",9/12/23 22:28
GPT-2 (1542M),Language,,OpenAI,Industry,"A Radford, J Wu, R Child, D Luan, D Amodei",2/14/19,Language Models are Unsupervised Multitask Learners,https://openai.com/blog/better-language-models/,1.70E+03,Highly cited,,1.50E+09,"""GPT-2 is a large transformer-based language model with 1.5 billion parameters""",1.49E+21,"We use COMPUTE = FORWARD COMPUTE PER TOKEN * 3 BACKWARD FORWARD ADJUSTMENT* N EPOCHS * N TOKENS IN TRAINING DATASET

The number of epochs is not reported, but this other paper [1] claims in table 1 that it is 20 or 100 epochs. 100 epochs is consistent with the original GPT paper.

[1] https://arxiv.org/abs/1906.06669",,,3.00E+09,"“All results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017 and which after de-duplication and some heuristic based cleaning contains slightly over 8 million documents for a total of 40 GB of text.”
40GB is approximately 3e9 words.
",3.40E+12,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,Self-supervised learning,4692.89,"https://en.wikipedia.org/wiki/GPT-2#:~:text=The%20cloud%20compute%20costs%20for,full%201.5%20billion%20parameter%20model).",Yes,Industry,,"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",9/14/23 19:14
Hanabi 4 player,Games,Hanabi,"DeepMind,University of Oxford,Carnegie Mellon University,Google Brain",Industry - Academia Collaboration (Industry leaning),,2/1/19,The Hanabi Challenge: A New Frontier for AI Research,https://arxiv.org/abs/1902.00506,2.29E+02,Historical significance,Adapted some SOTA RL algorithms to a new task that posed research challenges,7.64E+05,source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389,4.30E+18,14.13e+12 FLOP/s * 7 days * 86400 s/day * 0.50 utilization = 4.3e+18 FLOP,,,,,,,,,,,0.34,,,Industry,,,8/11/23 16:53
MT-DNN,Language,,Microsoft,Industry,"X Liu, P He, W Chen, J Gao",1/31/19,Multi-Task Deep Neural Networks for Natural Language Understanding,https://arxiv.org/abs/1901.11504,5.53E+02,,,3.30E+08,,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
Decoupled weight decay regularization,Vision,Image classification,University of Freiburg,Academia,Ilya Loshchilov and Frank Hutter,1/4/19,Decoupled weight decay regularization.,https://arxiv.org/abs/1711.05101,2.06E+03,Highly cited,,3.65E+07,"From author communication

WideResNet 28-10 models with 36.5 million parameters (3.65E+07)",2.47E+18,"From author communication

Per image: 5.24 billion FLOPs (5.24E+09)  Per training run: 50k times 5.24E+09 times 1800 epochs = 2.47E+18 FLOPs",CIFAR-10,,5.00E+04,,1.73E+09,"From author communication

Best estimate: 1.73E+09",,,,,8.07,,,Academia,,,6/8/23 0:39
Transformer ELMo,Language,Language modelling,"AI2,Allen Institute for AI,University of Washington",Industry - Academia Collaboration (Industry leaning),"ME Peters, M Neumann, L Zettlemoyer, W Yih",1/1/19,Dissecting Contextual Word Embeddings: Architecture and Representation,https://www.semanticscholar.org/paper/Dissecting-Contextual-Word-Embeddings%3A-Architecture-Peters-Neumann/ac11062f1f368d97f4c826c317bf50dcc13fdb59,3.08E+02,,,5.60E+07,,,,,More info on this is extractable with some time,,,,,,,,,,,,Industry,,,8/15/23 16:37
GPipe (Amoeba),Vision,Image classification,Google,Industry,"Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, Zhifeng Chen",11/16/18,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,https://arxiv.org/abs/1811.06965,4.86E+02,,,5.57E+08,Section 4,,,ImageNet,,1.28E+06,Table 4,,,,,,,,,,Industry,,,8/15/23 15:54
GPipe (Transformer),Language,Translation,Google,Industry,"Y Huang, Y Cheng, A Bapna, O Firat",11/16/18,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,https://arxiv.org/abs/1811.06965,4.86E+02,,,6.00E+09,Section 5: ,,,,,2.00E+10,"[WORDS]

Section 5: ""We use a
corpus of parallel documents over 102 languages and English, containing a total of 25 billion training examples, ranging from 10^4 to 10^9 per language""

10^9 sentences * 20 words per sentence",,,,,,,,,Yes,Industry,,,6/8/23 0:39
BERT-Large,Language,Next sentence prediction,Google,Industry,"J Devlin, MW Chang, K Lee, K Toutanova",10/11/18,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://arxiv.org/abs/1810.04805,2.38E+04,Highly cited,,3.40E+08,,2.85E+20,more info here https://docs.google.com/document/d/1B8x6XYcmB1u6Tmq3VcbAtj5bzhDaj2TcIPyK6Wpupx4/edit?usp=sharing,,,3.30E+09,"""For the pre-training corpus we
use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)""",7.90E+10,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,Self-supervised learning,999.93,,Yes,Industry,,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",8/10/23 15:24
MetaMimic,Games,,Google,Industry,"Tom Le Paine, Sergio Gomez",10/11/18,One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,https://arxiv.org/abs/1810.05017,1.90E+01,,,2.20E+07,"""This representational demand motivates the introduction of high-capacity deep neural networks. We found the architecture, shown in Figure 3, with residual connections, 20 convolution layers with 512 channels
for a total of 22 million parameters, and instance normalization to drastically improve performance, as shown in Figure 6 of the Experiments section.""",,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
BigGAN-deep 512x512,Drawing,Image generation,"Heriot-Watt University,DeepMind",Industry - Academia Collaboration,"A Brock, J Donahue, K Simonyan",9/28/18,Large Scale GAN Training for High Fidelity Natural Image Synthesis,https://arxiv.org/abs/1809.11096,1.98E+03,Highly cited,,1.13E+08,"I used the publicly available implementation available at [1]

There I loaded the biggan-deep512/1 model, and ran script [2] to compute the number of parameters

[1] https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb

[2]
n_params = 0
for var in module.variables:
  n_params += np.prod(var.shape.as_list())
  pass

print(n_params)",3.00E+21,"Estimate taken from here

https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening",JFT-300M,,2.92E+08,"""To confirm that our design choices are effective for even larger and more complex and diverse datasets, we also present results of our system on a subset of JFT-300M (Sun et al., 2017). The full JFT-300M dataset contains 300M real-world images labeled with 18K categories. Since the category distribution is heavily long-tailed, we subsample the dataset to keep only images with the 8.5K most common labels. The resulting dataset contains 292M images – two orders of magnitude larger than ImageNet. """,,,,,Google TPU V3,,10448.44,,,Industry,,"Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple ""truncation trick,"" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.",6/7/23 17:10
ESRGAN,Vision,Image super-resolution,"Chinese University of Hong Kong, Chinese Academy of Sciences, Nanyang Technological University",Academia,"Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen Change Loy, Yu Qiao, Xiaoou Tang",9/1/18,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,https://arxiv.org/abs/1809.00219,1.50E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
RCAN,Vision,Image super-resolution,Northeastern University,Academia," Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, Yun Fu",7/8/18,Image Super-Resolution Using Very Deep Residual Channel Attention Networks,https://openaccess.thecvf.com/content_ECCV_2018/html/Yulun_Zhang_Image_Super-Resolution_Using_ECCV_2018_paper.html,1.80E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,8/30/23 22:54
Population-based DRL,Games,Capture the flag,DeepMind,Industry,"Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, Thore Graepel",7/3/18,Human-level performance in first-person multiplayer games with population-based deep reinforcement learning,https://arxiv.org/abs/1807.01281,4.34E+02,,,1.22E+08,"Calculated from the architecture schematic in Figure S11 on pg 55 of the Capture the Flag supplementary materials. This is dominated by the size of the vision module, which is 116 million parameters, followed by the temporal processors which is 4.3 million parameters. The RL policy itself is only 0.79 million parameters. Also, I'm pretty uncertain if I'm right about how I calculated these parameters.

Source: 
https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",3.49E+19,"Source: 
https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",,,,,6.00E+10,"""Agents were trained for two billion steps, corresponding to approximately 450K
games.""

""We train a
population of 30 different agents together, which provides a diverse set of teammates and opponents to
play with, and is also used to evolve the internal rewards and hyperparameters of agents and learning
process""

30 * 2e9 = 6e10",,,,,130.36,,Yes,Industry,,"Recent progress in artificial intelligence through reinforcement learning (RL) has shown great success on increasingly complex single-agent environments and two-player turn-based games. However, the real-world contains multiple agents, each learning and acting independently to cooperate and compete with other agents, and environments reflecting this degree of complexity remain an open challenge. In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag, using only pixels and game points as input. These results were achieved by a novel two-tier optimisation process in which a population of independent RL agents are trained concurrently from thousands of parallel matches with agents playing in teams together and against each other on randomly generated environments. Each agent in the population learns its own internal reward signal to complement the sparse delayed reward from winning, and selects actions using a novel temporally hierarchical representation that enables the agent to reason at multiple timescales. During game-play, these agents display human-like behaviours such as navigating, following, and defending based on a rich learned representation that is shown to encode high-level game knowledge. In an extensive tournament-style evaluation the trained agents exceeded the win-rate of strong human players both as teammates and opponents, and proved far stronger than existing state-of-the-art agents. These results demonstrate a significant jump in the capabilities of artificial agents, bringing us closer to the goal of human-level intelligence.",5/29/23 20:51
ShuffleNet v2,Vision,,"Tsinghua University, Megvii Inc",Industry - Academia Collaboration,"N Ma, X Zhang, HT Zheng",6/30/18,ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design,https://arxiv.org/abs/1807.11164,1.41E+03,Highly cited,,2.28E+06,,,,,,,,3.00E+08,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,,Industry,,,5/29/23 20:51
MobileNetV2,Vision,,Google,Industry,"M Sandler, A Howard, M Zhu",6/18/18,MobileNetV2: Inverted Residuals and Linear Bottlenecks,https://ieeexplore.ieee.org/document/8578572,5.71E+03,Highly cited,,3.40E+06,Rados,,,,,,,6.00E+08,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,,Industry,,,8/10/23 15:23
GPT,Language,,OpenAI,Industry,"A Radford, K Narasimhan, T Salimans, I Sutskever",6/1/18,Improving Language Understanding by Generative Pre-Training,https://openai.com/blog/language-unsupervised/,2.26E+03,Highly cited,,1.17E+08,"""The model had 117M parameters in total.""

source: https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2",1.76E+19,"COMPUTE = FORWARD COMPUTE PER TOKEN * 3 BACKWARD FORWARD ADJUSTMENT * EPOCHS * DATASET SIZE

""We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens.""
",BooksCorpus,"""We use the BooksCorpus dataset [71] for training the language model""",1.00E+09,"""BookCorpus is a large collection of free novel books written by unpublished authors, which contains 11,038 books (around 74M sentences and 1G words) of 16 different sub-genres (e.g., Romance, Historical, Adventure, etc.).""
https://paperswithcode.com/dataset/bookcorpus

BookCorpus seems to have about 5000MB of content
source: https://huggingface.co/datasets/bookcorpusopen

Assuming a byte-pair encoder similar to GPT-2, there are 8 bytes / token.

So approximately 5000MB / 8 bytes / token = 5e9 / 8 tokens",1.20E+11,"https://github.com/amirgholami/ai_and_memory_wall estimates 9.6e+10. The ELECTRA paper (https://arxiv.org/pdf/2003.10555.pdf) Table 1 reports 3.0E+10 FLOP for GPT. But: ""Infer FLOPs assumes a single length-128 input"". If we instead assume 512 tokens as per GPT's training process, then I think the calculation would be 4x larger, i.e. 1.2E+11. This is closer to the estimate of 9.6E+10 in the link.",,,,Self-supervised learning,68.72,,Yes,Industry,,"Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).",6/14/23 15:28
ResNeXt-101 32x48d,Vision,Image classification,Facebook,Industry,"Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, Laurens van der Maaten",5/2/18,Exploring the Limits of Weakly Supervised Pretraining,https://arxiv.org/abs/1805.00932,6.19E+02,,,8.29E+08,"Table 6
",1.77E+21,"Table 6: 153e9 mult-adds.
Section 2.4: ""minibatches of 8,064 images"".

Compute = 2 * 3 * mult-adds * dataset size = 2 * 3 * 153e9 * 1.925e9 = 2.2e17 FLOP",,"Instagram images, captioned with hashtags",1.93E+09,Table 3: 1925e6,3.12E+10,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,"""Mahajan et al. (2018) required 19
GPU years to train their ResNeXt101-32x48d""
https://arxiv.org/abs/2103.00020",,,,,,Industry,,,9/12/23 5:52
YOLOv3,Vision,Object detection,University of Washington,Academia,"Joseph Redmon, Ali Farhadi",4/8/18,YOLOv3: An Incremental Improvement,https://arxiv.org/abs/1804.02767,7.71E+03,Highly cited,,5.69E+07,"Feature extractor (ignoring biases)
32*3*3*3 +
64*3*3*32 +
32*1*1*64 +
64*3*3*32 +
128*3*3*64 +
2*(64*1*1*128 +
128*3*3*64) +
256*3*3*128 +
8*(128*1*1*256 +
256*3*3*128) +
512*3*3*256 + 
8*(256*1*1*512 + 
512*3*3*256) + 
1024*3*3*512 + 
4*(512*1*1*1024 +
1024*3*3*512) +
4*4*1024*1000

source: table 1
This is assuming the average pooling step changes the output size from 8x8 to 4x4.

The weights file is 237MB. If the weights are saved as float32, 4 bytes per weight, then there are approximately 237M/4=59M parameters, consistent with the calculation above.",5.09E+19,"We use the formula training_compute = ops_per_forward_pass * 3.5 * n_epochs * n_examples

Assuming 160 epochs of training as in https://arxiv.org/pdf/1612.08242.pdf",ImageNet,,1.28E+06,Source: https://image-net.org/download.php,1.87E+10,"Table 2, Darknet-53. Note that the inference compute depends on the image resolution..",,,"NVIDIA M40,NVIDIA GTX Titan X",,295.76,,,Academia,,"We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at this https URL",7/11/23 16:50
Rotation,Drawing,Image completion,École des Ponts ParisTech,Academia,"Spyros Gidaris, Praveer Singh, Nikos Komodakis",3/21/18,Unsupervised Representation Learning by Predicting Image Rotations,https://arxiv.org/abs/1803.07728,1.16E+03,Highly cited,,8.60E+07,https://openai.com/blog/image-gpt/#rfref53,,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
Chinese - English translation,Language,Translation,Microsoft,Industry,"H Hassan, A Aue, C Chen, V Chowdhary",3/1/18,Achieving Human Parity on Automatic Chinese to English News Translation,https://www.microsoft.com/en-us/research/publication/achieving-human-parity-on-automatic-chinese-to-english-news-translation/,5.30E+02,,,,,,,,,,,,,,,,,,,Yes,Industry,,,6/8/23 0:39
Residual Dense Network,Vision,Image super-resolution,"Northeastern University,University of Rochester",Academia," Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu",2/24/18,Residual Dense Network for Image Super-Resolution,https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Residual_Dense_Network_CVPR_2018_paper.html,1.78E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,8/30/23 22:55
Spectrally Normalized GAN,Vision,Image generation,"Preferred Networks Inc,Ritsumeikan University,National Institute of Informatics",Industry - Academia Collaboration,"Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida",2/16/18,Spectral Normalization for Generative Adversarial Networks,https://arxiv.org/abs/1802.05957,2.74E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,8/30/23 22:57
DeepLabV3+,Vision,Semantic segmentation,Google,Industry,"Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam",2/7/18,Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,https://arxiv.org/abs/1802.02611v3,5.37E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,8/10/23 15:23
AmoebaNet-A (F=190),Vision,Image classification,Google Brain,Industry,"E Real, A Aggarwal, Y Huang, QV Le",2/5/18,Regularized Evolution for Image Classifier Architecture Search,https://arxiv.org/abs/1802.01548,1.43E+03,Highly cited,,8.70E+07,Table 2,,,,,,,,,,,,,,,,Industry,,,8/11/23 17:50
AmoebaNet-A (F=448),Vision,Image classification,Google Brain,Industry,"Esteban Real, Alok Aggarwal, Yanping Huang, Quoc V Le",2/5/18,Regularized Evolution for Image Classifier Architecture Search,https://arxiv.org/abs/1802.01548,1.71E+03,Highly cited,,4.69E+08,Table 2,3.85E+20,"450 K40 GPUs for 20k models (approx. 7 days).
(From Imagenet paper-data, Besiroglu et al., forthcoming) ",Imagenet-1k,,1.28E+06,,,,,,,,5858.75,,,Industry,,"The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier---AmoebaNet-A---that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-the-art 83.9% / 96.6% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.",8/11/23 17:50
IMPALA,Games,Atari,DeepMind,Industry,"Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, Koray Kavukcuoglu",2/5/18,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://arxiv.org/abs/1802.01561,6.75E+02,,,1.60E+06,"""Figure 3 in the paper states that the large architecture has 1.6 million parameters. I am using the large model because it was the only one trained on all the Atari games at once, which seems like the most impressive task in the suite.""

Source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",1.68E+20,source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389,,,2.40E+11,"From fig 6, there were 1e10 environment frames, and 24 agents. Thus we note down 2.4e11 for the ""dataset size""",,,,,NVIDIA P100,,2553.82,,Yes,Industry,,"In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.",6/7/23 16:52
ELMo,Language,,"AllenAI, University of Washington",Industry,"ME Peters, M Neumann, M Iyyer, M Gardner",2/1/18,Deep contextualized word representations,https://arxiv.org/abs/1802.05365,7.48E+03,Highly cited,,9.40E+07,,,3300e12 - https://github.com/amirgholami/ai_and_memory_wall,,,,,2.60E+10,"Rados dataset (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,Yes,Industry,,,5/29/23 20:51
ULM-FiT,Language,Text classification,"University of San Francisco, Insight Centre NUI Galway",Industry - Academia Collaboration,"J Howard, S Ruder",1/18/18,Universal Language Model Fine-tuning for Text Classification,https://arxiv.org/abs/1801.06146,1.94E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
Refined Part Pooling,Vision,Person retrieval,"Tsinghua University,University of Technology Sydney,University of Texas at San Antonio",Academia,"Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, Shengjin Wang",1/9/18,Beyond Part Models: Person Retrieval with Refined Part Pooling (and a Strong Convolutional Baseline),https://arxiv.org/abs/1711.09349,1.24E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,6/8/23 0:39
AlphaZero,Games,,DeepMind,Industry,"D Silver, T Hubert, J Schrittwieser, I Antonoglou",12/5/17,Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm,https://arxiv.org/abs/1712.01815,1.08E+03,Highly cited,,,,3.67E+22,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,,,7.00E+05,"""We trained a separate instance of AlphaZero for each game. Training proceeded
for 700,000 steps""",,"This post claims 0.8 seconds per move for the 40-day training version of the model (Go)

https://www.yuzeh.com/data/agz-cost.html",,,,,162054.7,,Yes,Industry,,"The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.",5/29/23 20:51
PNAS-net,Vision,Image classification,"Johns Hopkins University,Google AI,Stanford University",Industry - Academia Collaboration (Industry leaning),"C Liu, B Zoph, M Neumann, J Shlens",12/2/17,Progressive Neural Architecture Search,https://arxiv.org/abs/1712.00559,1.15E+03,Highly cited,,8.60E+07,,,,,,,,,,,,,,,,,Industry,,,9/6/23 18:13
PNASNet-5,,,"Johns Hopkins University,Google AI,Stanford University",Industry - Academia Collaboration (Industry leaning),"C Liu, B Zoph, M Neumann, J Shlens",12/2/17,Progressive Neural Architecture Search,https://arxiv.org/abs/1712.00559,1.34E+03,Highly cited,,,,6.63E+19,"8 times less compute than Zoph (2018), which used 500 p100s for 4 days.
(From Imagenet paper-data, Besiroglu et al., forthcoming) ",Imagenet-1k,,1.28E+06,,,,,,,,991.48,,,Industry,,"We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al. (2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet.",9/6/23 18:13
TriNet,Vision,Person re-identification,"Visual Computing Institute,Aachen University",Academia,"Alexander Hermans, Lucas Beyer, Bastian Leibe",11/21/17,In Defense of the Triplet Loss for Person Re-Identification,https://arxiv.org/abs/1703.07737,1.99E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,8/30/23 23:03
ProgressiveGAN,Vision,Image generation,NVIDIA,Industry,"Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen",10/27/17,"Progressive Growing of GANs for Improved Quality, Stability, and Variation",https://arxiv.org/abs/1710.10196,3.91E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,8/30/23 23:06
CapsNet (MNIST),Vision,Character recognition,Google Brain,Industry,"S Sabour, N Frosst, GE Hinton",10/26/17,Dynamic Routing Between Capsules,https://arxiv.org/abs/1710.09829,2.74E+03,Highly cited,,8.20E+06,"""In terms of number of parameters the baseline has 35.4M while CapsNet
has 8.2M parameters and 6.8M parameters without the reconstruction subnetwork""",,"It should be feasible to estimate this from the information in the paper, but it would require carefully checking the FLOP involved for capsules.",MNIST,,6.00E+04,Section 5: The dataset has 60K and 10K images for training and testing respectively.,,,,,,,,,,Industry,,,8/15/23 12:21
CapsNet (MultiMNIST),Vision,Character recognition,Google Brain,Industry,"S Sabour, N Frosst, GE Hinton",10/26/17,Dynamic Routing Between Capsules,https://arxiv.org/abs/1710.09829,2.74E+03,Highly cited,,1.14E+07,"""This model has 24.56M parameters which is 2 times more parameters
than CapsNet with 11.36M parameters.""",,,,,,,,,,,,,,,,Industry,,,8/11/23 17:49
LRSO-GAN,Vision,Person re-identification,University of Technology Sydney,Academia,"Zhedong Zheng, Liang Zheng, Yi Yang",10/22/17,Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro,https://arxiv.org/abs/1701.07717,1.40E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,8/30/23 23:10
AlphaGo Zero,Games,Go,DeepMind,Industry,"D Silver, J Schrittwieser, K Simonyan, I Antonoglou",10/18/17,Mastering the game of Go without human knowledge,https://www.researchgate.net/publication/320473480_Mastering_the_game_of_Go_without_human_knowledge,5.81E+03,Highly cited,,4.64E+07,Quick calculation,3.41E+23,"source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389


AGZ had two models, one of which was small and another of which was large. The compute for AGZ is for the large model, which has 40 residual blocks instead of 20.",,,5.80E+09,"""Over the course of training, 29 million games of self-play were generated""

Approx 200 moves per Go game on average

https://homepages.cwi.nl/~aeb/go/misc/gostat.html

Thus 200 * 29e6 = 5.8e9",,,,,,,1544149.42,,Yes,Industry,,"A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo. © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.",5/29/23 20:51
SENet (ImageNet),Vision,Image classification,Chinese Academy of Sciences ; University of Oxford,Academia,"Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu",9/5/17,Squeeze-and-Excitation Networks,https://arxiv.org/abs/1709.01507,7.94E+03,Highly cited,,2.81E+07,Table 16,,,ImageNet,,,,3.87E+09,Table 16,,,,,,,,Academia,,,5/29/23 20:51
NeuMF (Pinterest),Recommendation,Collaborative filtering,"Shandong University,Texas A&M,National University of Singapore,Columbia University",Academia,"X He, L Liao, H Zhang, L Nie, X Hu",8/16/17,Neural Collaborative Filtering,https://arxiv.org/abs/1708.05031,2.43E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,6/8/23 20:40
Cutout-regularized net,Vision,Image classification,"University of Guelph,Canadian Institute for Advanced Research,Vector Institute",Industry - Academia Collaboration," Terrance DeVries, Graham W. Taylor",8/15/17,Improved Regularization of Convolutional Neural Networks with Cutout,https://arxiv.org/abs/1708.04552,1.45E+03,Highly cited,,,,,,,,,,,,,,,,,https://www.yuzeh.com/data/agz-cost.html,,Industry,,,8/31/23 17:01
OpenAI TI7 DOTA 1v1,Games,DOTA,OpenAI,Industry,"Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Józefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique P. d.O. Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, Susan Zhang",8/11/17,OpenAI Five defeats Dota 2 world champions,https://openai.com/five/,1.00E+03,Highly cited,,1.50E+07,"Section 4 states: ""we used a model with over 150 million parameters"" but this is for the 5v5 agent, not the 1v1.",6.0461E+20,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,,,,,,,,,,,2873.99,,,Industry,,"On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.",9/15/23 19:36
RetinaNet-R101,Vision,Object detection,Facebook AI research,Industry,"TY Lin, P Goyal, R Girshick, K He, P Dollar",8/7/17,Focal loss for dense object detection,https://arxiv.org/abs/1708.02002,8.42E+03,Highly cited,,5.30E+07,source: table 2 in https://arxiv.org/pdf/1911.09070.pdf,2.07E+18,"""We use synchronized SGD over 8 GPUs with a total of 16 images per minibatch (2 images per GPU). Unless otherwise specified, all models are trained for 90k iterations with an initial learning rate of 0.01, which is then divided by 10 at 60k and again at 80k iterations. We use horizontal image flipping as the only form of data augmentation unless otherwise noted. Weight decay of 0.0001 and momentum of 0.9 are used. The training loss is the sum the focal loss and the standard smooth L1 loss used for box regression [10]. Training time ranges between 10 and 35 hours for the models in Table 1e.""

NVIDIA M40 GPU

35*60**2*0.3*8*6.83E+12 = 2.07e18",COCO,,1.35E+05,trainval135k split,1.27E+11,source: table 2 in https://arxiv.org/pdf/1911.09070.pdf,,,NVIDIA M40,,,,,Industry,,,8/15/23 15:49
RetinaNet-R50,Vision,Object detection,Facebook AI research,Industry,"TY Lin, P Goyal, R Girshick, K He",8/7/17,Focal loss for dense object detection,https://arxiv.org/abs/1708.02002,8.42E+03,Highly cited,,3.40E+07,source: table 2 in https://arxiv.org/pdf/1911.09070.pdf,,,,,,,9.70E+10,source: table 2 in https://arxiv.org/pdf/1911.09070.pdf,,,,,,,,Industry,,,6/8/23 0:39
JFT,Vision,,"CMU,Google Research",Industry - Academia Collaboration,"ChenSun,AbhinavShrivastava,SaurabhSingh,andAbhinavGupta",8/4/17,Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.,https://arxiv.org/abs/1707.02968,1.14E+03,Highly cited,,,,8.43E+20,"Tesla K80 performance: 8.13 TFLOP/s

Assume 40% utilization

60 days * 50 GPUs * 40% utilization * 8.13 TFLOP/s/GPU = 8.43*10^20 FLOP",JFT-300M,,3.00E+08,,,,,,NVIDIA Tesla K80,,21396.42,,,Industry,,"The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.",9/19/23 20:43
NASNet-A,Vision,Image classification,Google Brain,Industry,"B Zoph, V Vasudevan, J Shlens",7/21/17,Learning Transferable Architectures for Scalable Image Recognition,https://arxiv.org/abs/1707.07012,3.10E+03,Highly cited,,8.90E+07,,,,,,,,,,,,,,,,,Industry,,,8/11/23 17:46
#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!
PSPNet,Vision,Image segmentation,Chinese University of Hong Kong,Industry - Academia Collaboration (Academia leaning),"Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia",7/21/17,Pyramid Scene Parsing Network,https://ieeexplore.ieee.org/document/8100143,6.07E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,8/31/23 17:01
ShuffleNet v1,Vision,,Megvii Inc,Industry,"X Zhang, X Zhou, M Lin, J Sun",7/3/17,ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,https://arxiv.org/abs/1707.01083,2.78E+03,Highly cited,,2.43E+06,,,,,,,,1.40E+08,"Table 4 (ShuffleNet 1x, g=8)

https://arxiv.org/abs/1707.01083",,,,,,,,Industry,,,6/8/23 0:39
NoisyNet-Dueling,Games,Atari Games,DeepMind,Industry,"M Fortunato, MG Azar, B Piot, J Menick",6/30/17,Noisy Networks for Exploration,https://arxiv.org/abs/1706.10295v3,4.80E+02,SOTA improvement,,,,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
DeepLabV3,Vision,Semantic segmentation,Google,Industry,"Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam",6/17/17,Rethinking Atrous Convolution for Semantic Image Segmentation,https://arxiv.org/abs/1706.05587,3.90E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,6/8/23 20:39
HRA,Games,Ms Pacman,Microsoft Maluuba,Industry - Academia Collaboration (Industry leaning),"H Van Seijen, M Fatemi, J Romoff, R Laroche",6/13/17,Hybrid Reward Architecture for Reinforcement Learning,https://arxiv.org/abs/1706.04208,2.22E+02,,,,,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
Transformer,Language,Translation,"Google Research,Google Brain",Industry,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin",6/12/17,Attention Is All You Need,https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf,2.52E+04,Highly cited,,2.13E+08,"This page suggests the transformer has 213M parameters.

""Although there are others architectures that make use of attention layers, none achieves so good results so fast. Not only that, but the only model that can compite against Transformer is the Slicenet22, proposed just fifteen days before. It takes much longer to train, due to the huge amount of parameters it requires (348 million against the 213 millions of Transformer), and the BLEU scores it achieves are slightly worse on average. In short, up to date it offers no profit over Transformer.""

https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html",7.42E+18,"""The model was trained during 300000 steps, roughly 3.5 days, using 8 NVIDIA P100 GPUs.""

source: https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html

NVIDIA Tesla P100 has 9.3 teraFLOPS single-precision performance

source: https://www.nvidia.com/en-gb/data-center/tesla-p100/

We assume 0.33 utilization performance, in line with OpenAI's ""AI and compute"" article

source: https://openai.com/blog/ai-and-compute/",,,3.60E+08,"[WORDS]

""For English-French, we used the significantly larger WMT
2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [31]""",5.40E+10,"Source: rados dataset (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,"We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using
the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the
bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps
(3.5 days).",NVIDIA P100,Self-supervised learning,111.17,,Yes,Industry,,,8/11/23 17:41
EDSR,Vision,Image super-resolution,Seoul National University,Academia,"Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee",6/10/17,Enhanced Deep Residual Networks for Single Image Super-Resolution,https://arxiv.org/abs/1707.02921,3.07E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,8/31/23 17:08
PointNet++,,3D segmentation,Stanford University,Academia,"Charles R. Qi, Li Yi, Hao Su, Leonidas J. Guibas",6/7/17,PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space,https://arxiv.org/abs/1706.02413,4.02E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
Inflated 3D ConvNet,Vision,Action recognition,"DeepMind,University of Oxford",Industry - Academia Collaboration,"Joao Carreira, Andrew Zisserman",6/1/17,"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",https://arxiv.org/abs/1705.07750,3.98E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,8/31/23 17:11
SRGAN,Vision,Image super-resolution,Twitter,Industry,"Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi",5/25/17,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,https://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html,1.10E+04,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,8/31/23 17:13
DeepLab (2017),Vision,Image segmentation,"University King College,Johns Hopkins University,Google",Industry - Academia Collaboration,"Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille",4/27/17,"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",https://ieeexplore.ieee.org/abstract/document/7913730,1.01E+04,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,8/15/23 20:25
MobileNet,Vision,,Google,Industry,"AG Howard, M Zhu, B Chen, D Kalenichenko",4/17/17,MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,https://arxiv.org/abs/1704.04861,9.19E+03,Highly cited,,4.20E+06,,,,,,,,1.14E+09,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,,Industry,,,6/8/23 20:39
WGAN-GP,Vision,Image generation,"Montreal Institute for learning Algorithms,Courant Institute of Mathematical Sciences",Academia,"Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville",3/31/17,Improved Training of Wasserstein GANs,https://arxiv.org/abs/1704.00028,6.04E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,8/31/23 17:17
Mask R-CNN,Vision,Image segmentation,Facebook AI Research,Industry,"Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick",3/30/17,Mask R-CNN,https://arxiv.org/abs/1703.06870,1.50E+04,Highly cited,,,,,,COCO,,,,,,,"Training with
ResNet-50-FPN on COCO trainval35k takes 32 hours
in our synchronized 8-GPU implementation (0.72s per 16-
image mini-batch), and 44 hours with ResNet-101-FPN",,,,,,Industry,,,5/29/23 20:51
Prototypical networks,Vision,Image classification,"University of Toronto,Twitter",Industry - Academia Collaboration," Jake Snell, Kevin Swersky, Richard S. Zemel",3/15/17,Prototypical Networks for Few-shot Learning,https://arxiv.org/abs/1703.05175,3.57E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,8/31/23 17:36
DnCNN,Vision,Image super-resolution,"Harbin Institute of Technology,Hong Kong Polytechnic University,ULSee Inc.,Xi’an Jiaotong University",Industry - Academia Collaboration (Academia leaning),"Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, Lei Zhang",2/1/17,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,https://ieeexplore.ieee.org/abstract/document/7839189,4.00E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,8/31/23 20:56
MoE,Language,Language modelling / Machine translation,"Jagiellonian University,Google Brain",Industry - Academia Collaboration (Industry leaning),"N Shazeer, A Mirhoseini, K Maziarz, A Davis",1/23/17,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,https://arxiv.org/abs/1701.06538,6.87E+02,,,8.70E+09,"Table 5

https://arxiv.org/abs/1701.06538",9.39E+19,"12 days 
64 NVIDIA K40 GPUS (see hardware data sheet for performance)
0.33 util rate",,,1.00E+11,"[WORDS]

""We constructed a similar training set consisting of shuffled unique sentences from Google’s internal
news corpus, totalling roughly 100 billion words""",,,,,NVIDIA Tesla K40t,,8484.35,,,Industry,,"The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",8/11/23 17:38
DeepStack,Games,Poker,"University of Alberta,Charles University,Czech Technical University",Academia,"Matej Moravčík, Martin Schmid, Neil Burch, Viliam Lisý, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, Michael Bowling",1/6/17,DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker,https://arxiv.org/abs/1701.01724,6.18E+02,,,2.50E+06,"Figure 3, p.9

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",1.45E+19,"The largest source of compute necessary for training seems to be the data generation job on 20 GPUs. We count this towards the training compute because it requires simulation using the network. This is analogous to the AlphaGo systems simulating Go games.

From p.26: ""For the flop network, one million poker flop situations (from after the flop cards are dealt) were generated and solved. These situations were solved using DeepStack’s depth limited solver with the turn network used for the counterfactual values at public states immediately after the turn card. We used a cluster of 20 GPUS and one-half of a GPU year of computation time.""

Assume they used P100 GPUs because they were common at the time (P100 was released in 2016 and this paper was published in 2017).

But assume low utilization of 10% to hedge on (a) lower-performing GPUs being used, (b) non-FLOP computations taking up a lot of the data generation job.

Calculation:
6 months * 30 days * 24 hours * 3600 seconds * 9.3e12 FLOP/s * 0.1 utilization = 1.446336e+19 FLOP.",,,1.00E+07,"""The turn network was trained by solving 10 million randomly generated poker turn
games. These turn games used randomly generated ranges, public cards, and a random pot
size (10).""",,,,,,,0,,,Academia,Speculative,,9/12/23 5:56
AlphaGo Master,Games,Go,DeepMind,Industry,"D Silver, J Schrittwieser, K Simonyan, I Antonoglou",1/1/17,Mastering the game of Go without human knowledge,https://www.researchgate.net/publication/320473480_Mastering_the_game_of_Go_without_human_knowledge,5.81E+03,Highly cited,,,,1.50E+23,"This is a guess. There was no single journal publication that accompanied this model, that gave information about architecture/model training time etc. All I could find was that it has the same architecture as AlphaGo Zero, and that it had roughly the same power consumption as AGZ. See for instance: 
https://deepmind.com/blog/article/alphago-zero-starting-scratch

Since AGZ reaches the ELO of AlphaGo Master in about 20 days (half of the total training time), I estimate the compute to be around half that of AGZ. I round this down to 1.5e23, and I expect this to only be accurate within an OOM.",,,,,,,,,,,852748.08,,,Industry,,"A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo. © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.",9/12/23 5:56
Libratus,Games,Poker,Carnegie Mellon University,Academia,"N Brown, T Sandholm, S Machine",1/1/17,Libratus: The Superhuman AI for No-Limit Poker,https://www.cs.cmu.edu/~noamb/papers/17-IJCAI-Libratus.pdf,9.70E+01,SOTA improvement,Claims to be first ML system to reach superhuman level at No Limit Poker Texas Hold Em,,,5.51E+20,"""In total, Libratus used about 25 million core hours. Of those, about 13 million core hours were used for exploratory experiments and evaluation. About 6 million core hours were spent on the initial abstraction and equilibrium finding component, another 3 million were used for nested subgame solving, and about 3 million were used on the self-improvement algorithm.""

""Like many data-centric supercomputers, Bridges offers a relatively a modest number of FLOPS, but lots of memory: 895 teraflops and 130 TB, respectively.""

I just used the first bullet point (as those are usually independent systems and you only benchmark one of them).
The first system has 752 nodes a 2CPUs a 14cores each.

source: https://www.top500.org/news/bridges-supercomputer-boots-up-at-pittsburgh/



1. 12M core hours for 196 cores
2. We have  895 TFLOPS for 752 nodes a 2 CPUs a 14 cores
2.1 That's 42.5 GFLOPS per core.
3. Running this for 12M h
3.1 12 * 10^6 * 60 * 60 * 42.5 * 10^9 FLOP/S = 1.823e21 FLOPs
4. Assuming 30% utilization
 1.823e21 * 0.3
→ 5.51e20 FLOPs",,,,,,,,"In total, Libratus used about 25 million core hours. Of those,
about 13 million core hours were used for exploratory experiments and evaluation. About 6 million core hours were spent
on the initial abstraction and equilibrium finding component,
another 3 million were used for nested subgame solving, and
about 3 million were used on the self-improvement algorithm.",,,6253.49,,,Academia,,"No-limit Texas Hold’em is the most popular variant of poker in the world. Heads-up no-limit Texas Hold’em is the main benchmark challenge for AI in imperfect-information games. We present Libratus, the first—and so far only—AI to defeat top human professionals in that game. Libratus’s architecture features three main modules, each of which has new algorithms: pre-computing a solution to an abstraction of the game which provides a high-level blueprint for the strategy of the AI, a new nested subgame-solving algorithm which repeatedly calculates a more detailed strategy as play progresses, and a self-improving module which augments the pre-computed blueprint over time.",9/12/23 5:56
YOLOv2,Vision,Object detection,"University of Washington,Allen Institute for AI",Industry - Academia Collaboration,"Joseph Redmon, Ali Farhadi",12/25/16,"YOLO9000: Better, Faster, Stronger",https://arxiv.org/abs/1612.08242,9.37E+03,Highly cited,,5.10E+07,Source: https://resources.wolframcloud.com/NeuralNetRepository/resources/YOLO-V2-Trained-on-MS-COCO-Data_1,,,,,,,,,,,,,,,,Industry,,,9/12/23 5:56
Diabetic Retinopathy Detection Net,Vision,,"UT Austin,UC Berkeley,Google",Industry - Academia Collaboration,"V Gulshan, L Peng, M Coram, MC Stumpe, D Wu",12/13/16,Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs,https://jamanetwork.com/journals/jama/article-abstract/2588763,3.54E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/12/23 5:56
GAN-Advancer,Vision,Image classification,OpenAI,Industry,"Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen",12/5/16,Improved Techniques for Training GANs,https://dl.acm.org/doi/10.5555/3157096.3157346,6.06E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/12/23 18:01
Elastic weight consolidation,,,DeepMind,Industry,"J Kirkpatrick, R Pascanu",12/2/16,Overcoming catastrophic forgetting in neural networks,https://arxiv.org/abs/1612.00796,2.16E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/12/23 5:56
PointNet,Other,3d segmentation,Stanford University,Academia,"CR Qi, H Su, K Mo, LJ Guibas",12/2/16,PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation,https://arxiv.org/abs/1612.00593,5.04E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/12/23 5:56
Image-to-image cGAN,,,UC Berkeley,Academia,"P Isola, JY Zhu, T Zhou",11/21/16,Image-to-Image Translation with Conditional Adversarial Networks,https://arxiv.org/abs/1611.07004,9.86E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/12/23 5:56
RefineNet,Vision,Object detection,"University of Adelaide,Australian Centre for Robotic Vision",Industry - Academia Collaboration,"Guosheng Lin, Anton Milan, Chunhua Shen, Ian Reid",11/20/16,RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation,https://arxiv.org/abs/1611.06612v3,2.06E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/12/23 5:56
PolyNet,Vision,Image classification,The Chinese University of Hong Kong,Academia,"X Zhang, Z Li, C Change Loy",11/17/16,PolyNet: A Pursuit of Structural Diversity in Very Deep Networks,https://arxiv.org/abs/1611.05725,2.82E+02,SOTA Improvement,"The Very Deep PolyNet, designed following this direction, demonstrates substantial improvements over the state-of-the-art on the ILSVRC 2012 benchmark. Compared to Inception-ResNet-v2, it reduces the top-5 validation error on single crops from 4.9% to 4.25%, and that on multi-crops from 3.7% to 3.45%.",9.20E+07,,,,,,1.28E+06,,,,,,,,,,,Academia,,,8/6/23 22:15
ResNeXt-50,Vision,Image classification,"UC San Diego, Facebook",Industry - Academia Collaboration,"Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He",11/16/16,Aggregated Residual Transformations for Deep Neural Networks,https://arxiv.org/abs/1611.05431,4.80E+03,Highly cited,,2.50E+07,"""If you’re thinking about ResNets, yes, they are related. ResNeXt-50 has 25M parameters (ResNet-50 has 25.5M).""

https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d",,,,,,,8.40E+09,"Rados  (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,,Industry,,,5/29/23 20:51
Deeply-recursive ConvNet,Vision,Image super-resolution,Seoul National University,Academia,"Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee",11/11/16,Deeply-Recursive Convolutional Network for Image Super-Resolution,https://arxiv.org/abs/1511.04491,1.97E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,8/31/23 21:49
NASv3 (CIFAR-10),Vision,,Google Brain,Industry,"Barret Zoph, Quoc V. Le",11/5/16,Neural Architecture Search with Reinforcement Learning,https://arxiv.org/abs/1611.01578,2.97E+03,Highly cited,,3.74E+07,Table 1,2.20E+21,"50 epochs * 50,000 images * 10.0 GFLOPSs * 12800 networks * 2 add-multiply * 3 backward pass 
= 1.9e6 PF = 22 pfs-days

source: https://openai.com/blog/ai-and-compute/",,,,,,,,,,,13069.35,,,Industry,,"Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.",8/11/23 17:41
Differentiable neural computer,,,Google DeepMind,Industry,"Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwińska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, Adrià Puigdomènech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu & Demis Hassabis",10/12/16,Hybrid computing using a neural network with dynamic external memory,https://www.nature.com/articles/nature20101,1.24E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/6/23 18:39
Xception,Vision,Image classification,Google,Industry,François Chollet,10/7/16,Xception: Deep Learning with Depthwise Separable Convolutions,https://arxiv.org/abs/1610.02357,,Highly cited,,,,,,,,,,,,,,,,1961.34,,,Industry,,"We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.",6/7/23 16:08
GNMT,Language,Translation,Google,Industry,"Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean",9/26/16,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,https://research.google/pubs/pub45610/,4.50E+03,Highly cited,,2.78E+08,"Table 5 in 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer'

https://arxiv.org/abs/1701.06538",6.90E+21,"sqrt(10 * 100) factor added because production model used 2-3 orders of magnitude more data, but only 1 epoch rather than 10.
96 K80 GPU’s * 9 days * 8.5 TFLOPS * 0.33 utilization * sqrt(10 * 100)  
= 6.9e6 PF = 79 pfs-days

source: https://openai.com/blog/ai-and-compute/",,,3.60E+08,"[WORDS]
"" On WMT En→Fr, the training set contains 36M sentence pairs. On WMT En→De, the training set contains 5M sentence pairs.""

36M sentences * 10 words/sentence",,,,,NVIDIA Tesla K80,,307573.5,,,Industry,,"Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (""wordpieces"") for both input and output. This method provides a good balance between the flexibility of ""character""-delimited models and the efficiency of ""word""-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.",6/8/23 0:39
Wide Residual Network,Vision,Image classification,Université Paris-Est,Academia,"Sergey Zagoruyko, Nikos Komodakis",9/19/16,Wide Residual Networks,https://arxiv.org/abs/1605.07146,4.52E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,6/8/23 0:39
MS-CNN,Vision,Object detection,"SVCL UC San Diego, IBM",Industry - Academia Collaboration,"Zhaowei Cai, Quanfu Fan, Rogerio S. Feris, Nuno Vasconcelos",9/17/16,A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection,https://link.springer.com/chapter/10.1007/978-3-319-46493-0_22,1.32E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
ResNet-1001,Vision,Image classification,Microsoft,Industry,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",9/17/16,Identity Mappings in Deep Residual Networks,https://link.springer.com/chapter/10.1007/978-3-319-46493-0_38,6.89E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/5/23 23:11
Stacked hourglass network,Vision,Pose estimation,University of Michigan,Academia,"Alejandro Newell, Kaiyu Yang, Jia Deng",9/17/16,Stacked Hourglass Networks for Human Pose Estimation,https://link.springer.com/chapter/10.1007/978-3-319-46484-8_29,3.60E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
TSN,Vision,Action recognition,"ETH Zurich,The Chinese University of Hong Kong,Shenzhen Institute of Advanced Technology",Academia,"Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, Luc Van Gool",9/17/16,Temporal Segment Networks: Towards Good Practices for Deep Action Recognition,https://link.springer.com/chapter/10.1007/978-3-319-46484-8_2,2.62E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,8/31/23 21:50
Youtube recommendation model,Recommendation,,Google,Industry,"Paul Covington, Jay Adams, and Emre Sargin",9/15/16,Deep Neural Networks for YouTube Recommendations,https://research.google/pubs/pub45530/,1.55E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/5/23 23:10
WaveNet,Speech,,Google DeepMind,Industry,"A Oord, S Dieleman, H Zen, K Simonyan",9/12/16,WaveNet: A Generative Model for Raw Audio,https://arxiv.org/abs/1609.03499,3.12E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,8/31/23 21:51
Multi-task Cascaded CNN,Vision,Face detection,"Chinese Academy of Sciences,Chinese University of Hong Kong",Academia,"Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, Yu Qiao",8/26/16,Joint Face Detection and Alignment using Multitask cascaded convolutional networks,https://arxiv.org/abs/1604.02878,3.40E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/5/23 23:20
DenseNet-264,Vision,Image classification,"Tsinghua University,Facebook AI research,Cornell University",Industry - Academia Collaboration (Academia leaning),"G Huang, Z Liu, L Van Der Maaten",8/25/16,Densely Connected Convolutional Networks,https://arxiv.org/abs/1608.06993,1.78E+04,Highly cited,,3.40E+07,,,,,,,,,,,,,,,,,Industry,,,8/11/23 19:18
Named Entity Recognition model,Language,Named Entity Recognition model,University of Toronto,Academia,"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin- ton",7/21/16,Layer Normalization,https://arxiv.org/abs/1607.06450,4.13E+03,Highly cited,,,,9.69E+16,"8 hours of training for NER
GeForce GTX TITAN X GPU
0.33 utilization rate
",,,,,,,,,,,0.63,,,Academia,,,5/29/23 20:51
Part-of-sentence tagging model,Language,POS tagging,University of Toronto,Academia,"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin- ton",7/21/16,Layer Normalization.,https://arxiv.org/abs/1607.06450,4.13E+03,Highly cited,,,,1.45E+17,"12 hours of training for POS tagging
GeForce GTX TITAN X GPU
0.33 utilization rate
",,,,,,,,,,,0.97,,,Academia,,,6/8/23 0:39
Character-enriched word2vec,Language,,Facebook AI research,Industry,"P Bojanowski, E Grave, A Joulin",7/15/16,Enriching Word Vectors with Subword Information,https://arxiv.org/abs/1607.04606,6.35E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/5/23 23:29
fastText,Language,,Facebook AI research,Industry,"A Joulin, E Grave, P Bojanowski, T Mikolov",7/6/16,Bag of Tricks for Efficient Text Classification,https://arxiv.org/abs/1607.01759,3.09E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/5/23 23:12
Wide & Deep,Recommendation,,Google,Industry,"HT Cheng, L Koc, J Harmsen, T Shaked",6/24/16,Wide & Deep Learning for Recommender Systems,https://arxiv.org/abs/1606.07792,1.61E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/5/23 23:14
R-FCN,Vision,Object detection,"Microsoft research, Tsinghua university",Industry - Academia Collaboration (Industry leaning),"Jifeng Dai, Y. Li, Kaiming He, and Jian Sun",6/21/16,R-fcn: Object detection via region-based fully convolutional networks.,https://arxiv.org/abs/1605.06409,4.49E+03,Highly cited,,,,6.15E+16,"1,464  images in 2012 VOC (https://paperswithcode.com/dataset/pascal-voc)/
9,963 images in 2007 VOC (https://www.tensorflow.org/datasets/catalog/voc)
83K training images in MS COCO  (https://paperswithcode.com/dataset/coco)

They used a Nvidia K40 GPU and report training time/image in seconds (table 3)

Assumed a 0.33 util rate",PASCAL VOC (2007 and 2012 vesrions) + MS COCO,,9.44E+04,,,,,,,,5.51,,,Industry,,,5/29/23 20:51
DMN,Language,,Salesforce,Industry,"Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, Richard Socher",6/20/16,Ask Me Anything: Dynamic Memory Networks for Natural Language Processing,https://arxiv.org/abs/1506.07285,1.19E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
Spatiotemporal fusion ConvNet,Vision,Video,"Graz University of Technology,University of Oxford",Academia,"Christoph Feichtenhofer, Axel Pinz, Andrew Zisserman",6/1/16,Convolutional Two-Stream Network Fusion for Video Action Recognition,https://openaccess.thecvf.com/content_cvpr_2016/html/Feichtenhofer_Convolutional_Two-Stream_Network_CVPR_2016_paper.html,2.28E+03,Highly cited,,,,,,UCF101,,9.72E+04,"[SECONDS OF VIDEO]

They use UCF101, whose paper says
""We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data""",,,,,,,,,,Academia,,,9/5/23 23:31
Symmetric Residual Encoder-Decoder Net,Vision,Image super-resolution,"Nanjing University,University of Adelaide",Academia,"Xiao-Jiao Mao, Chunhua Shen, Yu-Bin Yang",3/30/16,Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections,https://arxiv.org/abs/1603.09056v2,1.18E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/5/23 23:35
SqueezeNet,Vision,Image classification,"DeepScale,UC Berkeley,Stanford University",Industry - Academia Collaboration,"Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer",2/24/16,SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size,https://arxiv.org/abs/1602.07360,4.40E+03,Highly cited,,1.20E+06,"The paper says ""SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.""

AlexNet has 60 million parameters.",,,,,,,,,,,,,,,,Industry,,,9/6/23 18:13
Inception-ResNet-V2,Vision,Image classification,Google,Industry,"Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi",2/23/16,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",https://arxiv.org/abs/1602.07261,8.21E+03,Highly cited,,5.60E+07,,,,,,,,2.64E+09,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,,Industry,,,5/29/23 20:51
Inceptionv4,Vision,Image classification,Google,Industry,"Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi",2/23/16,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",https://arxiv.org/abs/1602.07261,8.21E+03,Highly cited,,4.30E+07,"""The folks from Google strike again with Inception-v4, 43M parameters.""

https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d",,,,,,,2.46E+10,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,,Industry,,,5/29/23 20:51
A3C FF hs,Games,Atari Games,"Google, University of Montreal",Industry - Academia Collaboration (Industry leaning),"V Mnih, AP Badia, M Mirza, A Graves",2/4/16,Asynchronous Methods for Deep Reinforcement Learning,http://arxiv.org/abs/1602.01783v2,5.28E+03,SOTA improvement,,,,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
Convolutional Pose Machines,Vision,Pose estimation,"The Robotics Institute,Carnegie Mellon University",Academia,"Shih-En Wei, Varun Ramakrishna, Takeo Kanade, Yaser Sheikh",1/30/16,Convolutional Pose Machines,https://arxiv.org/abs/1602.00134,2.42E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/7/23 15:35
AlphaGo Lee,Games,Go,DeepMind,Industry,"David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel & Demis Hassabis",1/27/16,Mastering the game of Go with deep neural networks and tree search,https://www.nature.com/articles/nature16961,1.08E+04,Highly cited,,,,1.90E+21,"This number is pretty uncertain. I expect it to be right to around a factor of 3, at least compared to AlphaGo Fan.

The architecture used was pretty much the same as AlphaGo Fan, but it was ""trained for longer"" and had around 5.33x the number of convolutional layers of AlphaGo Fan (256/48 = 5.33). 

The convolutional layers are the major contributor to the training compute, so I somewhat arbitrarily just multiply the compute for AlphaGo Fan by 5. Thus 3.8e20 * 5 = 1.9e21

Otherwise there has been little said about this model specifically - I've mainly relied on the source for AlphaGo Zero and AlphaGo Fan, linked below

AlphaGo Fan: https://www.nature.com/articles/nature16961

AlphaGo Zero: https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ",,,2.94E+07,"We trained the policy network pσ to classify positions according to expert moves played in the KGS data set. This data set contains 29.4 million positions from 160,000 games played by KGS 6 to 9 dan human players; 35.4% of the games are handicap games.",,,,,,,14041.8,,,Industry,Speculative,"The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.",8/3/23 20:35
Advantage Learning,Games,Atari Games,Google DeepMind,Industry,"MG Bellemare, G Ostrovski, A Guez",12/15/15,Increasing the Action Gap: New Operators for Reinforcement Learning,http://arxiv.org/abs/1512.04860v1,1.04E+02,SOTA improvement,,,,,,,,,,,,,,,,,,,Industry,,"This paper introduces new optimality-preserving operators on Q-functions. We first describe an operator for tabular representations, the consistent Bellman operator, which incorporates a notion of local policy consistency. We show that this local consistency leads to an increase in the action gap at each state; increasing this gap, we argue, mitigates the undesirable effects of approximation and estimation errors on the induced greedy policies. This operator can also be applied to discretized continuous space and time problems, and we provide empirical results evidencing superior performance in this context. Extending the idea of a locally consistent operator, we then derive sufficient conditions for an operator to preserve optimality, leading to a family of operators which includes our consistent Bellman operator. As corollaries we provide a proof of optimality for Baird's advantage learning algorithm and derive other gap-increasing operators with interesting properties. We conclude with an empirical study on 60 Atari 2600 games illustrating the strong potential of these new operators.",8/15/23 15:24
BPL,Drawing,Image generation,"University of Toronto,New York University,Massachusetts Institute of Technology",Academia,"BM Lake, R Salakhutdinov, JB Tenenbaum",12/11/15,Human-level concept learning through probabilistic program induction,https://science.sciencemag.org/content/350/6266/1332/,2.01E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/12/23 22:28
ResNet-110 (CIFAR-10),Vision,Image classification,Microsoft,Industry,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",12/10/15,Deep Residual Learning for Image Recognition,https://arxiv.org/abs/1512.03385,8.58E+04,Highly cited,,1.70E+06,Table 6,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
ResNet-152 (ImageNet),Vision,Image classification,Microsoft,Industry,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",12/10/15,Deep Residual Learning for Image Recognition,https://arxiv.org/abs/1512.03385,8.58E+04,Highly cited,,6.00E+07,Taken from https://arxiv.org/abs/1605.07146,1.21E+19,"(11.4 *10^9) mult-adds per forward pass
2 FLOPS/ mult-add
3.5 for forward & backward pass
1.2 * 10^6 examples in dataset
128 epochs

Source:x",ILSVRC 2012,"They won ILSVRC 2015, but actually the classification dataset is the same as 2012",1.28E+06,"""We evaluate our method on the ImageNet 2012 classification dataset [36] that consists of 1000 classes. The models are trained on the 1.28 million training images""",2.26E+10,Table 1,,,,,92.03,,,Industry,,"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",6/14/23 15:12
DeepSpeech2 (English),Speech,Speech recognition,Baidu Research- Silicon Valley AI Lab,Industry,"Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Hannun, Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh, David Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yogatama, Jun Zhan, Zhenyao Zhu",12/8/15,Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,https://arxiv.org/abs/1512.02595,2.21E+03,Highly cited,,3.80E+07,All networks have 38 million parameters.,2.60E+19,"1 timestep = (1280 hidden units)^2 * (7 RNN layers * 4 matrices for bidirectional + 2 DNN layers) * (2 for doubling parameters from 36M to 72M) = 98 MFLOP
20 epochs * 12,000 hours * 3600 seconds/hour * 50 samples/sec * 98 MFLOP * 3 add-multiply * 2 backprop 
= 26,000 PF = 0.30 pfs-days

See also AI and Compute by Dario Amodei and OpenAI https://openai.com/research/ai-and-compute",,,1.63E+08,"""Our English speech system is trained on 11,940 hours of speech, while the Mandarin system is trained on 9,400 hours.""

11,940 * 13,680 = 163339200",1.80E+09,,,,"NVIDIA GTX Titan X,NVIDIA Quadro K1200",,150.78,,,Industry,,"We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.",9/15/23 19:34
Inception v3,Vision,Image classification,"Google, University College London",Industry - Academia Collaboration (Industry leaning),"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna",12/2/15,Rethinking the inception architecture for computer vision.,https://arxiv.org/abs/1512.00567,1.47E+04,Highly cited,,2.36E+07,Table 3 from Xception paper,,,ILSVRC 2012,,1.20E+06,"The full dataset is a lot larger and has far more categories. When people say ""ImageNet"" they're usually referring to the subset of the full dataset with 1000 categories and 1.2million images, found here: https://image-net.org/challenges/LSVRC/2012/",1.15E+11,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,,,Industry,,,5/29/23 20:51
Netflix Recommender System,Recommendation,,Netflix,Industry,"CA Gomez-Uribe, N Hunt",12/1/15,"The Netflix Recommender System: Algorithms, Business Value, and Innovation",https://dl.acm.org/doi/pdf/10.1145/2843948,1.09E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/5/23 23:15
Multi-scale Dilated CNN,Vision,Image segmentation,"Princeton University,Intel Labs",Industry - Academia Collaboration,"Fisher Yu, Vladlen Koltun",11/23/15,Multi-Scale Context Aggregation by Dilated Convolutions,https://arxiv.org/abs/1511.07122,5.84E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/7/23 15:40
Dueling Network,,,Google DeepMind,Industry,"Z Wang, T Schaul, M Hessel",11/20/15,Dueling Network Architectures for Deep Reinforcement Learning,https://arxiv.org/abs/1511.06581,2.00E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/5/23 23:16
AlphaGo Fan,Games,Go,Google DeepMind,Industry,"D Silver, A Huang, CJ Maddison, A Guez, L Sifre",10/1/15,Mastering the game of Go with deep neural networks and tree search,https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ,5.18E+03,SOTA improvement,,8.21E+06,"The input to the policy network is a 19 × 19 × 48 image stack consisting of 48 feature planes. The first hidden layer zero pads the input into a 23 × 23 image, then convolves k filters of kernel size 5 × 5 with stride 1 with the input image and applies a rectifier nonlinearity. Each of the subsequent hidden layers 2 to 12 zero pads the respective previous hidden layer into a 21 × 21 image, then convolves k filters of kernel size 3 × 3 with stride 1, again followed by a rectifier nonlinearity. The final layer convolves 1 filter of kernel size 1 × 1 with stride 1, with a different bias for each position, and applies a softmax function. The match version of AlphaGo used k = 192 filters; Fig. 2b and Extended Data Table 3 additionally show the results of training with k = 128, 256 and 384 filters.

The input to the value network is also a 19 × 19 × 48 image stack, with an additional binary feature plane describing the current colour to play. Hidden layers 2 to 11 are identical to the policy network, hidden layer 12 is an additional convolution layer, hidden layer 13 convolves 1 filter of kernel size 1 × 1 with stride 1, and hidden layer 14 is a fully connected linear layer with 256 rectifier units. The output layer is a fully connected linear layer with a single tanh unit.",3.80E+20,"Assume 0.3 utilisation rate, 1e13 GPU FLOP/s [single precision]. Trained in three stages using 50 GPUs over 3 weeks + 1 day + 1 week

Training compute = (50 GPUs)(29 days)(86400s/day)(0.3 utilisation rate)(1e13 FLOP/s) = 3.8e20 FLOPs",,,,Supervised learning + self-play,,"Distributed: 176 GPUs + 1202 PUs + 40 search threads
Single machine: 8 GPUs + 48 CPUs 

https://www.nature.com/articles/nature16961",,,,,3076.07,,,Industry,,"A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.",5/29/23 20:51
Deep Deterministic Policy Gradients,,,Google DeepMind,Industry,"TP Lillicrap, JJ Hunt, A Pritzel, N Heess, T Erez",9/9/15,Continuous control with deep reinforcement learning,https://arxiv.org/abs/1509.02971,6.35E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/7/23 15:45
BPE,Language,Translation,University of Edinburgh,Academia,"R Sennrich, B Haddow, A Birch",8/31/15,Neural Machine Translation of Rare Words with Subword Units,https://arxiv.org/abs/1508.07909,4.06E+03,Highly cited,,,,,,WMT'15,,3.75E+07,"[WORDS]
""We perform experiments on data from the shared translation task of WMT 2015. For English→German, our training set consists of 4.2 million sentence pairs, or approximately 100 million tokens. For English→Russian, the training set consists of 2.6 million sentence pairs, or approximately 50 million tokens""

100M tokens, around half will be in English, 0.75 words per token

",,,,,,,,,,Academia,,,6/8/23 0:39
"Listen, Attend and Spell",,,"Google,Carnegie Mellon University",Industry - Academia Collaboration,"William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals",8/20/15,"Listen, attend and spell: A neural network for large vocabulary conversational speech recognition",https://ieeexplore.ieee.org/document/7472621,1.62E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/5/23 23:38
BatchNorm,,,Google,Industry,"S Ioffe, C Szegedy",6/15/15,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,https://arxiv.org/abs/1502.03167,2.92E+04,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!
YOLO,Vision,Object detection,"University of Washington,Allen Institute for AI,Facebook AI research",Industry - Academia Collaboration,"Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi",6/8/15,"You Only Look Once: Unified, Real-Time Object Detection",https://arxiv.org/abs/1506.02640,1.75E+04,Highly cited,,2.72E+08,"Calculation based on figure 3 of the paper:
7 * 7 * 3 * 64 + 3 * 3 * 64 * 192 + 1 * 1 * 192 * 128 + 3 * 3 * 128 * 256 + 1 * 1 * 256 * 256 + 3 * 3 * 256 * 512 + 4 * (1 * 1 * 512 * 256 + 3 * 3 * 256 * 512) + 1 * 1 * 512 * 512 + 3 * 3 * 512 * 1024 + 2 * (1 * 1 * 1024 * 512 + 3 * 3 * 512 * 1024) + 4 * (3 * 3 * 1024 * 1024) + 7 * 7 * 1024 * 4096 + 4096 * 7 * 7 * 30",,"This should be calculatable.
",,,,,,,,,,,,,,Industry,,,9/15/23 19:07
GoogLeNet / InceptionV1,Vision,Image classification,"Google,University of Michigan,University of North Carolina",Industry - Academia Collaboration (Industry leaning),"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich",6/7/15,Going deeper with convolutions,https://arxiv.org/abs/1409.4842,3.28E+04,Highly cited,,6.80E+06,Computed summing the parameters on table 1 of section 5,1.56E+18,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,ILSVRC 2014,,1.20E+06,"""The ILSVRC 2014 classification challenge involves the
task of classifying the image into one of 1000 leaf-node categories in the Imagenet hierarchy. There are about 1.2 million images for training, 50,000 for validation and 100,000 images for testing
[...]
We participated in the challenge with no external data
used for training.
""",,,,,,,14.16,,,Industry,,,6/16/23 15:05
Faster R-CNN,Vision,Object detection,Microsoft Research,Industry,"S Ren, K He, R Girshick, J Sun",6/4/15,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,https://arxiv.org/abs/1506.01497,2.30E+04,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
Trajectory-pooled conv nets,Vision,Action recognition,"Chinese University of Hong Kong,Chinese Academy of Sciences",Academia,"Limin Wang, Yu Qiao, Xiaoou Tang",5/19/15,Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors,https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Wang_Action_Recognition_With_2015_CVPR_paper.html,3.79E+03,Highly cited,,9.11E+06,"The input layer takes either a single RGB frame (224x224x3) for the spatial stream or a stack of 10 optical flow frames (224x224x20) for the temporal stream.
The first convolutional layer has 96 filters of size 7x7 with stride 2.
This is followed by max pooling with size 3x3 and stride 2.
The second convolutional layer has 256 filters of size 5x5 with stride 2.
After that is another max pooling layer (3x3, stride 2).
The third convolutional layer has 512 filters of size 3x3 with stride 1.
The fourth convolutional layer has 512 filters of size 3x3 with stride 1.
The fifth convolutional layer has 512 filters of size 3x3 with stride 1.
Next is a max pooling layer (3x3, stride 2).
The fully-connected layers have 4096, 2048, and 101 neurons respectively.

(7*7*20+1)*96 + (5*5*20+1)*256 + (3*3*20+1)*512 + (3*3*20+1)*512 + (3*3*20+1)*512 + 2*4096 + (4096+1)*2048 + (2048+1)*101 = 9106245",,,"ImageNet, UCF101",,,"They pretrain on ImageNet, and use UCF101 for actions. Its paper says ""We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data"".",,,,,,,,,,Academia,,,8/15/23 13:45
Deep LSTM for video classification,Vision,Video,"University of Maryland,University of Texas,Google",Industry - Academia Collaboration,"Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals, Rajat Monga, George Toderici",5/1/15,Beyond Short Snippets: Deep Networks for Video Classification,https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Ng_Beyond_Short_Snippets_2015_CVPR_paper.html,2.26E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/7/23 16:11
Fast R-CNN,Vision,Object detection,Microsoft Research,Industry,R Girshick,4/30/15,Fast R-CNN,https://arxiv.org/abs/1504.08083,1.58E+04,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,8/15/23 13:51
Constituency-Tree LSTM,Language,Semantic embedding,"MetaMind Inc,Stanford University",Industry - Academia Collaboration,"KS Tai, R Socher, CD Manning",2/28/15,Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks,https://arxiv.org/abs/1503.00075,2.62E+03,Highly cited,,2.05E+05,"Table 1

https://arxiv.org/abs/1503.00075",,,,,,,,,,,,,,,,Industry,,,9/6/23 18:14
DQN-2015,Games,Atari Games,Google,Industry,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, Demis Hassabis ",2/25/15,Human-level control through deep reinforcement learning,https://www.nature.com/articles/nature14236,1.57E+04,Highly cited,,1.69E+06,"The input to the neural network consists of an 84x84x4 image produced by the preprocess-ing mapw. The first hidden layer convolves 32 filters of 8x8 with stride 4 with theinput image and applies a rectifier nonlinearity. The second hidden layer con-volves 64 filters of 4x4 with stride 2, again followed by a rectifier nonlinearity.This is followedby a thirdconvolutional layer thatconvolves 64 filtersof 3x3 withstride 1 followed by a rectifier. The final hidden layer is fully-connected and con-sists of 512 rectifier units. The output layer is a fully-connected linear layer with asingle output for each valid action. The number of valid actions varied between 4 and 18 on the games we considered.

Example num params here: https://colab.research.google.com/drive/1Ty6SFYWd7EcKoxJohucL2OdiLR_3oXnI?usp=sharing",,"This should be calculatable, just needs careful reasoning about compute per frame.",,,5.00E+07,"Methods: ""we trained for a total of 50 million frames""",,,,,,,,,,Industry,,,8/15/23 14:54
CRF-RNN,Vision,Image segmentation,"University of Oxford,Stanford University,Baidu",Industry - Academia Collaboration,"Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, Philip H. S. Torr",2/11/15,Conditional Random Fields as Recurrent Neural Networks,https://arxiv.org/abs/1502.03240,2.66E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/7/23 16:04
"MSRA (C, PReLU)",Vision,Image classification,Microsoft Research,Industry,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",2/6/15,Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,https://arxiv.org/abs/1502.01852,2.01E+04,Highly cited,,8.70E+07,"I used the architecture in table 3
I ignored biases, and assumed a SPP bin size of 256

",2.40E+19,"""training C on eight K40 GPUs, takes about 3-4 weeks""
0.33 util rate
(From Imagenet paper-data, Besiroglu et al., forthcoming) ",ILSVRC 2012,"They won ILSVRC 2015, but actually the classification dataset is the same as 2012",1.28E+06,"""We perform the experiments on the 1000-class ImageNet 2012 dataset"", paper; ImageNet 2012 train set size from https://huggingface.co/datasets/imagenet-1k",,,588,,,,2166.22,,,Industry,,"Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.",6/8/23 0:39
ADAM (CIFAR-10),Vision,Image classification,"University of Amsterdam, OpenAI, University of Toronto",Industry - Academia Collaboration,"DP Kingma, J Ba",12/22/14,Adam: A Method for Stochastic Optimization,https://arxiv.org/abs/1412.6980,8.11E+04,Highly cited,,,CIFAR-10 with c64-c64-c128-1000 architecture,6.05E+16,"From https://openai.com/blog/ai-and-compute/ Appendix

less than 0.0007 pfs-days (86400*10^15*0.0007)",,,,,,,,,,,0.6,,,Industry,,,5/29/23 20:51
DeepLab,Vision,Image segmentation,"Google, University of California Los Angeles",Industry - Academia Collaboration,"Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille",12/22/14,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,https://arxiv.org/abs/1412.7062,3.70E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
NTM,,,Google DeepMind,Industry,"Alex Graves, Greg Wayne, Ivo Danihelka",12/10/14,Neural Turing Machines,https://arxiv.org/abs/1410.5401,1.93E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
Cascaded LNet-ANet,Vision,,The Chinese University of Hong Kong,Academia,"Z Liu, P Luo, X Wang, X Tang",11/28/14,Deep Learning Face Attributes in the Wild,https://arxiv.org/abs/1411.7766,4.01E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/7/23 16:32
Fully Convolutional Networks,Vision,Image segmentation,"University of California,Berkeley",Academia,"J Long, E Shelhamer, T Darrell",11/14/14,Fully Convolutional Networks for Semantic Segmentation,https://arxiv.org/abs/1411.4038,2.47E+04,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/7/23 16:29
LRCN,Vision,Video description,"UT Austin, UMass Lowell, UC Berkeley",Academia,"Jeff Donahue, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini Venugopalan, Sergio Guadarrama, Kate Saenko, Trevor Darrell",11/7/14,Long-term Recurrent Convolutional Networks for Visual Recognition and Description,https://arxiv.org/abs/1411.4389,5.22E+03,Highly cited,,1.43E+08,"1st model: CaffeNet fc6 feature extractor (4096-length vectors) -> LSTM with 1024 hidden units

2nd model: CaffeNet fc6 feature extractor (4096-length vectors) -> 2 layer LSTM with 1000 hidden units

3rd mode: Like the second, but has encoder and decoder LSTMs (both with 2 layers)

AlexNet (close relative to CaffeNet) has 61M params.

LSTM RNN number of parameters is given by L*(n*m + n^2 + n) where L:= Number of layers, n:= hidden units, m:= input vector length
",,,TaCoS,"Largest model is for image captioning:
Pretrained with ILSVRC 2021 (1.2M images)
Trained on 40k video-sentence pairs from TaCoS",4.00E+04,,,,,,,Reinforcement learning,,,,Academia,,,5/29/23 20:51
Deeply-supervised nets,Vision,Image classification,"University of California,Microsoft Research",Industry - Academia Collaboration (Academia leaning),"Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, Zhuowen Tu",9/18/14,Deeply-Supervised Nets,https://arxiv.org/abs/1409.5185,2.51E+03,"Highly cited,SOTA improvement",,,,,,"MNIST, CIFAR-10, CIFAR-100, SVHN","According to the paper, the Deeply-Supervised Nets (DSN) model was trained and evaluated on these image classification datasets:

MNIST - handwritten digits dataset with 60,000 training images and 10,000 test images.
CIFAR-10 - 60,000 32x32 color images across 10 classes, with 50,000 for training and 10,000 for testing.
CIFAR-100 - similar to CIFAR-10 but with 100 classes and 600 images per class.
SVHN - Street View House Numbers dataset with over 600,000 images of digits for training and 26,000 images for testing.",8.70E+05,60000+50000+60000+600000,,,,,,,,,,Industry,,,9/7/23 15:49
DSN,Vision,Image classification,"University of California,Microsoft Research",Industry - Academia Collaboration (Academia leaning),"Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, Zhuowen Tu",9/18/14,Deeply-Supervised Nets,https://arxiv.org/abs/1409.5185,2.51E+03,"Highly cited,SOTA improvement",,,,,,"MNIST, CIFAR-10, CIFAR-100, SVHN","According to the paper, the Deeply-Supervised Nets (DSN) model was trained and evaluated on these image classification datasets:

MNIST - handwritten digits dataset with 60,000 training images and 10,000 test images.
CIFAR-10 - 60,000 32x32 color images across 10 classes, with 50,000 for training and 10,000 for testing.
CIFAR-100 - similar to CIFAR-10 but with 100 classes and 600 images per class.
SVHN - Street View House Numbers dataset with over 600,000 images of digits for training and 26,000 images for testing.",8.70E+05,60000+50000+60000+600000,,,,,,,,,,Industry,,,8/15/23 13:52
Seq2Seq LSTM,Language,Translation,Google,Industry,"I Sutskever, O Vinyals, QV Le",9/10/14,Sequence to Sequence Learning with Neural Networks,https://arxiv.org/abs/1409.3215,1.57E+04,Highly cited,,1.92E+09,"The resulting LSTM has 384M parameters of which 64M are pure recurrent connections (32M for the “encoder” LSTM and 32M
for the “decoder” LSTM).
The paper uses an ensemble of 5 LSTMs.",5.60E+19,"384E+6 parameters * 2 FLOP/parameter * (348E+6 + 304E+6 points per epoch) * 7.5 epochs * 3 FLOP/point ~= 1.126656e+19 FLOP
Times 5 independent models in ensemble => 5.6E+19 FLOP

If we assume NVIDIA K40 (in use at the time): 10 days * 24 * 60 * 60 seconds/day * 8 GPUs * 33% * 5e12 FLOP/s * 5 models in ensemble ~= 5.7E+19 FLOP",WMT'14 dataset,,6.52E+08,"[WORDS]
""We used the WMT’14 English to French dataset. We trained our models on a subset of 12M sentences consisting of 348M French words and 304M English words, which is a clean “selected”
subset from [29].""",,,,,,,79.6,,,Industry,,,8/3/23 23:25
VGG16,Vision,,University of Oxford,Academia,Karen Simonyan; Andrew Zisserman,9/4/14,Very Deep Convolutional Networks for Large-Scale Image Recognition,https://arxiv.org/abs/1409.1556,6.13E+04,Highly cited,,1.38E+08,"Source: Table 2
https://arxiv.org/abs/1409.1556",9.25E+18,"2.5 weeks * 4 Titan Black GPUs * 0.30 utilization

Section 3.3: ""On a system equipped with
four NVIDIA Titan Black GPUs, training a single net took 2–3 weeks depending on the architecture.""



",ILSVRC-2012,,1.30E+06,"""In this section, we present the image classification results achieved by the described
ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012–2014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels).""",1.53E+10,"""Remarkably, although the depth is significantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 billion FLOPs).""

Source: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf",,,NVIDIA GTX Titan Black,,82.8,,,Academia,,,6/7/23 13:23
VGG19,Vision,,University of Oxford,Academia,"K Simonyan, A Zisserman",9/4/14,Very Deep Convolutional Networks for Large-Scale Image Recognition,https://arxiv.org/abs/1409.1556,6.13E+04,Highly cited,,1.44E+08,"Source: Table 2
https://arxiv.org/abs/1409.1556",,,ILSVRC-2012,,1.30E+06,"""In this section, we present the image classification results achieved by the described
ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012–2014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels).""",1.96E+10,"""Remarkably, although the depth is significantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 billion FLOPs).""

Source: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf",,,,,,,,Academia,,,5/29/23 20:51
RNNsearch-50*,Language,Translation,"Universite de Montréal,Jacobs University Bremen",Academia,"D Bahdanau, K Cho, Y Bengio",9/1/14,Neural Machine Translation by Jointly Learning to Align and Translate,https://arxiv.org/abs/1409.0473,1.92E+04,Highly cited,,,,1.56E+18,"From https://openai.com/blog/ai-and-compute/ Appendix.

0.018 pfs-days
(86400*10^15*0.018)

252 hours in a Quadro K-6000 GPU",WMT'14 + selection,,3.48E+08,"[WORDS]
""WMT ’14 contains the following English-French parallel corpora: Europarl (61M words), news
commentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively,
totaling 850M words. Following the procedure described in Cho et al. (2014a), we reduce the size of
the combined corpus to have 348M words using the data selection method by Axelrod et al. (2011).""",,,,,,,81.48,,,Academia,,,8/8/23 16:44
AdClickNet,,,Facebook,Industry,"X He, J Pan, O Jin, T Xu, B Liu, T Xu, Y Shi",8/24/14,Practical Lessons from Predicting Clicks on Ads at Facebook,https://dl.acm.org/doi/10.1145/2648584.2648589,5.86E+02,,,,,,,,,,,,,,,,,,,,Industry,,,9/7/23 16:41
SmooCT,Games,,University College London,Academia,"Johannes Heinrich, David Silver",7/1/14,Self-Play Monte-Carlo Tree Search in Computer Poker,https://www.semanticscholar.org/paper/Self-play-Monte-Carlo-tree-search-in-computer-poker-Heinrich-Silver/7b687599b4425aa959036071030e1212a3b359c7,1.60E+01,SOTA improvement,First RL system to achieve superhuman level at Poker Limit Texas Hold Em,,,6.90E+16,"""Each three-player agent was trained for about 12 billion episodes, requiring about 48 hours of training time [...] on a modern computer without using parallelization""

Assume an Intel i7 so 400e9 FLOP/s.
6.9e16 = 400e9*60*60*48",,,1.20E+10,"""Each three-player agentwas trained for about 12 billion episodes""

An episode seems to be a round of betting.",,,48,,,,,,,Academia,,,8/1/23 10:46
DeepFace,Vision,Face verification,"Tel Aviv University,Facebook",Industry - Academia Collaboration,"Y Taigman, M Yang, MA Ranzato",6/23/14,DeepFace: Closing the Gap to Human-Level Performance in Face Verification,https://ieeexplore.ieee.org/document/6909616,5.74E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/7/23 16:41
Multiresolution CNN,Video,Video classification,"Google,Stanford University",Industry - Academia Collaboration,"A Karpathy, G Toderici, S Shetty, T Leung",6/23/14,Large-Scale Video Classification with Convolutional Neural Networks,https://ieeexplore.ieee.org/document/6909619,5.90E+03,Highly cited,,1.26E+08,"""Using shorthand notation, the full [single frame] architecture is C(96, 11, 3)-N-P-C(256, 5, 1)-N-P-C(384, 3, 1)-C(384, 3, 1)-C(256, 3, 1)-P-FC(4096)-FC(4096), where C(d, f, s) indicates a convolutional layer with d filters of spatial size f ×f, applied to the input with stride s""

Two such single-frame architectures are concatenated as shown in figure 2

""Since the input is only of half the
spatial size as the full-frame models, we take out the last
pooling layer to ensure that both streams still terminate in a
layer of size 7×7×256. ""

We assume the input are T=10 frames with C=3 color channels each

2*(256*(10*3*5*5+1) + 384*(256*3*3+1) + 384*(384*3*3+1) + 256*(384*3*3+1)) + (2*7*7*256 + 1)*4096 + (4096+1)*4096



",,,,"""We further estimate the size of our dataset of sampled frames to be on the order of 50 million examples and that our networks have each seen approximately 500 million examples throughout the training period in total.""

So 5e+7 datapoints and 10 epochs.",5.00E+07,"""We further estimate the size of our dataset of sampled frames to be on the order of 50 million examples and that our networks have each seen approximately 500 million examples throughout the training period in total.""

So 5e+7 datapoints and 10 epochs.",,,,,,,,,,Industry,,,9/6/23 18:14
SPPNet,Vision,Image classification,"Microsoft,Xi’an Jiaotong University,University of Science and Technology of China",Industry - Academia Collaboration,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",6/18/14,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,https://arxiv.org/abs/1406.4729,7.41E+03,Highly cited,,,,3.41E+18,"""All networks in this paper can be
trained on a single GeForce GTX Titan GPU (6 GB memory) within two to four weeks.""
4.7e12 FLOP/s * 4* 7*24*60*60 seconds * 0.3 utilisation",Imagenet-1k,,1.28E+06,"Section 3.1: ""We train the networks on the 1000-category training
set of ImageNet 2012.""",,,672,"""All networks in this paper can be trained on a single GeForce GTX Titan GPU (6 GB memory) within two to four weeks.""",NVIDIA GeForce GTX TITAN,,65.07,,,Industry,,,8/1/23 10:30
GANs,Drawing,Image generation,Universite de Montréal,Academia,"Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio",6/10/14,Generative Adversarial Networks,https://arxiv.org/abs/1406.2661,3.69E+04,Highly cited,,,The paper outlines the G-D framework but doesn't provide information about the structures of their generator and discriminator.,5.18E+17,"From https://openai.com/blog/ai-and-compute/ Appendix

""Less than 0.006 pfs-days""
(86400*10^15*0.006)

Seems extremely speculative, unless someone at OpenAI privately corresponded with the authors. There is no information about compute or training in the GANs paper.",CIFAR-10,,6.00E+04,"""We trained adversarial nets an a range of datasets including MNIST[23], the Toronto Face Database (TFD) [28], and CIFAR-10 [21].""

MNIST has 60k images 
https://en.wikipedia.org/wiki/MNIST_database

TFD seems to have 2925 examples (?)
https://www.cs.toronto.edu/~urtasun/courses/CSC411/hw3-411.pdf

CIFAR-10 has 60k images
https://www.cs.toronto.edu/~kriz/cifar.html

",,,,,,,6.09,,,Academia,Speculative,,8/8/23 16:33
Two-stream ConvNets for action recognition,Video,Video classification,University of Oxford,Academia,"Karen Simonyan, Andrew Zisserman",6/9/14,Two-Stream Convolutional Networks for Action Recognition in Videos,https://arxiv.org/abs/1406.2199,6.22E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/7/23 16:52
GRUs,Language,,"University of Montreal, Jacobs University, University du Maine",Academia,"K Cho, B Van Merriënboer, C Gulcehre",6/3/14,Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation,https://arxiv.org/abs/1406.1078,1.50E+04,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
Dropout (2014),,,University of Toronto,Academia,"Nitish Shrivasta, Geoffrey Hinton, Alex Krizhevsky",6/1/14,Dropout: A Simple Way to Prevent Neural Networks from Overfitting,https://jmlr.org/papers/v15/srivastava14a.html,3.19E+04,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/7/23 16:56
HyperNEAT,Games,Atari Games,University of Texas,Academia,"M Hausknecht, J Lehman",3/5/14,A Neuroevolution Approach to General Atari Game Playing,https://ieeexplore.ieee.org/abstract/document/6756960,1.95E+02,,,2.40E+05,"The ANN consists of three layers (Fig. 3): a substrate layer inwhich information from the game screen (raw pixels, objects, ornoise) is given as input to the network; a processing layer whichadds a nonlinear internal representation; and a nonlinear outputlayer from which actions are read and conveyed to the Atari em-ulator. Both the input and output layers are fully connected tothe processing layer. The substrate dimensionality of the inputand processinglayers is 810 in the case of the object repre-sentation and 1621 for the pixel and noise representations.3The output layer consists of a 33 substrate mirroring the ninepossible directions of the Atari 2600 joystick and a single noderepresenting thefire button",,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
DBN for NLP,Language,Text classification,"Microsoft,University of Toronto",Industry - Academia Collaboration,"R Sarikaya, GE Hinton, A Deoras",2/11/14,Application of Deep Belief Networks for Natural Language Understanding,https://ieeexplore.ieee.org/document/6737243,4.45E+02,,,1.02E+06,"Assuming 1000 input features, 35 classes and 3 hidden layers of 500 units each",,,,,1.78E+05,The training data has 27K automatically transcribed utterances amounting to 178K words.,,,,,,,,,,Industry,,,9/8/23 1:20
GloVe (32B),Language,Semantic embedding,Stanford University,Academia,"J Pennington, R Socher, CD Manning",1/1/14,GloVe: Global Vectors for Word Representation,https://nlp.stanford.edu/projects/glove/,2.25E+04,Highly cited,,1.20E+08,400k vocab * 300 vector dimensions,,"""The total run-time is split between populating X
and training the model. The former depends on
many factors, including window size, vocabulary
size, and corpus size. Though we did not do so,
this step could easily be parallelized across multiple machines (see, e.g., Lebret and Collobert
(2014) for some benchmarks). Using a single
thread of a dual 2.1GHz Intel Xeon E5-2658 machine, populating X with a 10 word symmetric
context window, a 400,000 word vocabulary, and
a 6 billion token corpus takes about 85 minutes.
Given X, the time it takes to train the model depends on the vector size and the number of iterations. For 300-dimensional vectors with the above settings (and using all 32 cores of the above machine), a single iteration takes 14 minutes. See Fig. 4 for a plot of the learning curve""

""We run 50 iterations for vectors smaller than
300 dimensions, and 100 iterations otherwise (see
Section 4.6 for more details about the convergence
rate).""

But we are interested in the 42B token model",Common Crawl,,4.20E+10,"""We trained our model on five corpora of varying sizes: a 2010 Wikipedia dump with 1 billion tokens; a 2014 Wikipedia dump with 1.6 billion tokens; Gigaword 5 which has 4.3 billion tokens; the combination Gigaword5 + Wikipedia2014, which has 6 billion tokens; and on 42 billion tokens of web data, from Common Crawl

[To demonstrate the scalability of the model, we also trained it on a much larger sixth corpus, containing 840 billion tokens of web data, but in this case we did not lowercase the vocabulary, so the results are not directly comparable.]""",N/A,Embeddings are precalculated,,"Section 4.6 in original paper (https://nlp.stanford.edu/pubs/glove.pdf)

85 min to populate coocurrence matrix
+ 25 training iterations

Each iteration takes 14 minutes on 32 cores ",,,,,,Academia,,,9/6/23 18:15
GloVe (6B),Language,Semantic embedding,Stanford University,Academia,"J Pennington, R Socher, CD Manning",1/1/14,GloVe: Global Vectors for Word Representation,https://nlp.stanford.edu/projects/glove/,2.25E+04,Highly cited,,1.20E+08,400k vocab * 300 vector dimensions,,"""The total run-time is split between populating X
and training the model. The former depends on
many factors, including window size, vocabulary
size, and corpus size. Though we did not do so,
this step could easily be parallelized across multiple machines (see, e.g., Lebret and Collobert
(2014) for some benchmarks). Using a single
thread of a dual 2.1GHz Intel Xeon E5-2658 machine, populating X with a 10 word symmetric
context window, a 400,000 word vocabulary, and
a 6 billion token corpus takes about 85 minutes.
Given X, the time it takes to train the model depends on the vector size and the number of iterations. For 300-dimensional vectors with the above settings (and using all 32 cores of the above machine), a single iteration takes 14 minutes. See Fig. 4 for a plot of the learning curve""

""We run 50 iterations for vectors smaller than
300 dimensions, and 100 iterations otherwise (see
Section 4.6 for more details about the convergence
rate).""

Details of dual 2.1GHz Intel Xeon E5-2658 machine:
https://www.intel.com/content/www/us/en/products/sku/61428/intel-xeon-processor-e52658-20m-2-10-ghz-8-0-gts-intel-qpi/specifications.html",Gigaword5 + Wikipedia2014,,6.00E+09,"""We trained our model on five corpora of varying sizes: a 2010 Wikipedia dump with 1 billion tokens; a 2014 Wikipedia dump with 1.6 billion tokens; Gigaword 5 which has 4.3 billion tokens; the combination Gigaword5 + Wikipedia2014, which has 6 billion tokens; and on 42 billion tokens of web data, from Common Crawl

[To demonstrate the scalability of the model, we also trained it on a much larger sixth corpus, containing 840 billion tokens of web data, but in this case we did not lowercase the vocabulary, so the results are not directly comparable.]""",N/A,Embeddings are precalculated,,"Section 4.6 in original paper (https://nlp.stanford.edu/pubs/glove.pdf)

85 min to populate coocurrence matrix
+ 25 training iterations

Each iteration takes 14 minutes on 32 cores ",,,,,,Academia,,,9/6/23 18:15
OverFeat,Vision,Image classification,New York University,Academia,"Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, Yann LeCun",12/21/13,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",https://arxiv.org/abs/1312.6229,5.15E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/7/23 16:57
Image generation,Vision,Image clustering,Univeristy of Amsterdam,Academia,"DP Kingma, M Welling",12/20/13,Auto-Encoding Variational Bayes,https://arxiv.org/abs/1312.6114,1.56E+04,Highly cited,,,,4.75E+14,"From https://openai.com/blog/ai-and-compute/ Appendix

""less than 0.0000055 pfs-days""
(86400*10^15*0.0000055)",MNIST,,6.00E+04,"""We trained generative models of images from the MNIST and Frey Face datasets""

MNIST has 60k images
https://en.wikipedia.org/wiki/MNIST_database

Frey Face has 2k images
https://cs.nyu.edu/~roweis/data.html",,,,,,,0.01,,,Academia,,,6/8/23 0:39
DQN,Games,Atari,DeepMind,Industry,"V Mnih, K Kavukcuoglu, D Silver, A Graves",12/19/13,Playing Atari with Deep Reinforcement Learning,https://arxiv.org/abs/1312.5602,6.68E+03,Highly cited,,8.36E+05,"The input to the neural network consists is an 84 × 84 × 4 image produced by φ. The first hidden layer convolves 16 8 × 8 filters with stride 4 with the input image and applies a rectifier nonlinearity [10, 18]. The second hidden layer convolves 32 4 × 4 filters with stride 2, again followed by a rectifier nonlinearity. The final hidden layer is fully-connected and consists of 256 rectifier units. The output layer is a fully connected linear layer with a single output for each valid action. The number of valid actions varied between 4 and 18 on the games we considered.",2.30E+15,"Network is 84x84x3 input, 16, 8x8, stride 4, 32 4x4 stride 2, 256 fully connected
First layer: 20*20*3*16*8*8 = 1.23M add-multiplies
Second layer: 9*9*16*32*4*4 = 0.66M add-multiplies
Third layer: 9*9*32*256 = 0.66M add-mutliplies
Total ~ 2.55M add-multiplies
2.5 MFLOPs * 5M updates * 32 batch size * 2 multiply-add * 3 backward pass
= 2.3 PF = 2.7e-5 pfs-days

",,,,,,,,,,,0.04,,,Industry,,,6/14/23 14:57
Network in Network,,,National University of Singapore,Academia,"M Lin, Q Chen, S Yan",12/16/13,Network In Network,https://arxiv.org/abs/1312.4400,5.50E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
DBLSTM,Speech,Speech recognition,Univeristy of Toronto,Academia,"A Graves, N Jaitly, A Mohamed",12/8/13,Hybrid speech recognition with Deep Bidirectional LSTM,https://ieeexplore.ieee.org/document/6707742,1.46E+03,Highly cited,,2.99E+07,"""The DBLSTM network had five bidirectional hidden levels, with 500 LSTM cells in each of the forward and backward
layers, and a size 3385 softmax output layer, giving a total of
29.9M weights.""",,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
TransE,Other,Entity embedding,"CNRS,Google",Industry - Academia Collaboration,"Antoine Bordes, Nicolas Usunier, Alberto Garcia- Duran, Jason Weston, and Oksana Yakhnenko",12/5/13,Translating Embeddings for Modeling Multi- relational Data,https://papers.nips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html,7.04E+03,Highly cited,,,,1.34E+18,"8 GPUs (they don''t specify which, so I used the average for FP32 for 2017 from the write-up table)
8 hours 
0.33 util rate
",,,1.70E+07,"""it can be successfully trained on a large scale data set with 1M
entities, 25k relationships and more than 17M training samples""",,,,,,,17.58,,,Industry,,,8/3/23 20:32
TensorReasoner,,,Stanford University,Academia,"R Socher, D Chen, CD Manning, A Ng",12/1/13,Reasoning With Neural Tensor Networks for Knowledge Base Completion,https://papers.nips.cc/paper/2013/hash/b337e84de8752b27eda3a12363109e80-Abstract.html,1.66E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/12/23 18:01
Visualizing CNNs,Vision,,New York University,Academia,"MD Zeiler, R Fergus",11/12/13,Visualizing and Understanding Convolutional Networks,https://arxiv.org/abs/1311.2901,1.30E+04,Highly cited,,,,5.32E+17,"1 GPU * 12 days * 1.54 TFLOPS/GTX 580 * 0.33 utilization 
= 532 PF = 0.0062 pfs-days

Source: https://openai.com/blog/ai-and-compute",,,,,,,,,NVIDIA GeForce GTX 580,,9.02,,,Academia,,,9/12/23 22:07
R-CNN (T-net),Vision,Object detection,UC Berkeley,Academia,"Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik",11/11/13,Rich feature hierarchies for accurate object detection and semantic segmentation,https://arxiv.org/abs/1311.2524,1.91E+04,Highly cited,,6.90E+07,"Computed from architecture description in Caffee

https://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/detection.ipynb",,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
Word2Vec (large),Language,Semantic embedding,Google,Industry,"T Mikolov, I Sutskever, K Chen, GS Corrado",10/16/13,Distributed Representations of Words and Phrases and their Compositionality,https://arxiv.org/abs/1310.4546,2.87E+04,Highly cited,,6.92E+08,"To maximize the accuracy on the phrase analogy task, we increased the amount of the training data by using a dataset with about 33 billion words. We used the hierarchical softmax, dimensionality of 1000, and the entire sentence for the context.",3.89E+16,"From https://openai.com/blog/ai-and-compute/ Appendix.

""less than 0.00045 pfs days""
(86400*10^15*0.00045)",,,3.30E+10,"""For training the Skip-gram models, we have used a large dataset consisting of various news articles (an internal Google dataset with one billion words). We discarded from the vocabulary all words that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K""",,,,,,,0.55,,,Industry,,,6/14/23 14:54
Word2Vec (small),Language,Semantic embedding,Google,Industry,"T Mikolov, I Sutskever, K Chen, GS Corrado",10/16/13,Distributed Representations of Words and Phrases and their Compositionality,https://arxiv.org/abs/1310.4546,2.87E+04,Highly cited,,2.08E+08,"We discarded from the vocabulary all words
that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K [...] Starting with the same news data as in the previous experiments, we first constructed the phrase
based training corpus and then we trained several Skip-gram models using different hyperparameters. As before, we used vector dimensionality 300 and context size 5.",,,,,6.92E+05,"""For training the Skip-gram models, we have used a large dataset consisting of various news articles (an internal Google dataset with one billion words). We discarded from the vocabulary all words that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K""",,,,,,,,,,Industry,,,6/8/23 0:39
Mitosis,Vision,,IDSIA,Academia,"Dan C. Cireşan, Alessandro Giusti, Luca M. Gambardella, Jürgen Schmidhuber",9/22/13,Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks,https://link.springer.com/chapter/10.1007/978-3-642-40763-5_51,1.46E+03,ICPR 2012 mitosis detection competition winner,,3.72E+04,Sum numbers of weights in Table 1.b,1.37E+17,"""Training each network requires one day of computation with an optimized GPU
implementation""

Assuming 1.58E+12 FLOP/second on FP32 (from the table in the Estimating compute post), we get

3600*24*1.58E+12 = 1.37E+17 FLOP",,,1.00E+06,"The dataset is built in two stages. First a classifier is trained on small sample, and used to curate a more representative larger dataset.

The final dataset has 1M instances

""We build the actual training set, composed by 1 million instances, which includes
all mitosis pixels (6.6% of the training instances). The remaining 95.4% is sampled
from non-mitosis pixels by assigning to each pixel p a weight D(p).""",,,,,,,2,,,Academia,,,5/29/23 20:51
Image Classification with the Fisher Vector: Theory and Practice,Vision,Image Classification,"Universidad Nacional de Cordoba,Xerox Research Centre Europe,Inteligent Systems Lab Amsterdam,University of Amsterdam,LEAR Team,INRIA Grenoble",Industry - Academia Collaboration,"orge Sanchez, Florent Perronnin, Thomas Mensink, Jakob Verbeek",6/12/13,Image Classification with the Fisher Vector: Theory and Practice,https://hal.inria.fr/hal-00830491v2/document,1.71E+03,Highly cited,,,,9.08E+13,"They use a Intel Xeon E5-2470 Processor for 2 hours. This can do 12,617 MOps/Sec 
https://www.cpubenchmark.net/cpu.php?cpu=Intel+Xeon+E5-2470+%40+2.30GHz&id=2003",ImageNet,,,,,,2,,,,0,,,,,,6/8/23 0:39
SemVec,Language,,Microsoft Research,Industry,"T Mikolov, W Yih, G Zweig",6/9/13,Linguistic Regularities in Continuous Space Word Representations,https://www.aclweb.org/anthology/N13-1090/,3.63E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/12/23 18:02
SearchFusion,Vision,Object detection,"Univeristy of Trento,University of Amsterdam",Academia,"JRR Uijlings, KEA Van De Sande, T Gevers",4/2/13,Selective search for object recognition,https://link.springer.com/article/10.1007/s11263-013-0620-5,5.59E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/12/23 18:05
PreTrans-3L-250H,Speech,Speech recognition,Univeristy of Toronto,Academia,"Alex Graves, Abdel-rahman Mohamed, Geoffrey Hinton",3/22/13,Speech Recognition with Deep Recurrent Neural Networks,https://arxiv.org/abs/1303.5778,7.79E+03,Highly cited,,4.30E+07,Table 1,,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
Maxout Networks ,Vision,Image classification,University of Montreal,Academia," Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, Yoshua Bengio",2/18/13,Maxout Networks ,https://arxiv.org/abs/1302.4389,2.58E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,6/8/23 0:39
Textual Imager,Vision,,Stanford University,Academia,"R Socher, M Ganjoo, H Sridhar, O Bastani",1/16/13,Zero-Shot Learning Through Cross-Modal Transfer,https://arxiv.org/abs/1301.3666,1.21E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/12/23 18:03
RNN (SGD+CLR) (music),Audio,,Universite de Montréal,Academia,"Yoshua Bengio, Nicolas Boulanger-Lewandowski and Razvan Pascanu",12/14/12,Advances in Optimizing Recurrent Networks,https://arxiv.org/pdf/1212.0901.pdf,6.47E+02,,,1.96E+05,"It uses 400 hidden units (selected via hyperparameter tuning)
The input size is 88 (corresponding to the 88 piano pitches)
It uses rectified linear units, so no activation function parameters
So the number of parameters would be:
Input to hidden weights: 88 * 400 = 35,200
Hidden to hidden weights: 400 * 400 = 160,000
Biases: 400
Total: ~195,600 parameters
Above estimate is by Claude 2. Should be checked manually.",,,,,,,,,,,,,,,,,Speculative,"After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modeling sequences, their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges
with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment. The experiments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error",9/14/23 19:24
Bayesian automated hyperparameter tuning,,,"University of Toronto,University of Sherbrooke,Harvard University",Academia,"J Snoek, H Larochelle, RP Adams",12/2/12,Practical Bayesian optimization of machine learning algorithms,https://arxiv.org/abs/1206.2944,5.67E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/12/23 18:07
AlexNet,Vision,Image classification,University of Toronto,Academia,"Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton",9/30/12,ImageNet Classification with Deep Convolutional Neural Networks,https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html,8.51E+04,Highly cited,,6.00E+07,Our neural network architecture has 60 million parameters.,4.70E+17,"1.2M images * 90 epochs * 0.75 GFLOP * (2 add-multiply) * (3 backward pass) 
= 470 PF = 0.0054 pfs-days

Source: https://openai.com/blog/ai-and-compute/",ImageNet,,1.20E+06,"""ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories. The images were collected from the web and labeled by human labelers using Amazon’s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of 1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images.""",,,,,NVIDIA GeForce GTX 580,,8,,,Academia,,,6/7/23 13:21
Context-dependent RNN,Language,Language modelling,"Microsoft Research,Brno University of Technology",Industry - Academia Collaboration (Industry leaning),"Tomas Mikolov, Geoffrey Zweig",7/27/12,Context Dependent Recurrent Neural Network Language,https://www.microsoft.com/en-us/research/wp-content/uploads/2012/07/rnn_ctxt_TR.sav_.pdf,7.07E+02,SOTA Improvement,New SOTA perplexity on PTB,3.14E+06,Calculated by Claude 2. Should be manually verified.,,,,,,,,,,,,,,,,Industry,Unverified,"Recurrent neural network language models (RNNLMs) have recently demonstrated state-of-the-art performance across a variety of tasks. In this paper, we improve their performance by providing a contextual real-valued input vector in association with each word. This vector is used to convey contextual information about the sentence being modeled. By performing Latent Dirichlet Allocation using a block of preceding text, we achieve a topic-conditioned RNNLM. This approach has the key advantage of avoiding the data fragmentation associated with building multiple topic models on different data subsets. We report perplexity results on the Penn Treebank data, where we achieve a new state-of-the-art. We further apply the model to the Wall Street Journal speech recognition task, where we observe 
improvements in word-error-rate.",9/7/23 17:15
MV-RNN,Language,Text classification,Stanford University,Academia,"R. Socher, B. Huval, C. D. Manning, and A. Y. Ng",7/12/12,Model,https://www.aclweb.org/anthology/D12-1110/,1.46E+03,Highly cited,,3.51E+06,"""We represent a word as both a continuous vector and a matrix of parameters. We initialize all word vectors x ∈ Rn with pre-trained 50-dimensional word vectors from the unsupervised model of Collobert and Weston (2008). [...] Every word is also associated with a matrix X.  [...] If the vectors have dimensionality n, then each word’s matrix has dimensionality X ∈ Rn×n.""

""We propose the following combination function which is input dependent:
p = fA,B(a, b) = f(Ba, Ab) = g(W x (Ba Ab)) ,(2)
where A, B are matrices for single words, the global W ∈ Rn×2n is a matrix that maps both transformed words back into the same n-dimensional space.""

""For computing nonterminal phrase matrices, we define the function
P = fM(A, B) = WMA, B, (3)
where WM ∈ Rn×2n, so P ∈ Rn×n just like each input matrix.""

""If every word is represented by an n-dimensional vector and additionally by an n × n matrix, the dimensionality of the whole model may become too large with commonly used vector sizes of n = 100. In order to reduce the number of parameters, we represent word matrices by the following low-rank plus diagonal approximation: A = UV + diag(a), (5)
where U ∈ Rn×r, V ∈ Rr×n, a ∈ Rnand we set the rank for all experiments to r = 3.""

""We train these representations by adding on top of each parent node a simple softmax classifier
to predict a class distribution over, e.g., sentiment or relationship classes: d(p) = softmax(Wlabelp). If there are K labels, then d ∈ RK is a K-dimensional multinomial distribution""

In total there are V*(n+n*r + r*n) + n*2n + n*2n + (n+1)*k parameters, where n is the vector dimension, r is the low-rank decomposition dimension, V is the vocabulary size and k is the number of classes.

In the experiments we have that n=50, r=3, k=? and V=?. I'm guesstimating k=5 and V=10k.",,,,,,,,,,,,,,,,Academia,,,9/7/23 17:15
Unsupervised High-level Feature Learner,Vision,Image classification,Google,Industry,"Quoc V. Le, Marc'Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff Dean, Andrew Y. Ng",7/12/12,Building High-level Features Using Large Scale Unsupervised Learning,https://arxiv.org/abs/1112.6209,2.91E+03,"SOTA Improvement,Highly cited","""we trained our network to obtain 15.8% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative improvement over the previous state-of-the-art""",1.00E+09,"""To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet)""",6.00E+17,"Assuming 1 epoch, 10 million images and 1 billion parameters, 6*N*D = 6*10^17 FLOP",,10 million 200x200 images extracted from Youtube videos,1.00E+07,10 million 200x200 images extracted from Youtube videos,,,72,"""We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. """,,Unsupervised,,Hardware not reported,,Industry,Likely,"We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images using unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200×200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting with these learned features, we trained our network to obtain 15.8% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative improvement over the previous state-of-the-art.",9/20/23 19:59
Ngram corpus,Language,,Google,Industry,"Yuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden, Jon Orwant, Will Brockman and Slav Petrov",7/8/12,Syntactic Annotations for the Google Books NGram Corpus,https://aclanthology.org/P12-3029/,4.89E+02,,,,,,,,,,,,,,,,,,,,Industry,,,9/12/23 18:07
Dropout (CIFAR),Vision,Character recognition,University of Toronto,Academia,"GE Hinton, N Srivastava, A Krizhevsky",6/3/12,Improving neural networks by preventing co-adaptation of feature detectors,https://arxiv.org/abs/1207.0580,6.68E+03,Highly cited,,,,4.27E+15,"""a single NVIDIA GTX 580 GPU. Training on CIFAR-10 takes roughly 90 minutes"" p17
1.581 TFLOP/s * 90 min * 60 s/min * 0.5 utilization",CIFAR-10,,,,,,1.5,90 minutes,NVIDIA GeForce GTX 580,,,,,Academia,,,8/9/23 20:38
Dropout (ImageNet),Vision,Image classification,University of Toronto,Academia,"GE Hinton, N Srivastava, A Krizhevsky",6/3/12,Improving neural networks by preventing co-adaptation of feature detectors,https://arxiv.org/abs/1207.0580,6.68E+03,Highly cited,,,"We achieved comparable performance of 48.6% error using a single neural network with
five convolutional hidden layers interleaved with “max-pooling” layer followed by two globally
connected layers and a final 1000-way softmax layer",2.73E+17,"""a single NVIDIA GTX 580 GPU... Training on ImageNet takes
roughly four days with dropout and two days without.""
1.581 TFLOP/s * 4 day * 86400 s/day * 0.5 utilization",ImageNet,,1.00E+06,"In 2010, a subset of 1000 classes
with roughly 1000 examples per class was the basis of an object recognition competition...",,,96,4 days with dropout; 2 days without dropout,NVIDIA GeForce GTX 580,,,,,Academia,,,8/9/23 20:39
Dropout (MNIST),Vision,Character recognition,University of Toronto,Academia,"GE Hinton, N Srivastava, A Krizhevsky",6/3/12,Improving neural networks by preventing co-adaptation of feature detectors,https://arxiv.org/abs/1207.0580,6.68E+03,Highly cited,,5.59E+06,,6.04E+15,"Num mul-add / forward pass
2 FLOPs / mult-add
3 total mult-add / fp mult-add
3000 epochs
60000 training samples",MNIST,,6.00E+04,"The MNIST database contains 60,000 training images and 10,000 testing images (Wikipedia)",1.12E+07,"mult-add / fp
* 2 FLOPs / mult-add",,,NVIDIA GeForce GTX 580,,0.1,,,Academia,,,6/7/23 13:19
Dropout (TIMIT),Speech,Speech recognition,University of Toronto,Academia,"GE Hinton, N Srivastava, A Krizhevsky",6/3/12,Improving neural networks by preventing co-adaptation of feature detectors,https://arxiv.org/abs/1207.0580,6.68E+03,Highly cited,,4.88E+07,The input to the net is 21 adjacent frames with an advance of 10ms per frame. The neural net has 4 fully-connected hidden layers of 4000 units per layer and 185 “softmax” output units that are subsequently merged into the 39 distinct classes used for the benchmark.,,,TIMIT,,4.16E+04,"4162 utterances, guesstimated avg 10 words per utterance",,,,,NVIDIA GeForce GTX 580,,,,,Academia,,,6/7/23 13:19
MCDNN (MNIST),Vision,Character recognition,IDSIA,Academia,"D Ciregan, U Meier, J Schmidhuber",2/13/12,Multi-column Deep Neural Networks for Image Classification,https://arxiv.org/abs/1202.2745v1,4.83E+03,Highly cited,,1.99E+06,"We train five DNN columns per normalization, resulting in a total of 35 columns for the entire MCDNN.
[Each DNN has an architecture] 1x29x29-20C4-MP2-40C5-MP3-150N-10N DNN ",3.73E+15,"Num of multiply-adds per forward pass
2 FLOPs/mult-add
3 (fp+bp FLOPs / fp FLOPs)
800 epochs
60.000 training size
35 networks

""Training a DNN takes almost 14 hours and after 500 training epochs little additional improvement is observed""",MNIST,,6.00E+04,"The MNIST database contains 60,000 training images and 10,000 testing images (Wikipedia)",2.59E+07,"Num mult-add per fp per network
35 networks
2 FLOPs/mult-add",,,,,0.08,,,Academia,,,8/9/23 20:39
HOGWILD!,,,University of Wisconsin Madison,Academia,"F Niu, B Recht, C Ré, SJ Wright",11/11/11,HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent,https://arxiv.org/abs/1106.5730,2.12E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/12/23 18:08
NLP from scratch,Language,,"NEC Laboratories, Princeton",Industry - Academia Collaboration,"Ronan Collobert, J. Weston, L. Bottou, Michael Karlen, K. Kavukcuoglu, P. Kuksa",11/8/11,Natural Language Processing (Almost) from Scratch,https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf,7.64E+03,Highly cited,,5.00E+06,"""The capacity of our network architectures lies mainly in the word lookup table, which contains 50 × 100,000 parameters to train. [...] most of the trainable parameters are located in the lookup tables.""",,,,,8.52E+08,"""Section 4 leverages large unlabeled data sets (∼ 852 million words)""",,,,,,,,,,Industry,,,5/29/23 20:51
Domain Adaptation,Vision,Object Recognition,"University of Maryland,College Park",Academia,"Raghuraman Gopalan, Ruonan Li, Rama Chellappa",11/6/11,Domain Adaptation for Object Recognition: An Unsupervised Approach,http://ftp.idiap.ch/pub/courses/EE-700/material/05-12-2012/2011_ICCV_DomainAdaptation.pdf,1.06E+03,Highly cited,,1.53E+04,"Did not take into account initial image feature extraction, only novel stuff.

1. Perform PCA on the feature matrices from both domains. Learnable parameters are projection matrices.
= 800 (# features) x 200 (reduced dimension) x 2 (once per subdomain)

2. Perform partial least squares regression. Learnable parameters are

Matrix P with dimensions 200 (# features) x 30 (dimension of latent space)
Matrix Q with dimensions 1 (# responses) x 30 (dimension of latent space)
Projection matrix of X onto latent space:  200 (# features) x 30 (dimension of latent space)
Projection matrix of Y onto latent space:  1 (# responses) x 30 (dimension of latent space)
",,,Dataset introduced in 'Adapting Visual Category Models to New Domains',,4.65E+03,"Dataset introduced in 'Adapting Visual Category Models to New
Domains'",,,,,,Supervised,,,,Academia,,,6/8/23 0:39
Adaptive Subgrad,,,"Univeristy of California Berkley,Technion- Israel Institute of Technology,Google",Industry - Academia Collaboration,"J Duchi, E Hazan, Y Singer",10/3/11,Adaptive Subgradient Methods for Online Learning and Stochastic Optimization,https://dl.acm.org/doi/10.5555/1953048.2021068,8.81E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/12/23 18:09
Bayesian Starcraft,Games,,Collège de France,Academia,"G Synnaeve, P Bessiere",8/31/11,A Bayesian Model for RTS Units Control applied to StarCraft,https://ieeexplore.ieee.org/document/6032006,8.60E+01,,,1.31E+04,"It's a bayes net, parameters are probabilty tables for probability that X happens in direction i given that we go in direction i. There are 25 directions.",,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
Recursive sentiment autoencoder,Language,,Stanford University,Academia,"R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning",7/1/11,Semi-supervised recursive autoencoders for predicting sentiment distributions,https://aclanthology.org/D11-1014/,1.48E+03,Highly cited,,,,,,,,,"They use several datasets for self-supervised and supervised learning
",,,,,,,,,,Academia,,,9/12/23 18:11
Cross-Lingual POS Tagger,Language,Part-of-speech tagging,"Carnegie Mellon University,Google Research",Industry - Academia Collaboration,"Dipanjan Das, Slav Petrov",6/19/11,Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections,https://aclanthology.org/P11-1061/,3.16E+02,,,,,,,,,,,,,,,,,,,,Industry,,,9/12/23 18:12
RNN-SpeedUp,Language,,"Brno University of Technology,Johns Hopkins University",Academia,"T. Mikolov, S. Kombrink, L. Burget, J. Cernock ˇ y, and S. Khudanpur",5/22/11,Extensions of recurrent neural network language model,https://ieeexplore.ieee.org/document/5947611,1.24E+03,Highly cited,,,,,,Penn Tree Bank,,6.98E+05,"Section 3: ""The data used in the following experiments were obtained from
Penn Tree Bank: sections 0-20 were used as training data (about
930K tokens)""

0.75 words per token for English",,,,,,,,,,Academia,,,9/12/23 18:13
Deep rectifier networks,,,University of Montreal,Academia,"X Glorot, A Bordes, Y Bengio",4/13/11,Deep sparse rectifier neural networks,http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf,7.22E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/7/23 17:05
Optimized Single-layer Net,,,"University of Michigan,Stanford University",Academia,"A Coates, A Ng, H Lee",4/11/11,An analysis of single-layer networks in unsupervised feature learning,http://proceedings.mlr.press/v15/coates11a.html,2.66E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/12/23 18:14
Culturome,Language,,Harvard University,Industry - Academia Collaboration,"JB Michel, YK Shen, AP Aiden, A Veres, MK Gray",12/16/10,Quantitative Analysis of Culture Using Millions of Digitized Books,https://science.sciencemag.org/content/331/6014/176,2.27E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/12/23 22:32
KN5 LM + RNN 400/10 (WSJ),Speech,Transcription,"Brno University of Technology, Johns Hopkins University",Academia,"T. Mikolov, M. Karafiat, L. Burget, J. Cernock ´ y, and S. Khudanpur",9/26/10,Recurrent neural network based language model.,https://www.researchgate.net/publication/221489926_Recurrent_neural_network_based_language_model,5.67E+03,Highly cited,,8.00E+07,"""- size of vector x is equal to
size of vocabulary V (this can be in practice 30 000 − 200 000)
plus size of context layer. Size of context (hidden) layer s is
usually 30 − 500 hidden units.""

""In further experiments, we denote modified Kneser-Ney
smoothed 5-gram as KN5. Configurations of neural network
LMs, such as RNN 90/2, indicate that the hidden layer size is
90 and threshold for merging words to rare token is 2.""",6.14E+16,"""Convergence is usually
achieved after 10-20 epochs.""

Assuming a backward-forward ratio of 2:1, since this is a shallow network",WSJ,,6.40E+06,"The training corpus consists of 37M words from NYT section of English Gigaword. As it is very time consuming to train
RNN LM on large data, we have used only up to 6.4M words
for training RNN models (300K sentences) - it takes several
weeks to train the most complex models",1.60E+08,Roughly twice the number of parameters,,,,,2.03,,,Academia,,,5/29/23 20:51
RNN 500/10 + RT09 LM (NIST RT05),Speech,Transcription,"Brno University of Technology,Johns Hopkins University",Academia,"T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, and S. Khudanpur",9/26/10,Recurrent neural network based language model.,https://www.researchgate.net/publication/221489926_Recurrent_neural_network_based_language_model,5.67E+03,Highly cited,,5.27E+06,"""Size of context (hidden) layer s is usually 30 − 500 hidden units.""

""The acoustic HMMs are based on cross-word tied-states triphones trained discriminatively using MPE criteria. Feature extraction use 13 Mel-PLP’s features with deltas, double and triple deltas reduced by HLDA to 39-dimension feature vector""

10k words vocabulary

(39+500)*500 + 500*10000
",3.41E+15,"""Convergence is usually achieved after 10-20 epochs.""

Assuming a backward-forward ratio of 2:1, since this is a shallow network",NIST RT05,,5.40E+06,"""Table 4: Comparison of very large back-off LMs and RNN LMs
trained only on limited in-domain data (5.4M words).""",1.05E+07,Roughly twice the number of parameters,,,,,0.11,,,Academia,,,8/15/23 15:26
YouTube Video Recommendation System,Recommendation,,Google,Industry,"J Davidson, B Liebald, J Liu, P Nandy",9/26/10,The YouTube Video Recommendation System,https://dl.acm.org/doi/10.1145/1864708.1864770,1.07E+03,Highly cited,,,,,,,,1.00E+10,"""We currently handle millions of users
and tens of billions of activity events with a total footprint
of several terabytes of data""

If 10M users each watch 1000 videos, that's 10B visualizations, which matches their ""activity events"" count.",,,,,,,,,,Industry,,,9/12/23 18:21
Fisher-Boost,Vision,,Xerox Research Centre Europe (XRCE),Industry,Florent PerronninJorge SánchezThomas Mensink,9/5/10,Improving the Fisher Kernel for Large-Scale Image Classification,https://link.springer.com/chapter/10.1007/978-3-642-15561-1_11,3.06E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/12/23 18:31
RBM-tuning,,,University of Toronto,Academia,GE Hinton,8/2/10,A practical guide to training restricted boltzmann machines,https://link.springer.com/chapter/10.1007/978-3-642-35289-8_32,3.34E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/12/23 18:32
ReLU (LFW),Vision,Face recognition,University of Toronto,Academia,"Nair, V., Hinton, G. E.",6/15/10,Rectified linear units improve restricted boltzmann machines,https://dl.acm.org/doi/10.5555/3104322.3104425,1.40E+04,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!
ReLU (NORB),Vision,Object recognition,University of Toronto,Academia,"Nair, V., Hinton, G. E.",6/15/10,Rectified linear units improve restricted boltzmann machines,https://dl.acm.org/doi/10.5555/3104322.3104425,1.40E+04,Highly cited,,1.62E+07,"""The stereo-pair images are subsampled from their original resolution of 108 × 108 × 2 to 32 × 32 × 2 to speed up experiments [...]  the architecture
with the best results have 4000 units in the first layer
and 2000 in the second [...] there are 58,320 test
cases (9,720 cases per class) ""

So the architecture has (32*32*2+1)x4000 + (4000+1)*2000 + (2000+1)*58,320/9,720 parameters",,,,,2.92E+05,"""There are 291,600 training cases (48,600 cases per class) and 58,320 test cases (9,720 cases per class).""",,,,,,,,,,Academia,,,5/29/23 20:51
Deconvolutional Network,Vision,,New York University,Academia,"Matthew D. Zeiler, Dilip Krishnan, Graham W. Taylor and Rob Fergus",6/13/10,Deconvolutional Networks,https://ieeexplore.ieee.org/document/5539957,1.52E+03,Highly cited,,,,,,,,,,,Inference time of the largest model was 55s on Caltech 101 images.,,,,,,,,Academia,,,9/12/23 22:07
Mid-level Features,Vision,,"INRIA,Ecole,New York University",Academia,"YL Boureau, F Bach, Y LeCun, J Ponce",6/13/10,Learning mid-level features for recognition,https://ieeexplore.ieee.org/document/5539963,1.31E+03,Highly cited,,,This is extracting low-level SIFT features then max-pooling them and using in a linear SVM. The training compute could be estimated loosely for the SVM part.,,,,,,,,,,,,,,,,Academia,,,9/12/23 22:08
Word Representations,Language,,"University of Montreal,University of Illinois at Urbana- Champaigne",Academia,"Joseph Turian, Lev-Arie Ratinov, Yoshua Bengio",6/1/10,Word Representations: A Simple and General Method for Semi-Supervised Learning,https://aclanthology.org/P10-1040.pdf,2.51E+03,Highly cited,,,,,,,,3.70E+07,"Section 6: ""After cleaning, there are 37 million words (58%
of the original) in 1.3 million sentences""",,,,,,,,,,Academia,,,6/8/23 0:39
Feedforward NN,Vision,Digit recognition,University of Montreal,Academia,"X Glorot, Y Bengio",5/13/10,Understanding the difficulty of training deep feedforward neural networks,https://proceedings.mlr.press/v9/glorot10a.html,1.33E+04,Highly cited,,7.08E+06,"pg250 of the paper, section 2.3: 
""We optimized feedforward neural networks with one to
five hidden layers, with one thousand hidden units per
layer""

Input is a flattened 32x32 image, which corresponds to an input vector of length 3072

Output is a number from 0-9, so 10 neurons

No. of params: 3072*1000 + 4*1000*1000 + 1000*10 = 7,082,000
",3.50E+14,"Roughly two times the number of parameters for ops per forward pass. 

So 2*7082000 params*3.5*140 epochs * 50k training images = 3.5e14",MNIST,,,,1.40E+07,Roughly twice the no. of params,,,,,0.01,,,Academia,,,5/29/23 20:51
6-layer MLP (MNIST),Vision,Character recognition,IDSIA ; University of Lugano & SUPSI,Academia,"Dan Claudiu Ciresan, Ueli Meier, Luca Maria Gambardella, Juergen Schmidhuber",3/1/10,Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition,https://arxiv.org/abs/1003.0358,1.26E+03,Highly cited,,1.21E+07,Table 1,1.31E+14,"""Networks with up to 12 million weights can successfully be trained by plain gradient descent to achieve test errors below 1% after 20-30 epochs in less than 2 hours of training.""

I assume that the number of passes per epoch is 60k, the training set size.",MNIST,,6.00E+04,"""MNIST consists of two datasets, one for training (60,000 images) and one for testing (10,000 images). Many studies divide the training set into two sets consisting of 50,000 images for training and 10,000 for validation. Our network is trained on slightly deformed images, continually generated in on-line fashion; hence we may use the whole un-deformed training set for validation, without wasting training images""",,,,,,,0.01,,,Academia,,,5/29/23 20:51
Stacked Denoising Autoencoders,,,"University of Montreal,University of Toronto",Academia,"P Vincent, H Larochelle, I Lajoie, Y Bengio",1/3/10,Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion,https://www.jmlr.org/papers/v11/vincent10a.html,6.23E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/12/23 18:33
Learning deep architectures,,,"University of Montreal,Microsoft Research",Academia,Y Bengio,11/15/09,Learning deep architectures for AI,https://www.nowpublishers.com/article/Details/MAL-006,9.78E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/12/23 18:35
3D city reconstruction,3D reconstruction,,"University of Washington,Microsoft Research,Cornell University",Industry - Academia Collaboration (Academia leaning),"Sameer Agarwal, Noah Snavely, Ian Simon, Steven M. Seitz and Richard Szeliski",9/29/09,Building Rome in a Day,https://grail.cs.washington.edu/rome/,2.20E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/12/23 18:37
BellKor 2007,Recommendation,Movie ratings,AT&T Labs,Industry,"RM Bell, Y Koren, C Volinsky",9/21/09,The BellKor solution to the Netflix Prize,https://www.semanticscholar.org/paper/The-BellKor-solution-to-the-Netflix-Prize-Bell-Koren/f4ebb542c752a0dc423f94fd121e2edb8f6275ba,2.38E+02,,,,,,,Netflix Prize,,1.00E+08,"The training data set consists of 100,480,507
ratings",,,,,,,,,,Industry,,,6/8/23 0:39
Polarity Classifier,Language,,"University of Edinburgh,University of Pittsburgh",Academia,"Theresa Wilson, Janyce Wiebe, Paul Hoffmann.",9/1/09,Recognizing Contextual Polarity: An Exploration of Features for Phrase-Level Sentiment Analysis,https://aclanthology.org/J09-3003.pdf,7.87E+02,,,,,,,,,1.11E+04,"Section 3.3 reveals there are 11,112 sentences. Since this is phrase-level sentiment analysis sentences seem like the best unit",,,,,,,,,,Academia,,,9/12/23 18:48
MatrixFac for Recommenders,Recommendation,,"AT&T Labs,Yahoo Research",Industry,"Yehuda Koren, Robert Bell, and Chris Volinsky",8/7/09,Matrix factorization techniques for recommender systems,https://ieeexplore.ieee.org/document/5197422,8.91E+03,Highly cited,,,,,,Netflix Prize,,1.00E+08,,,,,,,,,,,Industry,,,9/12/23 18:49
RL mapping instructions (troubleshooting),Reading,Instruction interpretation,Massachusetts Institute of Technology,Academia,"SRK Branavan, H Chen, LS Zettlemoyer, R Barzilay",8/2/09,Reinforcement Learning for Mapping Instructions to Actions,https://aclanthology.org/P09-1010/,2.95E+02,,,1.33E+05,"""We use a policy gradient
algorithm to estimate the parameters of a log-linear model for action selection [...] In total, there are 4,438 features [in the Windows domain]. [...]  This difficulty can be attributed in part to the large branching factor of possible actions at each step — on average, there are 27.14 choices per action in the Windows domain""",,,Windows Help and Support,,1.33E+03,"Shown at beginning of section 7
Total number of documents is 128, average number of actions per document is 10.37",,,,,,,,,,Academia,,,9/12/23 22:28
BellKor 2008,Recommendation,Movie ratings,AT&T Labs,Industry,"RM Bell, Y Koren, C Volinsky",8/1/09,The BellKor 2008 Solution to the Netflix Prize,https://www.researchgate.net/publication/228766792_The_BellKor_2008_solution_to_the_Netflix_Prize,1.58E+02,,,,,,,Netflix Prize,,1.00E+08,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",,,,,,,,,,Industry,,,6/8/23 0:39
BellKor 2009,Recommendation,Movie ratings,AT&T Labs,Industry,Y Koren,8/1/09,The BellKor Solution to the Netflix Grand Prize,https://www2.seas.gwu.edu/~simhaweb/champalg/cf/papers/KorenBellKor2009.pdf,5.07E+02,Historical significance,Introduced new algorithms; won Netflix Grand Prize,,,,,Netflix Prize,,1.00E+08,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",,,,,,,,,,Industry,,,7/12/23 16:42
BigChaos OptiBlend,Recommendation,Movie ratings,AT&T Labs,Industry,"A Töscher, M Jahrer, RM Bell",8/1/09,The BigChaos Solution to the Netflix Grand Prize,https://www.asc.ohio-state.edu/statistics/statgen/joul_aut2009/BigChaos.pdf,2.37E+02,,,,,,,Netflix Prize,,1.00E+08,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",,,,,,,,,,Industry,,,9/12/23 18:51
Pragmatic Theory solution (Netflix 2009),Recommendation,Movie ratings,Pragmatic Theory Inc.,Industry,"M Piotte, M Chabbert",8/1/09,The Pragmatic Theory solution to the Netflix Grand Prize,https://www.asc.ohio-state.edu/statistics/statgen/joul_aut2009/PragmaticTheory.pdf,1.11E+02,,,,,,,Netflix Prize,,1.00E+08,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",,,,,,,,,,Industry,,,9/12/23 18:52
RL mapping instructions (games),Reading,Instruction interpretation,Massachusetts Institute of Technology,Academia,"SRK Branavan, H Chen, LS Zettlemoyer, R Barzilay",8/1/09,Reinforcement Learning for Mapping Instructions to Actions,https://aclanthology.org/P09-1010/,2.95E+02,,,8.09E+04,"""We use a policy gradient
algorithm to estimate the parameters of a log-linear model for action selection [...] In total, there are 8,094 features [in the Crossblock domain]. [...]  This difficulty can be attributed in part to the large branching factor of possible actions at each step — on average, there are [...] 9.78 [actions] in the Crossblock
domain""",,,Windows Help and Support,,2.93E+02,"Shown at beginning of section 7
Total number of documents is 50, average number of actions per document is 5.86

source: https://en.wikipedia.org/wiki/Netflix_Prize",,,,,,,,,,Academia,,,9/12/23 22:28
GPU DBNs,Other,,Stanford University,Academia,"R Raina, A Madhavan, AY Ng",6/15/09,Large-scale Deep Unsupervised Learning using Graphics Processors,http://www.machinelearning.org/archive/icml2009/papers/218.pdf,7.89E+02,,,1.00E+08,"""For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day.""",1.00E+15,https://www.getguesstimate.com/models/19602,,,1.00E+06,"Table 2 shows the running time for processing 1 million
examples for RBMs of varying size",,,,,,,0.06,,,Academia,,,9/6/23 18:15
Conv-DBN,,,Stanford University,Academia,"H Lee, R Grosse, R Ranganath, AY Ng",6/14/09,Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://dl.acm.org/doi/10.1145/1553374.1553453,2.96E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/8/23 1:20
Deep Boltzmann Machines,,,University of Toronto,Academia,"Ruslan Salakhutdinov, Geoffrey Hinton",4/16/09,Deep Boltzmann Machines,https://www.sciencedirect.com/topics/computer-science/deep-boltzmann-machine,2.67E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
Semantic Hashing,Other,,University of Toronto,Academia,"R Salakhutdinov, G Hinton",12/10/08,Semantic Hashing,https://www.cs.cmu.edu/~rsalakhu/papers/sdarticle.pdf,1.49E+03,Highly cited,,2.60E+06,,,,,,3.11E+05,Section 4.1,,,,,,,,,,Academia,,,5/29/23 20:51
BigChaos 2008,Recommendation,Movie ratings,AT&T Labs,Industry,"A Töscher, M Jahrer",11/25/08,The BigChaos Solution to the Netflix Prize 2008,https://www.researchgate.net/publication/228419683_The_bigchaos_solution_to_the_netflix_prize_2008,3.50E+01,Historical significance,Winners of the 2008 Netflix Price,,,,,Netflix Prize,,1.00E+08,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",,,,,,,,,,Industry,,,9/13/23 16:10
Sparse digit recognition SVM,Vision,,"Univeristy of Lubeck,Germany",Academia,"Kai Labusch, Erhadt Barth, Thomas Martinetz",11/19/08,Simple method for high-performance digit recognition based on sparse coding,https://pubmed.ncbi.nlm.nih.gov/19000969/,1.24E+02,,,,,,,,,,,,,,,,,,,,Academia,,,9/8/23 1:31
Boss (DARPA Urban Challenge),Driving,Self-driving car,Carnegie Mellon University,Industry - Academia Collaboration,"Chris Urmson, Joshua Anhalt, Drew Bagnell,Christopher Baker, Robert Bittner,M. N. Clark, John Dolan, Dave Duggins,Tugrul Galatali, Chris Geyer,Michele Gittleman, Sam Harbaugh,Martial Hebert, Thomas M. Howard,Sascha Kolski, Alonzo Kelly,Maxim Likhachev, Matt McNaughton,Nick Miller, Kevin Peterson, Brian Pilnick,Raj Rajkumar, Paul Rybski, Bryan Salesky,Young-Woo Seo, Sanjiv Singh, Jarrod Snider,Anthony Stentz, William “Red” Whittaker,Ziv Wolkowicki, and Jason Ziglar",7/23/08,Autonomous Driving in UrbanEnvironments: Boss and theUrban Challenge,https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.20255,1.84E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,6/8/23 0:39
Stacked Semisuperviser Autoencoders,Language,Document representation,"New York University,Microsoft",Industry - Academia Collaboration,"MA Ranzato, M Szummer",7/15/08,Semisupervised learning of compact document representations with deep networks,https://dl.acm.org/doi/10.1145/1390156.1390256,2.43E+02,,,3.00E+06,,,,,,6.61E+04,"""The 20 Newsgroups dataset contains 18845
postings taken from the Usenet newsgroup collection.
Documents are partitioned into 20 topics. The dataset
is split into 11314 training documents and 7531 test
documents. Training and test articles are separated in
time. Reuters has a predefined ModApte split of the
data into 11413 training documents and 4024 test doc-
uments. Documents belong to one of 91 topics. The
Ohsumed dataset has 34389 documents with 30689
words and each document might be assigned to more
than one topic, for a total of 23 topics. The dataset is
split into training and test by randomly selecting the
67% and the 33% of the data""

total # documents = 11314 + 11413 + 34389*0.6

I'm using #documents here since the task is document representation. Using #words would increase the size by ~3 OOMs",,,,,,,,,,Industry,,,9/12/23 22:09
Deep Multitask NLP Network,Language,Language modelling,NEC Laboratories,Industry,"Ronan Collobert, Jason Weston",7/5/08,"A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning
",http://icml2008.cs.helsinki.fi/papers/391.pdf,7.10E+03,"Highly cited,SOTA improvement",,1.50E+06,"With a word vector size of 50 and a vocabulary size of 30,000, the embedding matrix has 1,500,000 parameters. There are also some small convolutional and dense layers with far fewer parameters.",,,"PropBank, Penn Treebank, Wikipedia","PropBank (1M words) for semantic role labeling task
Penn Treebank (1M words) for part-of-speech tagging and chunking tasks
Stanford NER dataset for named entity recognition task
Wikipedia text (631M words) for unsupervised pretraining",6.33E+08,,,,168,1 week on 1 computer,,Unsupervised,,,,Industry,Speculative,"We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in stateof-the-art performance.",9/21/23 3:08
#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!
Denoising Autoencoders,,,University of Montreal,Academia,"Pascal Vincent, Hugo Larechelle, Yoshua Bengio, Pierre- Antoine Manzagol",7/5/08,Extracting and Composing Robust Features with Denoising Autoencoders,https://dl.acm.org/doi/10.1145/1390156.1390294,6.30E+03,,,,,,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
Multitask DNNs for NLP,Language,,NEC Laboratories,Industry,"R Collobert, J Weston",7/5/08,A unified architecture for natural language processing: Deep neural networks with multitask learning,https://dl.acm.org/doi/10.1145/1390156.1390177,5.76E+03,Highly cited,,,,,,,,6.31E+08,"Section 7: ""631 million words
from Wikipedia""",,,,,,,,,,Industry,,,9/12/23 19:03
,,,"NEC Laboratories,Google",Industry,"L Bottou, O Bousquet",12/3/07,The Tradeoffs of Large Scale Learning,https://dl.acm.org/doi/10.5555/2981562.2981583,1.70E+03,not an ML system,,,,,,,,,,,,,,,,,,,Industry,,,8/10/23 15:25
Semi-Supervised Embedding for DL,,,"Google,NUANCE Communications,UIUC,IDIAP",Industry - Academia Collaboration,"Jason Weston, Frederick, Ratle, Hossein Mobahi, Ronan Collobert",7/5/08,Deep Learning via Semi-Supervised Embedding,https://dl.acm.org/doi/10.1145/1390156.1390303,1.09E+03,,,,,,,,,,,,,,,,,,,,Industry,,,9/12/23 18:54
Multiscale deformable part model,Vision,,"University of Chicago & Toyota Technological Institute,Chicago & University of California,Irvine",Academia,"Pedro Felzenszwalb, David McAllester, Deva Ramanan",6/23/08,"A discriminatively trained, multiscale, deformable part model",https://ieeexplore.ieee.org/abstract/document/4587597,3.09E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/8/23 1:32
BLSTM for handwriting (2),Vision,Character recognition,"University of Bern,IDSIA,TU Munich",Academia,"Alex Graves, Marcus Liwicki, Horst Bunke, Jürgen Schmidhuber, Santiago Fernández",12/3/07,Unconstrained online handwriting recognition with recurrent neural networks,https://proceedings.neurips.cc/paper/2007/hash/4b0250793549726d5c1ea3906726ebfe-Abstract.html,3.41E+02,,,1.01E+05," For the raw input representation,
there were 4 input units and a total of 100,881 weights",,,,,,,,,,,,,,,,Academia,,,7/14/23 20:56
Enhanced Neighborhood-Based Filtering,Recommendation,,AT&T Labs,Industry,"RM Bell, Y Koren",10/28/07,Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights,http://brettb.net/project/papers/2007%20Scalable%20collaborative%20filtering%20with%20jointly%20derived%20neighborhood%20interpolation%20weights.pdf,6.87E+02,,,,,,,,,,,,,,,,,,,,Industry,,,9/12/23 19:13
BLSTM for handwriting (1),Vision,,"University of Bern,IDSIA,TU Munich",Academia,"M Liwicki, A Graves, S Fernàndez",9/23/07,A Novel Approach to On-Line Handwriting Recognition Based on Bidirectional Long Short-Term Memory Networks,https://people.idsia.ch//~juergen/icdar_2007.pdf,2.87E+02,SOTA Improvement,,,,,,,,,,,,,,,,,,,Academia,,,7/14/23 20:56
Regularized SVD for Collaborative Filtering,Recommendation,,Warsaw University,Academia,A Paterek,8/12/07,Improving regularized singular value decomposition for collaborative filtering,https://www.semanticscholar.org/paper/Improving-regularized-singular-value-decomposition-Paterek/f732d0f69fe4e84a95c32706b28b9e4ef1753c61,1.12E+03,Highly cited,,,,,,Netflix Prize,,1.00E+08,,,,,,,,,,,Academia,,,9/12/23 19:15
Restricted Bolzmann machines,Recommendation,,University of Toronto,Academia,"Russ Salukhutdinov, Andriy Mnih, GE Hinton",6/20/07,Restricted Boltzmann machines for collaborative filtering,https://dl.acm.org/doi/abs/10.1145/1273496.1273596?casa_token=cfdkH2x12MwAAAAA:sEUzfllIGyPcOfzgUoDPHlpC1ukfCAo8ewocBXWBswIIF9eS5HdFo30nOtfmIV8gm-XpBpQJJ5zYVO8,2.14E+03,,,,,,,Netflix Prize,,1.00E+08,"The training data set consists of 100,480,507
ratings",,,,,,,,,,Academia,,,6/8/23 0:39
Empirical evaluation of deep architectures,,,University of Montreal,Academia,"Hugo Larechelle, Dumithru Erhan, Aaron C Courville, James Bergsta, Yoshua Bengio",6/1/07,An empirical evaluation of deep architectures on problems with many factors of variation,https://dl.acm.org/doi/10.1145/1273496.1273556,1.12E+03,,,,,,,,,,,,,,,,,,,,Academia,,,9/12/23 19:16
λ-WASP,Language,,UT Austin,Academia,"YW Wong, R Mooney",6/1/07,Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus,https://www.aclweb.org/anthology/P07-1121/,3.83E+02,,,,,,,,,7.92E+02,"""Table 1 summarizes the results at the end of the learning curves (792 training examples for λWASP, WASP and SCISSOR, 600 for Z&C)""",,,,,,,,,,Academia,,,6/21/23 18:55
Greedy layer-wise DNN training,,,University of Montreal,Academia,"Y Bengio, P Lamblin, D Popovici",12/4/06,Greedy layer-wise training of deep networks,https://dl.acm.org/doi/10.5555/2976456.2976476,5.61E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/12/23 19:18
Sparse Energy-Based Model,Vision,,New York University,Academia,"M Ranzato, C Poultney, S Chopra, Y Cun",12/4/06,Efficient Learning of Sparse Representations with an Energy-Based Model.,https://papers.nips.cc/paper/2006/hash/87f4d79e36d68c3031ccf6c55e9bbd39-Abstract.html,1.60E+03,Highly cited,,,,,,MNIST,,6.00E+04,,,,,,,,,,,Academia,,,9/12/23 22:07
Local Binary Patterns for facial recognition,,,"Machine Vision group,Finland",Academia,"Timo Ahonen, Abdenour Hadid, and Matti Pietikainen",12/1/06,Face Description with Local Binary Patterns: Application to Face Recognition,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.456.1094&rep=rep1&type=pdf,3.11E+03,Highly cited,,,"Shallowly investigated, couldn't find much.
",,,,,,,,,,,,,,,,Academia,,,9/12/23 19:18
Deep Belief Nets,Vision,Character recognition,"University of Toronto, NUS",Academia,"GE Hinton, S Osindero, YW Teh",7/18/06,A fast learning algorithm for deep belief nets,https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf,1.61E+04,Highly cited,,1.60E+06,,,,MNIST,,6.00E+04,"""The network that performed best on the validation set was
then tested and had an error rate of 1.39%. This network was
then trained on all 60,000 training images8 until its error-rate
on the full training set was as low as its final error-rate had
been on the initial training set of 44,000 images.""",,,,,,,,,,Academia,,,5/29/23 20:51
DImensionality Reduction,Vision,Face recognition,University of Toronto,Academia,"GE Hinton, RR Salakhutdinov",7/18/06,Reducing the dimensionality of data with neural networks.,https://www.cs.toronto.edu/~hinton/science.pdf,1.57E+04,Highly cited,,3.80E+06,,,,,,7.00E+04,"After fine-tuning on all 60,000 training images, the autoencoder was tested on 10,000 new images and produced much better reconstructions than did PCA
(Fig. 2B)",,,,,,,,,,Academia,,,5/29/23 20:51
Semantic Taxonomy Induction,Language,,Stanford University,Academia,"Rion Snow, Dan Jurafsky, and Andrew Y. Ng",7/7/06,Semantic Taxonomy Induction from Heterogenous Evidence,https://www.aclweb.org/anthology/P06-1101/,5.71E+02,,,1.00E+02,"The main learning algorithm is a logistic classifier. The input is a matrix M, where the rows are pairs of words, and the columns (variables) are counts of occurrences of synthetic dependency paths between those two words.

Since there are on the order of 10~100 different types of syntactic relationships, this is the number of length-1 paths, and thus the number of parameters if only length-1 paths are used.

However, if the length of the paths considered is longer (say, 5), then the parameters would be on the order of (10~100)^5. It's not clear to me which is the case",,,,,8.51E+05,"[Classification task]

The labeled training set is
constructed by labeling the collected feature vectors as positive “known hypernym” or negative
“known non-hypernym” examples using WordNet
2.0; 49,922 feature vectors were labeled as positive training examples, and 800,828 noun pairs
were labeled as negative training examples.

800,828 + 49,922 = 850750",,,,,,,,,,Academia,,,9/6/23 18:15
CTC-Trained LSTM,Speech,Speech recognition,"IDSIA, TUM",Academia,"Alex Graves, Santiago Fernández, Faustino Gómez, Jürgen Schmidhuber",6/25/06,Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks,https://www.cs.toronto.edu/~graves/icml_2006.pdf,3.29E+03,Highly cited,,1.15E+05,"""The hidden layers were fully connected to themselves
and the output layer, and fully connected from the input layer. The input layer was size 26, the softmax output layer size 62 (61 phoneme categories plus the blank label), and the total number of weights was
114, 662.""

https://www.cs.toronto.edu/~graves/icml_2006.pdf",,,TIMIT,,4.16E+04,"4162 utterances, guesstimated avg 10 words per utterance",,,,,,,,,,Academia,,,5/29/23 20:51
DrLIM,Vision,Image embedding,New York University,Academia,R. Hadsell; S. Chopra; Y. LeCun,6/17/06,Dimensionality Reduction by Learning an Invariant Mapping,https://ieeexplore.ieee.org/document/1640964,2.69E+03,Highly cited,,3.71E+04,Architecture described in figure 3,,,,,2.17E+05,"""The dataset was split into 660 training images and a 312
test images. The result of training on all 10989 similar pairs
and 206481 dissimilar pairs is a 3-dimensional manifold in
the shape of a cylinder (see figure 8).""

206481 + 10989 = 217470",,,,,,,,,,Academia,,,6/8/23 0:39
Spatial Pyramid Matching,Vision,,"University of Illinois,INRIA,Ecole Normale",Academia,"S Lazebnik, C Schmid, J Ponce",6/17/06,Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories,https://inc.ucsd.edu/mplab/users/marni/Igert/Lazebnik_06.pdf,9.81E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/8/23 4:07
FAST,Vision,Corner detection,University of Cambridge,Academia,Edward Rosten and Tom Drummond,5/7/06,Machine Learning for High-Speed Corner Detection,https://link.springer.com/chapter/10.1007/11744023_34,5.42E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
RL for helicopter flight,Driving,Helicopter driving,"UC Berkeley,Stanford University",Academia,"H. Kim, Michael Jordan, Shankar Sastry, Andrew Ng",3/9/06,Autonomous helicopter flight via reinforcement learning,https://papers.nips.cc/paper/2003/hash/b427426b8acd2c2e53827970f2c2f526-Abstract.html,8.10E+01,,,,,,,,,,,,,,,,,,,,Academia,,,9/12/23 19:28
LeNet5 SVM,,,"CRAN,CENPARMI",Academia,"Fabian Lauer, Ching Y Suen, Gerard Bloch",2/2/06,A trainable feature extractor for handwritten digit recognition,https://hal.archives-ouvertes.fr/hal-00018426/en,3.65E+02,,,,,,,,,,,,,,,,,,,,Academia,,,9/12/23 19:29
Stanley (DARPA Grand Challenge 2),Driving,Self-driving car,Stanford University,Industry - Academia Collaboration,"S Thrun, M Montemerlo, H Dahlkamp",1/1/06,Stanley: The Robot that Wonthe DARPA Grand Challenge,https://www.researchgate.net/publication/220648006_Stanley_The_robot_that_won_the_DARPA_Grand_Challenge,2.56E+03,Highly cited,,," Our  approach  and  the underlying  probabilistic  Markov  model  possess  anumber  of  unknown  parameters.  These  parameters include the height threshold, the statistical acceptance  probability  threshold,  and  various  Markov chain error parameters the noise covariances of theprocess noise and the measurement noise. Stanley uses a discriminative learning algorithm for  locally  optimizing  these  parameters.",,,,,,,,,,,,,,,,Industry,,,9/6/23 18:15
BiLSTM for Speech,Speech,Speech recognition,IDSIA and TU Munich,Academia,"A Graves, J Schmidhuber",8/1/05,Framewise phoneme classification with bidirectional LSTM and other neural network architectures,https://www.sciencedirect.com/science/article/abs/pii/S0893608005001206,3.39E+03,Highly cited,,1.52E+05,"""The hidden layer sizes were chosen to ensure that all networks had roughly the same number of weights W (≈100,000). However, for the MLPs the network grew with the time-window size, and W varied between 22,061 and 152,061.""",2.41E+13,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,TIMIT,,3.70E+04,"https://catalog.ldc.upenn.edu/LDC93s1
One sample utterance has around 10 words

3696 utterances * 10 words = around 37k words",,,,,,,,,,Academia,,,8/3/23 23:14
Histograms of Oriented Gradients,Vision,,Inria Grenoble Rhône-Alpes,Academia,"N Dalal, B Triggs",6/25/05,Histograms of oriented gradients for human detection,https://ieeexplore.ieee.org/document/1467360,3.66E+04,Highly cited,,,,,,,,1.81E+03," we produced a new and significantly more
challenging data set, ‘INRIA’, containing 1805 64×128 im-
ages",,,,,,,,,,Academia,,,9/8/23 3:52
ConvNet similarity metric,Vision,,New York University,Academia,"S Chopra, R Hadsell, Y LeCun",6/20/05,"Learning a similarity metric discriminatively, with application to face verification",https://ieeexplore.ieee.org/document/1467314,3.05E+03,Highly cited,,,,,,,,1.40E+05,"The actual training set that was used contained
140,000 image pairs that were evenly split between genuine
and impostor.",,,,,,,,,,Academia,,,9/12/23 19:34
Hiero,Language,Translation,"University of Maryland, College Park",Academia,David Chiang,6/1/05,A Hierarchical Phrase-Based Model for Statistical Machine Translation,https://aclanthology.org/P05-1033/,1.49E+03,Highly cited,,1.20E+08,"Very unsure, but the paper mentions 
""We ran the training process of Section 3 on the same data, obtaining a grammar of 24M rules"" 
and 
""For our experiments we used the following features, analogous to Pharaoh’s default feature set:
• P(γ | α) and P(α | γ), the latter of which is not
found in the noisy-channel model, but has been
previously found to be a helpful feature (Och
and Ney, 2002);
• the lexical weights Pw(γ | α) and Pw(α | γ) (Koehn et al., 2003), which estimate how well the words in α translate the words in γ;
2
• a phrase penalty exp(1), which allows the
model to learn a preference for longer or
shorter derivations, analogous to Koehn’sphrase penalty (Koehn, 2003).""

Suggesting 24M rules * 5 features per rule (?)",,,,,1.71E+08,"[WORDS]
155M words dataset for the language model plus (7.2+9.2)M words for the translation model?",,,,,,,,,,Academia,,,5/29/23 20:51
SACHS,Other,,"Massachusetts Institute of Technology,Stanford University",Academia,"K. Sachs, O. Perez, D. Pe'er, D. A. Lauffenburger and G. P. Nolan",4/22/05,Causal Protein-Signaling Networks Derived from Multiparameter Single-Cell Data.,https://science.sciencemag.org/content/308/5721/523.long,1.68E+03,Highly cited,,1.78E+02,From https://www.bnlearn.com/bnrepository/,,,,,5.40E+03,"I think? 

"" The truncated singlecell data set (420 data points) shows a large
(11-arc) decline in accuracy, missing more connections and reporting more unexplained arcs than its larger (5400 data points) counterpart (fig. S4B). ""

Seems potentially wrong by maybe 20%. Might need to add 1200.",,,,,,,,,,Academia,,,9/6/23 18:16
LIRA,Vision,Character recognition,Instituto de Ciencias Aplicadas y Technologia,Academia,"E Kussul, T Baidyk",7/30/04,Improved method of handwritten digit recognition tested on MNIST database,https://www.sciencedirect.com/science/article/abs/pii/S0262885604000721,1.88E+02,,,1.00E+05,"""For the first modification of the Rosenblatt perceptron 10 neurons were included into the R-layer. [...] The number of the A-layer neurons was 256,000"" The relation between the S-layer and A-layer is hardcoded",,The coding time was 20 h and the training time was 45 h.,,,1.00E+04,,,,,,,,,,,Academia,,,5/29/23 20:51
Automated WSD via WordNet,Language,Word sense disambiguation,University of Sussex,Academia,"D McCarthy, R Koeling, J Weeds",7/1/04,Finding Predominant Word Senses in Untagged Text,https://aclanthology.org/P04-1036/,4.75E+02,,,,,,,,,5.00E+03,"They do two experiments, one on a dataset of 5.000 tagged words and
another one on two datasets containing a total of around 40 million words, of which they only select 38 unique words and manually annotate the senses?
I think the first one is more representative",,,,,,,,,,Academia,,,9/12/23 19:35
Sandstorm (DARPA Grand Challenge I),Driving,Self-driving car,Carnegie Mellon University,Academia,William Red L. Whittaker,6/14/04,DARPA Grand Challenge Technical Paper,https://ieeexplore.ieee.org/document/1336386,6.60E+01,,,,,,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
GPU implementation of neural networks,,,Soongsil University,Academia,"KS Oh, K Jung",6/1/04,GPU implementation of neural networks,https://www.sciencedirect.com/science/article/pii/S0031320304000524,4.71E+02,,,,,,,,,,,,,,,,,,,,Academia,,,9/8/23 3:58
Max-Margin Markov Networks,,,Stanford University,Academia,"B. Taskar, C. Guestrin, and D. Koller",3/1/04,Max-margin markov networks,https://papers.nips.cc/paper/2003/file/878d5691c824ee2aaf770f7d36c151d6-Paper.pdf,1.76E+03,Highly cited,,,,,,,,6.00E+02,"The data set is divided into 10 folds of ∼ 600 training and ∼ 5500 testing examples.
The accuracy results, ... are averages over the 10 folds",,,,,,,,,,Academia,,,9/8/23 3:52
CNN Best Practices,Vision,Character recognition,One Microsoft Way,Industry,"PY Simard, D Steinkraus, JC Platt",8/6/03,Best practices for convolutional neural networks applied to visual document analysis,https://ieeexplore.ieee.org/document/1227801,3.07E+03,Highly cited,,,,,,MNIST,,5.00E+04,,,,,,,,,,,Industry,,,5/29/23 20:51
Unsupervised Scale-Invariant Learning,Vision,,University of Oxford,Academia,"R Fergus, P Perona, A Zisserman",6/18/03,Object Class Recognition by Unsupervised Scale-Invariant Learning,https://ieeexplore.ieee.org/document/1211479,2.97E+03,Highly cited,,4.51E+02,"See Table 1
",,,,,3.50E+03,"See Table 2 and Figure 1.
There are 7 datasets, each with 200-800 of pictures. I pick 500 as the avg number of pictures",,,,,,,,,,Academia,,,5/29/23 20:51
Phrase-based translation,Language,Translation,University of Southern California,Academia,"Philipp Koehn, Franz Josef Och, Daniel Marcu",5/1/03,Statistical Phrase-Based Translation,https://dl.acm.org/doi/10.3115/1073445.1073462,4.27E+03,Highly cited,,9.18E+06,"There are various components to the system:

- Translation probability model phi
- The distortion probability distribution d
- A langage model p_LM
- A length factor w

Several translation probability models are considered. The most performant one is the AP word alignment model. The sentence length preferred by the authors is 3 words maximum. In the biggest corpus considered (320k phrase pairs) it produces a phrase translation probability table of 1996k entries.

The distortion probability model d is taken from  (Marcu and Wong, 2002).

The distortion probability model must have ~10 parameters at most

The language model p_LM is a back off trigram model from (Seymore and Rosenfeld,1997). AFAIK the cutoff used is not specified. Based on the example on section 4.3 of (Seymore and Rosefeld, 1997), a trigram probability model has about 3866964 + 2674322 + 641604 parameters.

""For each possible phrase translation anywhere in the sentence (we call it a translation option), we multiply its phrase translation probability with the language model probability for the generated English phrase. As language model probability we use the unigram probability for the first word, the bigram probability for the second, and the trigram probability for all following words""

The length factor w is an additional single parameter.

""In order to calibrate the output length, we introduce a
factor w for each generated English word in addition to
the trigram language model ""

In summary, the parameter count seems to be dominated by the trigram language model and the word alignment phrase translation model. ",,,,,2.00E+07,"[WORDS]
""We used the freely available Europarl corpus to carry out experiments. This corpus contains over 20 million words in each of the eleven official languages of the European Union, covering the proceedings of the European Parliament 1996-2001. 1755 sentences of length 5-15 were reserved for testing.""

""These results are consistent
over training corpus sizes from 10,000 sentence pairs to
320,000 sentence pairs. ""

So 20 million words or 320k sentence pairs.",,"""With our decoder, translating 1755 sentence of length 5-15 words
takes about 10 minutes on a 2 GHz Linux system.""",,,,,,,,Academia,,,5/29/23 20:51
NPLM,Language,Text autocompletion,Université de Montréal,Academia,"Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Jauvin",3/15/03,A Neural Probabilistic Language Model,https://dl.acm.org/doi/10.5555/944919.944966,7.63E+03,Highly cited,,1.19E+07,"""The number of free parameters is |V|(1 + nm + h) + h(1 + (n − 1)m) [...] For example, consider the following architecture used in the experiments on the AP (Associated Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order of the model is n = 6, the number of word features is m = 100""",1.30E+15,"""For example, consider the following architecture used in the experiments on the AP (Associated
Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order
of the model is n = 6, the number of word features is m = 100. The total number of numerical operations to process a single training example is approximately |V|(1+nm+h)+h(1+nm)+nm""

The first 800,000 words were used for training... reducing the vocabulary size to |V| = 16,383

convergence of the stochastic gradient ascent procedure was obtained after around 10
to 20 epochs for the Brown corpus

NOTE: there are two corpuses. The one represented in this calculation is the Brown one, which got a better improvement over sota",Brown corpus,,1.00E+06,"""Comparative experiments were performed on the Brown corpus which is a stream of 1,181,041 words, from a large variety of English texts and books. The first 800,000 words were used for training, the following 200,000 for validation (model selection, weight decay, early stopping) and the remaining 181,041 for testing. The number of different words is 47,578 (including punctuation, distinguishing between upper and lower case, and including the syntactical marks used to separate texts and paragraphs). Rare words with frequency ≤ 3 were merged into a single symbol, reducing the vocabulary size to |V| = 16,383.""",2.17E+07,,,,,,,,,Academia,,,6/8/23 0:39
LDA,Language,Document classification,"University of California, Stanford University",Academia,"David M. Blei, Andrew Y. Ng, Michael I. Jordan",2/2/03,Latent Dirichlet Allocation,https://jmlr.org/papers/volume3/blei03a/blei03a.pdf,3.87E+04,Highly cited,,,,,,,,,Multiple experiments with different tasks and datasets,,,,,,,,,,Academia,,,5/29/23 20:51
Statistical Shape Constellations,Vision,Image Classification,California Institute of Technology,Academia,"M. Weber, M. Welling, and P. Perona",1/1/03,Unsupervised Learning of Models for Recognition,https://link.springer.com/content/pdf/10.1007/3-540-45054-8_2.pdf,9.49E+02,Historical significance,,,,,,,,,,,,,,,,,,,Academia,,,9/13/23 16:10
Web mining + Decision tree recommender,,,Korea Advanced Institute of Science and Technology,Academia,"YH Cho, JK Kim, SH Kim",10/1/02,A personalized recommender system based on web usage mining and decision tree induction,https://reader.elsevier.com/reader/sd/pii/S0957417402000520?token=155B6D1937982D7D0271AFD1CFB034DFD7F3D1DE816B66C025EBC9D0A305BA6DA685DD62989DC05246C794CAC74CDAEF&originRegion=us-east-1&originCreation=20220325235441,6.56E+02,,,,,,,,,,,,,,,,,,,,Academia,,,9/8/23 3:48
Maximum Entropy Models for machine translation,Language,Translation,RWTH Aachen and University of Southern California,Academia,Franz Josef Och and Hermann Ney,7/6/02,Discriminative Training and Maximum Entropy Models for Statistical Machine Translation,https://aclanthology.org/P02-1038/,1.41E+03,Highly cited,,,,,,,,5.20E+05,"[WORDS]
Table 1",,,,,,,,,,Academia,,,9/8/23 2:31
Joint Probability Machine Translation,Language,,University of Southern California,Industry - Academia Collaboration,Daniel Marcu and William Wong,6/1/02,"A Phrase-Based, Joint Probability Model for Statistical Machine Translation",https://dl.acm.org/doi/10.3115/1118693.1118711,6.23E+02,,,,,,,Hansard Corpus,,1.07E+06,"[WORDS]
""To evaluate our system, we trained [...] our joint
probability model on a French-English parallel corpus of 100,000 sentence pairs from the Hansard corpus. The sentences in the corpus were at most
20 words long. The English side had a total of 1,073,480 words (21,484 unique tokens). The French side had a total of 1,177,143 words (28,132
unique tokens)""",,,,,,,,,,Industry,,,9/8/23 2:26
NEAT in neuroevolution,,,IDSIA Switzerland,Academia,"Justin Bayer, Daan Wierstra, Julian Togelius, Jürgen Schmidhuber",6/1/02,Evolving Neural Networks through Augmenting Topologies ,https://direct.mit.edu/evco/article/10/2/99/1123/Evolving-Neural-Networks-through-Augmenting,3.37E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/8/23 2:46
Tagging via Viterbi Decoding,Language,,AT&T Labs,Industry,Michael Collins,6/1/02,Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms,https://dl.acm.org/doi/10.3115/1118693.1118694,2.58E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,9/8/23 2:29
Thumbs Up?,Language,Sentiment classification,"Cornell University,IBM Almaden Research Center",Industry - Academia Collaboration,"Bo Pang, Lillian Lee, Shivakumar Vaithyanathan",5/28/02,Thumbs up? Sentiment Classification using Machine Learning Techniques,https://arxiv.org/abs/cs/0205070,1.07E+04,Highly cited,,,,,,IMDb,,2.05E+03,"yielding a corpus of 752 negative and
1301 positive reviews",,,,,,,,,,Industry,,,8/11/23 19:20
Decision tree (classification),Vision,Face recognition,Mitsubishi Electric Research Labs and Compaq CRL,Industry - Academia Collaboration,"P. Viola, M. Jones",12/8/01,Rapid object detection using a boosted cascade of simple features,https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf,2.34E+04,Highly cited,,1.20E+08,"From table 1, it looks like the number of weights depends on the dataset size, which in this case is 2*4916 faces+9544 non-faces = 19376, and multiplies that by the number of filters T = 6061, so no. of params = 1.2e8 (Note:I think ""features"" = ""filters"" in this paper)",6.30E+13,"
The training compute can be tediously worked out from the pseudocode. I think for dataset size D, number of filters T, the training compute is roughly 180k * D * 3 * T = 6.3e13 FLOPs",,They scraped the dataset personally for training,1.45E+04,Section 5: 4916 hand labeled faces  + 9544 non-face images = 14460,6.70E+07,"The inference compute depends on the image - because the algorithm works via pass/fail conditions of the decision tree, I think the compute varies a lot (e.g. if the first image fails then little compute is needed). They claim to take about 0.067s to classify an image using a 700MHz Pentium III processor - I'm not sure about how many FLOPs this required but an estimate is 1e9 FLOPs, which works out to 6.7e7 FLOPs for inference",,,,,,,,Industry,,,5/29/23 20:51
Gradient Boosting Machine,,,Stanford University,Academia,Jerome H. Friedman,10/1/01,Greedy function approximation: A gradient boosting machine,https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full,1.43E+04,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/8/23 4:21
Immediate trihead,Language,,Brown University,Academia,E Charniak,7/6/01,Immediate-Head Parsing for Language Models,https://dl.acm.org/doi/10.3115/1073012.1073029,4.22E+02,,,,,,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
FrameNet role labeling,Language,,University of Rochester,Academia,"Daniel Gildea, Daniel Jurafsky",9/1/00,Automatic Labeling of Semantic Roles,https://dl.acm.org/doi/10.1162/089120102760275983,2.50E+03,Highly cited,,,,,,FrameNet,,5.00E+04,"Abstract: ""The system is based on statistical classifiers trained on roughly 50,000 sentences""",,,,,,,,,,Academia,,,9/8/23 2:46
Peephole LSTM,Other,Periodic function approximation,IDSIA Switzerland,Academia,F.A. Gers; J. Schmidhuber,7/26/00,Recurrent nets that time and count,https://ieeexplore.ieee.org/document/861302,6.30E+02,,,1.70E+01,"""In absence of the 3 peephole connections there are 14 adjustable weights""",,,,,6.50E+07,See Table 2,,,,,,,,,,Academia,,,5/29/23 20:51
SVD in recommender systems,Recommendation,,University of Minnesota,Academia,"B Sarwar, G Karypis, J Konstan, J Riedl",7/14/00,Application of Dimensionality Reduction in Recommender System -- A Case Study,http://robotics.stanford.edu/~ronnyk/WEBKDD2000/papers/sarwar.pdf,2.13E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/8/23 2:26
Perceptron for Large Margin Classification,Vision,Character recognition,"University of California San Diego & Shannon Laboratory,AT&T Labs",Industry,Yoav Freund & Robert E. Schapire,12/1/99,Large Margin Classification Using the Perceptron Algorithm,https://link.springer.com/article/10.1023/A:1007662407062,1.73E+03,Highly cited,,,,,,MNIST,,6.00E+04,"""The dataset consists of 60,000 training examples and 10,000 test examples.""",,,,,,,,,,Industry,,,9/12/23 19:38
IBM Model 4,Language,Translation,University of Southern California & IBM & University of Pennsylvania,Industry - Academia Collaboration (Academia leaning),"Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky",7/2/99,Statistical machine translation,http://www-i6.informatik.rwth-aachen.de/publications/download/266/al-onaizan--1999.pdf,1.92E+03,Highly cited,,,,,,,,8.00E+05,"[WORDS]
See FIgure 6",,,,,,,,,,Industry,,,5/29/23 20:51
LSTM with forget gates,,,IDSIA Switzerland,Academia,"F. A. Gers, J. Schmidhuber, and F. Cummins",1/2/99,Learning to forget: Continual prediction with LSTM,https://ieeexplore.ieee.org/document/818041,4.52E+03,Highly cited,,2.76E+02,See Table 1,,,,,3.00E+04,"Training was stopped after at most 30000
training streams, each of which was ended
when the first prediction error or the
100000th successive input symbol occurred

NOTE this is a weird task. Not sure how to measure dataset size (#seqs? #symbols?)",,,,,,,,,,Academia,,,5/29/23 20:51
LeNet-5,Vision,Character recognition,AT&T Labs,Industry - Academia Collaboration (Industry leaning),"Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner",11/1/98,Gradient-based Learning Applied to Document Recognition,http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf,3.86E+04,Historical significance,,6.00E+04,"[LeNet5] contains 390408 connections, but only 60000 trainable free parameters because of the weight sharing ",2.81E+12,"""[LeNet5] contains 390408 connections"" = multiply-adds
MNIST - 60,000 data points
20 epochs",MNIST,,6.00E+04,"The MNIST database contains 60,000 training images and 10,000 testing images (Wikipedia)",7.81E+05,,,,,,,,,Industry,,,9/13/23 16:10
,Speech,,Johns Hopkins University,Academia,F Jelinek,1/15/98,Statistical Methods for Speech Recognition,https://mitpress.mit.edu/books/statistical-methods-speech-recognition,3.06E+03,"out of print, can't access it to see if it describes any concrete models",,,,,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!
Probabilistic modeling for object recognition,Vision,Face recognition,Carnegie Mellon University,Academia,"H Schneiderman, T Kanade",6/23/98,Probabilistic modeling of local appearance and spatial relationships for object recognition,https://ieeexplore.ieee.org/document/698586,6.02E+02,,,,,,,,,1.20E+05,"Section 5.1: ""We formed training sets from 991 faces images and 1,552
non-face images.""
""For each face image we generated
120 synthetic variations""",,,,,,,,,,Academia,,,9/12/23 20:27
RNN for speech,Speech,Speech synthesis,National Chiao Tung University,Academia,"SH Chen, SH Hwang, YR Wang",5/15/98,An RNN-based prosodic information synthesizer for Mandarin text-to-speech,https://ieeexplore.ieee.org/abstract/document/668817,2.31E+02,,,7.51E+03,"""The RNN generated a total of eigt output prosodic parameters. [...] The numbers of nodes in the first and second hidden layers were determined empirically and set to be 35 and 30, respectively""

Figure 1 contains an overview of the architecture.

Layer 1: (102 + 35 + 1)*35 parameters
Layer 2: (43 + 35 + 1)*30 parameters
Output layer: (30+8+1)*8 parameters",2.27E+11,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,,,1.41E+04,"The data base was divided into two parts: a training set and an open test set. These two sets consisted of 28 191 and 7051 syllables,
respectively.

Of the top 10,000 Chinese words, 15% have 1 syllable, 78% have 2 syllables, and 7% have more than two syllables. Assuming 2 syllables per word, the training set is around 14100 words.",,,,,,,,,,Academia,,,8/15/23 15:53
Sparse coding model for V1 receptive fields,,,"UC Davis,Cornell University",Academia,"Bruno A. Olshausen, David J. Field",12/1/97,Sparse coding with an overcomplete basis set: A strategy employed by V1?,https://www.sciencedirect.com/science/article/pii/S0042698997001697,4.26E+03,Highly cited,,,,,,,,1.00E+01,"In Simulation Methods: ""The data for training were taken from ten 512 × 512
pixel images of natural surroundings""",,,,,,,,,,Academia,,,9/8/23 1:33
LSTM,Language,Language modelling,The Technical University of Munich,Academia,Sepp Hochreiter ; Jurgen Schmidhuber,11/15/97,Long short-term memory,https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext,5.20E+04,Highly cited,,1.05E+04,"Table 2

http://www.bioinf.jku.at/publications/older/2604.pdf",2.10E+13,"""Due to limited computation time, training is stopped after 5 million sequence presentations""

Each sequence has p=100 elements in the long-delay setting.

COMPUTE = PRESENTATIONS * PRESENTATION LENGTH * UPDATE COMPUTE PER TOKEN",,,1.27E+06,"Table 8. The rightmost column lists numbers of training sequences required to achieve the stopping
criterion.

This applies to experiment 5 (multiplication)

Sequences have random lengths, on the order of 100-1000 (table 7 )",4.20E+04,"Appendix A.1
""LSTM's update complexity per time is [...] K + 2KH + KC + 2KSC + H I + C I + 4CSI steps [...] where K is the number of output units, C is the number of memory cell blocks, S > 0 is the size of the memory cell blocks, H is the number of hidden units, I is the (maximal) number of units forward-connected to memory cells, gate units and hidden units""

""W = KH + KCS + CSI + 2C is the number of weights""

So the update complexity is roughly twice the number of weights.

The authors take 1 FMA = 1 step, so this is roughly 4*W FLOP",,,,,,,,Academia,,,9/15/23 19:14
Bidirectional RNN,Speech,Speech recognition,"ATR Labs,Japan",Industry,"M. Schuster, KK Paliwal",11/1/97,Bidirectional recurrent neural networks,https://ieeexplore.ieee.org/document/650093,6.09E+03,Highly cited,,1.30E+04,"Page 7: ""The structures of all networks are adjusted so that
each of them has about the same number of free parameters
(approximately 13 000 here""",,,TIMIT,,7.39E+04,"""the training data set consisting of 3696 sentences
from 462 speakers""

Assuming avg sentence length of 20 words

3696 * 20 total words",,,,,,,,,,Industry,,,9/8/23 2:02
n-gram LM,Language,,Cambridge University Engineering & Carnegie Mellon University,Academia,"P Clarkson, R Rosenfeld",7/1/97,Statistical language modeling using the CMU-Cambridge toolkit,https://www.semanticscholar.org/paper/Statistical-language-modeling-using-the-toolkit-Clarkson-Rosenfeld/fdf4aa623e4d5b5edaeb873ed8e8b1cef0b59c87,9.54E+02,,,,,,,,,,,,,,,,Supervised,,,,Academia,,,9/8/23 1:35
SVM for face detection,Vision,,Massachusetts Institute of Technology,Academia,"E. Osuna, R. Freund, F. Girosi",6/17/97,Training Support Vector Machines: An Application to Face Detection,https://ieeexplore.ieee.org/document/609310,3.85E+03,Highly cited,,,,,,,,5.00E+04,"Section 1: ""The problem that we have to solve involves training a classifier
to discriminate between face and non-face patterns, using a
data set of 50,000points. """,,,,,,,,,,Academia,,,9/12/23 22:27
HMM Word Alignment,Language,Word alignment,University of Erlangen - Nuremburg,Academia,"Stephan Vogel, Hermann Ney, Christoph Tillmann",8/5/96,HMM-Based Word Alignment in Statistical Translation,https://dl.acm.org/doi/10.3115/993268.993313,1.10E+03,Highly cited,,,,,,,,4.42E+05,"[WORDS]
Table 1.
I take the sum of all words. Maybe it would be better to use only the sum of English or German words?",,,,,,Supervised,,,,Academia,,,6/8/23 0:39
System 11,Vision,Face detection,Carnegie Mellon University,Academia,"HA Rowley, S Baluja, T Kanade",6/18/96,Neural Network-Based Face Detection,https://ieeexplore.ieee.org/document/655647,6.01E+03,Highly cited,,6.45E+03,"System 11 is a combination of Network 1 and Network 2

Network 1 has 2095 connections and network 2 has 4357 connections (see table 1)",1.29E+10,"Since there is no parameter sharing, the forward compute is roughly twice that of the number of parameters. We use a 2:1 forward-backward ratio as this is a shallow network, with most connections in the first layer.

Number of passes (Section 2.1):
* ""Nearly 1,050 face examples were gathered from face databases [...]""
* ""Fifteen face examples are generated for the training set from each original image""

Training loop:
1. ""initial set of nonface images by generating 1,000 random images""
2. Train (presumably on whole set)
3. Run + collect false positives
4. ""Select up to 250 of these subimages [...] and add them into the training set [...] Go to step 2""

""A typical training run selects approximately 8,000 nonface images ""

Selecting 8,000 nonface images implies 8000/250 = 32 loops.

Assuming compute is 3 * N * D, we have
* Loop 1: D = 15*1050 + 1000
* Loop 2: D = 15*1050 + 1000 + 250
* So on.

Hence D overall is 32*(15*1050 + 1000) + 250*32/2*(32+1) = 668,000.

Hence compute = 3 * 6452 * 668e3 = 1.3e10.",,,9.05E+03,"""A typical training
run selects approximately 8000 non-face images from the
146,212,178 subimages that are available at all locations
and scales in the training scenery images.""

""Nearly 1050 face examples were gathered from face databases at CMU and Harvard [...] In the training set,15 face examples are generated from each
original image [...]""

""Create an initial set of non-face images by generating
1000 images with random pixel intensities""",1.29E+04,The connections are linear so roughly twice the number of parameters,,,,,,,,Academia,,,8/3/23 20:41
Support Vector Machines,,,AT&T Bell Laboratories,Industry,"C Cortes, V Vapnik",9/1/95,Support-Vector Networks,https://link.springer.com/article/10.1007/BF00994018,4.90E+04,Highly cited,,1.00E+08,"Section 6.2.2: ""...polynomials
of degree 4 (that have more than 10^8 free parameters)...""
They used 4-degree polynomials for MNIST",,,MNIST,,6.00E+04,"Section 6.2: ""The large database consists of 60,000 training and 10,000 test patterns""",,,,,,,,,,Industry,,,5/29/23 20:51
Random Decision Forests,,,AT&T Bell Laboratories,Industry,TK Ho,8/14/95,Random decision forests,https://ieeexplore.ieee.org/document/598994,4.68E+03,Highly cited,,,,,,MNIST,,6.00E+04,The images are from the 1992 NIST (National Institute of Standards and Technology) Competition,,,,,,,,,,Industry,,,5/29/23 20:51
Iterative Bootstrapping WSD,Language,,University of Pennsylvania,Academia,D Yarowsky,6/26/95,Unsupervised Word Sense Disambiguation Rivaling Supervised Methods,https://dl.acm.org/doi/10.3115/981658.981684,3.00E+03,Highly cited,,,,,,,,4.60E+08,the data were extracted from a 460 million word corpus,,,,,,,,,,Academia,,,9/8/23 2:00
Multi-cause Binary Clustering,,,Xerox,Industry,Eric Saund,1/1/95,A Multiple Cause Mixture Model for Unsupervised Learning,https://ieeexplore.ieee.org/document/6795568,1.76E+02,,,,,,,,,,,,,,,,,,,,Industry,,,9/8/23 2:00
GroupLens,Recommendation,,Massachusetts Institute of Technology,Academia,"Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Peter Bergstrom, John Riedl",10/22/94,GroupLens: an Open Architecture for Collaborative Filtering of Netnews,https://dl.acm.org/doi/10.1145/192844.192905,7.73E+03,Highly cited,,1.00E+08,"For each pair of users, the system computes the correlation between their scores in the articles they have rated.

Then to make the prediction of a score for a given article and user the system computes a weighted average taking into account the correlations with each other user, the average rating of each user and the average rating of the article.

So the system in total has n+m+n*n ~= n*n parameters, where n is the number of users and m is the number of articles.

To address scaling issues, the system is partioned into clusters of users. It's very unclear what is the number of users per cluster, though the Daily ratings traffic table provided suggests that is around 10k users ",,,,,1.00E+08,"For each pair of users, the system computes the correlation between their scores in the articles they have rated.

Then to make the prediction of a score for a given article and user the system computes a weighted average taking into account the correlations with each other user, the average rating of each user and the average rating of the article.

So the system in total has n+m+n*n ~= n*n parameters, where n is the number of users and m is the number of articles.

To address scaling issues, the system is partioned into clusters of users. It's very unclear what is the number of users per cluster, though the Daily ratings traffic table provided suggests that is around 10k users ",,,,,,,,,,Academia,,,9/22/23 23:34
Markov-driven POS tagger,Language,Part-of-speech tagging,EURECOM,Academia,Bernard Merialdo,6/1/94,Tagging English Text with a Probabilistic Model,https://dl.acm.org/doi/10.5555/972525.972526,7.88E+02,,,2.45E+06,"""The total number of free parameters is then:
(Nw - 1).NT + (NT - 1).NT.NT.""
Where:
Nw= Vocabulary size
NT = Number of tags

""In the treebank 159 different tags are used. These tags were projected on a smaller system of 76 tags designed by Evelyne Tzoukermann and Peter Brown (see Appendix). The results quoted in this paper all refer to this smaller system""
So NT = 76

https://www.aclweb.org/anthology/J94-2001/

There is no direct reference to Nw, but the data is from ""Lexicon and grammar in probabilistic tagging of written English."" which says

""(the new CLAWS lexicón has almost 26,500 entries)""
So tentatively Nw=26500

https://dl.acm.org/doi/10.3115/982023.982049",,,,,1.00E+06,"""We use the ""treebank"" data described in Beale (1988). It contains 42,186 sentences (about one million words) from the Associated Press.""
https://www.aclweb.org/anthology/J94-2001.pdf",,,,,,,,,,Academia,,,9/8/23 1:57
IBM-5,Language,Translation,IBM T.J. Watson Research Center,Industry,"Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, Robert L. Mercer",6/15/93,The Mathematics of Statistical Machine Translation: Parameter Estimation,https://dl.acm.org/doi/10.5555/972470.972474,5.75E+03,Highly cited,,1.66E+06,"The model is initiallized with 2.44E+09 translation probabilities, which are progressively culled until 1,658,364 remain. There are other parameters in the models (eg the fertility probabilities that relate each word in the input to the number of words it will align to) but the parameter count is dominated by the translation probabilities.",,,Proceedings of the Canadian parliament,,5.34E+07,"""They used the algorithm to extract a large number of translations from several years of the proceedings of the Canadian parliament. From these translations, we have chosen as our training data those for which both the English sentence and the French sentence are 30 or fewer words in length. This is a collection of 1,778,620 translations.""",,,,,,,,,,Industry,,,5/29/23 20:51
Fuzzy NN,Speech,Speech recognition,Indian Statistical Institute,Academia,"SK Pal, S Mitra",9/1/92,"Multilayer perceptron, fuzzy sets, and classification",https://ieeexplore.ieee.org/document/159058,1.22E+03,Highly cited,,1.17E+03,"Table II: ""he neural network has three hidden layers, with m hidden nodes in each layer"", m = 20, input dim. = 9, output dim. = 6",1.40E+09,1166 params * 2 FLOP/param * (3 for forward + backward pass) * 460 epochs * 436 examples,,,4.36E+02,"""The above-mentioned algorithm was tested on a set of 871 Indian Telugu vowel sounds"" and 50% of the dataset was used. 871*0.5 ~= 436",,,,,,,,,,Academia,,,6/21/23 19:17
REINFORCE in Stochastic Connectionism,,,Northeastern University,Academia,R. J. Williams,5/1/92,Simple statistical gradient-following algorithms for connectionist reinforcement learning,https://dl.acm.org/doi/10.1007/BF00992696,6.53E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/8/23 1:57
TD-Gammon,Games,Backgammon,IBM,Industry,G Tesauro,5/1/92,Practical Issues in Temporal Difference Learning,https://papers.nips.cc/paper/1991/file/68ce199ec2c5517597ce0a4d89620f55-Paper.pdf,1.34E+03,Highly cited,,2.50E+04,"The best performance was obtained with a network containing 80 hidden units and over 25,000 weights.",1.82E+13,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,,,6.30E+06,"""This network was trained
for over 300,000 training games""

Each backgammon game has an avg of around 21 movements
https://www.bkgm.com/rgb/rgb.cgi?view+712",,,,,,,,,,Industry,,,5/29/23 20:51
SRN-Encoded Grammatical Structures,Language,,"University of California,San Diego",Academia,J. L. Elman,9/1/91,"Distributed representations, simple recurrent networks, and grammatical structure",https://dl.acm.org/doi/10.1007/BF00114844,1.72E+03,Highly cited,,,,,,,,1.78E+05,4 training sets of 10k sentences each. Total number of words calculated by multiplying 10k and the avg. number of words per sentence in the training set.,,,,,,,,,,Academia,,,9/8/23 1:44
DIABETES,Other,Medical diagnosis,"Aalborg University,University of London",Academia,"S. Andreassen, R. Hovorka, J. Benn, K. G. Olesen, and E. R. Carson",6/24/91,A Model-based Approach to Insulin Adjustment,https://link.springer.com/chapter/10.1007/978-3-642-48650-0_19,1.32E+02,,,4.29E+05,From https://www.bnlearn.com/bnrepository/,,,,,,,,,,,,,,,,Academia,,,8/15/23 13:15
MLP as Bayesian Approximator,,,"Air Force Institute of Technology,OH,USA",Academia,D.W. Ruck & S.K. Rogers & M. Kabrisky & M.E. Oxley & B.W. Suter,12/1/90,The multilayer perceptron as an approximation to a Bayes optimal discriminant function,https://ieeexplore.ieee.org/abstract/document/80266,1.05E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/8/23 1:36
MADALINE III,,,Stanford University,Academia,"B Widrow, M. A. Lehr",9/1/90,"30 years of adaptive neural networks: perceptron, madaline, and backpropagation",https://ieeexplore.ieee.org/document/58323,3.01E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/6/23 18:16
ALVINN,Driving,,Carnegie Mellon University ,Academia,DA Pomerleau,12/1/89,ALVINN: an autonomous land vehicle in a neural network,https://proceedings.neurips.cc/paper/1988/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html,1.58E+03,Highly cited,,3.99E+03,http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15782-f06/slides/alvinn.pdf,8.12E+10,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,Road snapshots,,1.20E+03,"""Training involves first creating a set of 1200 road snapshots depicting roads with a wide variety of retinal orientations and positions, under a variety of lighting conditions and with realistic noise levels""",,,,,,,,,,Academia,,,5/29/23 20:51
Zip CNN,Vision,Character recognition,AT&T Bell Laboratories,Industry,Y. LeCun B. Boser J. S. Denker D. Henderson R. E. Howard W. Hubbard L. D. Jackel,12/1/89,Backpropagation applied to handwritten zip code recognition,https://ieeexplore.ieee.org/document/6795724,9.05E+03,Highly cited,,9.76E+03,"In summary, the network has 1256 units, 64,660 connections, and 9760 independent parameters",4.34E+10,"Its a deep CNN so we assume a backward-forward ratio of 2:1

""The network was trained for 23
passes through the training set (167,693 pattern presentations).""",Buffalo zips,"""The data base used to train and test the network consists of 9298 segmented numerals digitized from handwritten zip codes
that appeared on U.S. mail passing through the Buffalo, NY post office.
Examples of such images are shown in Figure 1. The digits were written
by many different people, using a great variety of sizes, writing styles,
and instruments, with widely varying amounts of care; 7291 examples
are used for training the network and 2007 are used for testing the generalization performance. One important feature of this data base is that
both the training set and the testing set contain numerous examples that
are ambiguous, unclassifiable, or even misclassified. """,7.29E+03,"The digits were written
by many different people, using a great variety of sizes, writing styles,
and instruments, with widely varying amounts of care; 7291 examples
are used for training the network and 2007 are used for testing the generalization performance",1.29E+05,Roughly twice the number of connections,,,,,,,,Industry,,,6/8/23 0:39
Universal approximation via Feedforward Networks,,,Technische Universität Wien Austria & University of California,Academia,Kurt Hornik & Maxwell Stinchcombe & Halbert White,3/9/89,Multilayer feedforward networks are universal approximators,https://www.sciencedirect.com/science/article/abs/pii/0893608089900208,2.17E+04,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/8/23 1:42
Time-delay neural networks,,,Carnegie Mellon University & ATR Interpreting Telephony Research Laboratories & University of Toronto ,Industry - Academia Collaboration,"A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang",3/3/89,Phoneme recognition using time-delay neural networks,https://ieeexplore.ieee.org/abstract/document/21701,3.45E+03,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
Innervator,Other,Pattern classification,"CalTech,Stanford University",Academia,"Geoffrey Miller, Peter Todd, and Shailesh Hegde",1/1/89,Designing neural networks using genetic algorithms,https://www.researchgate.net/publication/220885651_Designing_Neural_Networks_using_Genetic_Algorithms,1.13E+03,Highly cited,,1.00E+01,Each net has 5 units,1.20E+08,10 params * 6 FLOP/param/pass * 4 datapoints * 1000 epochs * 50 individuals * 10 generations,,,4.00E+00,,,,,,,,,,,,,,9/6/23 18:16
Q-learning,,,University of London,Academia,Christopher Watkins,1/1/89,Learning from delayed rewards,http://www.cs.rhul.ac.uk/~chrisw/thesis.html,8.03E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
Adaptive Broom Balancer,Games,Pole balancing,Stanford University,Academia,"VV Tolat, B Widrow",7/24/88,An Adaptive “Broom Balancer” with Visual Inputs,https://ieeexplore.ieee.org/document/23982,8.00E+01,,,1.10E+02,Figure 3,,,,,,,,,,,,,,,,Academia,,,6/8/23 0:39
,Vision,,"University of California, Santa Cruz",Academia,"Biederman, Irving",4/1/87,Recognition-by-components: A theory of human image understanding,https://psycnet.apa.org/record/1987-20898-001,7.59E+03,"theoretical, no model created",,,,,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
MADALINE II,Other,Pattern classification,Stanford University,Academia,"Rodney Winter, Bernard Widrow",7/24/88,MADALINE RULE II: A Training Algorithm for Neural Networks,https://ieeexplore.ieee.org/document/23872,8.10E+01,,,,,,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
Motion-Driven 3D Feature Tracking,Vision,,Roke Manor Research,Industry,Harris & Stephens,7/1/88,A Combined Corner and Edge Detector,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.434.4816&rep=rep1&type=pdf,1.91E+04,Highly cited,,,"""The simulation studies reported here all involved a 16-bit input pattern. """,,,,,1.50E+03,"""The total number of possible input patterns was 65,536. Training sets of 650 and 1500 patterns picked at random from this total were used.""",,,,,,,,,,Industry,,,9/8/23 1:41
NetTalk,Speech,Speech synthesis,Princeton University,Academia,"TJ Sejnowski, CR Rosenberg",6/6/87,Parallel Networks that Learn to Pronounce English Text,http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=03A3D3EDF0BAF35405ABCF083411B55E?doi=10.1.1.154.7012&rep=rep1&type=pdf,2.56E+03,Highly cited,,1.86E+04,"The connections in the network are specified by a total of 18629
weight parameters (including a variable threshold for each unit)",2.77E+10,18629 params * 2 FLOP/param * (3 for forward + backward pass) * 55 epochs * 1000 words/epoch * 4.5 letters/word,,,2.10E+04,"Learning. Two texts were used to train the network: phonetic transcriptions from informal, continuous speech of a child [9] and Miriam Webster’s Pocket Dictionary.

We used the first two pages of transcriptions, which contained 1024 words from a child in firstgrade

Dictionary. The Miriam Webster’s Pocket Dictionary that we used
had 20,012 words.

Thus 1024+20012 = 21036",,,,,,,,,,Academia,,,6/21/23 19:07
Optimized Multi-Scale Edge Detection,Vision,,Massachusetts Institute of Technology,Academia,John Canny,11/1/86,A Computational Approach To Edge Detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4767851,3.79E+04,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/8/23 1:37
Back-propagation,Other,Learning to complete triples,University of California,Academia,"Rumelhart, David E.; Hinton, Geoffrey E.; Williams, Ronald J.",10/1/86,Learning representations by back-propagating errors,https://www.semanticscholar.org/paper/Learning-representations-by-back-propagating-errors-Rumelhart-Hinton/052b1d8ce63b07fec3de9dbb583772d860b7c769,2.53E+04,Highly cited,,1.44E+02,Figure 4 includes a representation of the weights learned by the people to relationship network,1.24E+08,"We assume that the number of mult-adds per pass is equal to the number of parameters.

""We trained the network for 1500 sweeps""

There are 12*12 possible pairs of people, so we assume that is the dataset size",,,1.44E+02,"There are 12*12 possible pairs of people, so we assume that is the dataset size",2.88E+02,We assume that the number of mult-adds is equal to the number of parameters.,,,,Unsupervised,,,,Academia,,,5/29/23 20:51
PDP model for serial order,,,University of California,Academia,"Jordan, M.I.",1/5/86,Serial order: A parallel distributed processing approach,https://www.osti.gov/biblio/6910294,1.50E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/8/23 1:38
Error Propagation,,,University of California and University of Carnegie Mellon,Academia,"D. E. Rumelhart, G. E. Hinton, and R. J. Williams",1/3/86,Learning internal representations by error propagation,https://stanford.edu/~jlmcc/papers/PDP/Volume%201/Chap8_PDP86.pdf,2.73E+04,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/12/23 20:50
Learning past tenses,Language,Verb conjugation,Stanford University,Academia,"Rumelhart, D. E., & McClelland, J. L",1/3/86,Learning the past tenses of English verbs: Implicit rules or parallel distributed processing?,https://www.semanticscholar.org/paper/On-learning-the-past-tenses-of-English-verbs%3A-rules-Rumelhart-McClelland/4fa569625b5ab35e955a8d5be11a4aa9f59ca424,3.18E+02,,,2.12E+05,"Source: https://files.eric.ed.gov/fulltext/ED267419.pdf
p.9: network architecture is given, with two layers of hidden units. The hidden units are  called “Wickelfeature representation”. The “modifiable connections” are only between the hidden units. p.19: “All in all then, we used only 460 of the 1,210 possible Wickelfeatures. Using this representation, a verb is represented by a pattern of activation over a set of 460 Wickelfeature units.""",,,,,,,,,,,,,,,,Industry,,,9/6/23 18:15
Learnability theory of language development,Language,,Massachusetts Institute of Technology,Academia,Steven Pinker,7/1/84,Language learnability and language development,https://psycnet.apa.org/record/1985-97439-000,4.73E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/22/23 23:34
ASE+ACE,Games,Pole balancing,Stanford University,Academia,"Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson",9/1/83,Neuronlike adaptive elements that can solve difficult learning control problems,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6313077,4.30E+03,Highly cited,,3.24E+02,,,,,,,,,,,,,,,,,Academia,,,9/6/23 18:15
Hopfield network,Other,Sequence memorization,California Institute of Technology,Academia,JJ Hopfield,4/1/82,Neural networks and physical systems with emergent collective computational abilities,https://www.pnas.org/doi/10.1073/pnas.79.8.2554,2.33E+04,Highly cited,,9.90E+03,"My understanding is that the biggest Hopfield networks they studied had N=100 units. 

Each unit has 99 synapses Tij from each other unit, for a total of 100*99 parameters",,,,,,,,,,,,,,,,Academia,,,7/13/23 0:37
Kohonen network,Other,Dimensionality reduction,Helsinki University of Technology,Academia,T Kohonen,7/25/81,Self-organized formation of topologically correct feature maps,https://link.springer.com/article/10.1007/BF00337288,1.18E+04,Highly cited,,4.10E+03,"The input vectors are 3D.
I could not find the grid size, but from the images it looks 8x8.
So the network was 8x8x3 parameters.",,,,,,??? Seemingly no info,,,,,,,,,,Academia,,,5/29/23 20:51
Neocognitron,Vision,Character recognition,NHK Broadcasting Science Research Laboratories,Industry,"K Fukushima, S Miyake",4/1/80,Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position,https://link.springer.com/article/10.1007/BF00344251,5.78E+03,Highly cited,,1.14E+06,"""
The synaptic connections from S-layers to C-layers
are fixed and unmodifiable. [...]
The numbers of excitatory cells in these seven layers are: 16x16 in U0, 16x16x24 in Us1, 10x10x 24 in Uc1, 8x8x24 in Us2, 6x 6x 24 in Uc2, 2x2x24 in Us3, and 24 in Uc3 
[...]
 the number of input synapses to each S-cell is 5 x 5 in layer Us1 and 5x5x24 in layers Us2 and Us3
[...]
The number of excitatory input synapses to each C-cell is 5x5 in layers Uc1 and Uc2, and is 2x2 in
layer Uc3
""

The number of synapses into each S-layer is:

S1: (16*16*24)*(5*5) 
S2: (8*8*24)*(5*5*24)
S3: (2*2*24)*(5*5*24)

We assume one parameter a per synapse into each cell in a S-layer, and one parameter b per each cell in a S-layer.",2.28E+08,"""It does not necessarily mean that all of these input synapses are
always fully reinforced. In usual situations, only some of these input synapses are reinforced, and the rest of them remains in small values [...] Each of the five stimulus patterns has been presented 20 times to the network. By that time, self organization of the network has almost been completed.""

We multiply by 2 to account for multadds
",,,5.00E+00,"""In order to self-organize the network, we have presented five stimulus patterns ""0"", ""1"", ""2"", ""3"", and ""4"", which are shown in Fig. 6""",,,,,,,,,,Industry,,,5/29/23 20:51
Internal functionality of visual invariants,Vision,,Utrecht University,Academia,Koenderink & van Doom,5/2/79,The internal representation of solid shape with respect to vision,https://link.springer.com/article/10.1007/BF00337644,9.81E+02,Historical significance,,,,,,,,,??? Seemingly no info,,,,,,,,,,Academia,,,9/13/23 16:10
TD(0),,,University of Essex,Academia,Ian Witten,8/1/77,An adaptive optimal controller for discrete-time Markov environments,https://www.sciencedirect.com/science/article/pii/S0019995877903540,2.69E+02,Historical significance,,,,,,,,,??? Seemingly no info,,,,,,,,,,Academia,,,9/13/23 16:10
Cognitron,,,Biological Cybernetics,Industry,Kunihiko Fukushima,9/1/75,Cognitron: a self-organizing multilayered neural network,https://link.springer.com/article/10.1007%2FBF00342633,7.91E+02,Historical significance,Precursor of the Neocognitron,,,,,,,,??? Seemingly no info,,,,,,,,,,Industry,,,9/13/23 16:10
Naive Bayes,Vision,,Stanford Research Institute,Industry,Duda and Hart,9/1/74,Pattern Classification and Scene Analysis,https://www.semanticscholar.org/paper/Pattern-classification-and-scene-analysis-Duda-Hart/b07ce649d6f6eb636872527104b0209d3edc8188,2.31E+04,Highly cited,,,,,,,,,,,,,,,,,,,Industry,,,5/29/23 20:51
Punish/Reward,Games,Blackjack,IEEE,Academia,"Widrow, Gupta, and Maitra",9/1/73,Punish/Reward: Learning with a Critic in Adaptive Threshold Systems,https://ieeexplore.ieee.org/document/4309272,3.97E+02,,,2.10E+01,"Fig. 1 shows that there is a bias term, while Fig. 5 shows that the input is a sequence of 20 bits, corresponding to 20 weights. So the total number of parameters is 21.",,,,,,??? Seemingly no info,,,,,,,,,,Academia,,,7/10/23 14:25
Graph-based structural reasoning,,,Massachusetts Institute of Technology,Academia,Patrick Winston,9/1/70,Learning Structural Definitions from Examples,https://dspace.mit.edu/handle/1721.1/6884,1.81E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/8/23 1:22
BOXES,Games,Pole balancing,University of Edinburgh,Academia,Michie and Chambers,7/1/68,Boxes: An Experiment in Adaptive Control,https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.474.2430,5.90E+02,Historical significance,,,,,,,,,,,,,,,,,,,Academia,,,9/13/23 16:10
GLEE,Games,Tic Tac Toe,University of Edinburgh,Academia,Michie and Chambers,7/1/68,Boxes: An Experiment in Adaptive Control,https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.474.2430,5.90E+02,Historical significance,,,,,,,,,,,,,,,,,,,Academia,,,9/13/23 16:10
Samuel Neural Checkers II,Games,Checkers,University of Geneva,Academia,"Palmieri, G. and R. Sanna",11/1/67,Some studies in machine learning using the game of checkers. Part II,https://www.cs.virginia.edu/~evans/greatworks/samuel.pdf,7.47E+02,,,4.00E+01,"""The total number of parameters used at any one time has been varied from a very few to as many as 40""",,,,,,,,,,,,,,,,Academia,,,7/25/23 18:00
MENACE,Games,Tic Tac Toe,University of Edinburgh,Academia,Donald Michie,11/1/63,Experiments on the Mechanization of Game-Learning Part I. Characterization of the Model and its parameters,https://academic.oup.com/comjnl/article/6/3/232/360077,4.60E+01,,,,,,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
STeLLA,,,University of Canterbury,Academia,J.H. Andreae and Peter L. Joyce,6/1/63,STeLLA: A Scheme for a Learning Machine,https://www.researchgate.net/publication/252919025_STELLA_A_scheme_for_a_learning_machine,3.40E+01,,,,,,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
MADALINE I,,,Stanford University,Academia,William Combs Ridgway,7/1/62,An adaptive logic system with generalizing properties,https://www.proquest.com/openview/7898314db50a218b58052ac91e3bde1e/1?,7.50E+01,Historical significance,,,,,,,,,,,,,,,,,,,Academia,,,9/13/23 16:10
PAPA,,Binary classification,The University of Genoa,Academia,"A Gamba, L Gamberini, G Palmieri, R Sanna",9/1/61,Further experiments with PAPA,https://www.semanticscholar.org/paper/Further-experiments-with-PAPA-Gamba-Gamberini/c3a20b9aa86033cec29f08e69f4bc81e8b329ae2,2.40E+01,,,,,,,,,,,,,,,,,,,,Academia,,,9/7/23 17:30
Heuristic problem solving for AI,,,Massachusetts Institute of Technology,Academia,Marvin Minsky,1/1/61,Steps Toward Artificial Intelligence,https://ieeexplore.ieee.org/abstract/document/4066245,2.43E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,9/12/23 22:27
ADALINE,Vision,Pattern recognition,Stanford University,Academia,Widrow and Hoff,6/30/60,Adaptive switching circuits,https://isl.stanford.edu/~widrow/papers/c1960adaptiveswitching.pdf,6.33E+03,Highly cited,,1.70E+01,"""The machine's total experience is stored in the values of the weights a0,...,a16""",9.90E+03,"""The method of searching that has proven most useful is the method of steepest descent""

Apparently each pattern was only shown once to the system.

So the training compute is (forward pass compute) * (3 for backprop) * dataset size",,,1.00E+02,"""The best system, arrived at by slow precise adaptation on the full body of 100 noisy patterns, was able to classify these patterns as desired except for twelve errors.""

https://isl.stanford.edu/~widrow/papers/c1960adaptiveswitching.pdf",3.30E+01,We have 16 weights and a bias parameter. So 16 multadds and an add. The result is then thresholded to produce a binary output.,,,,,,,,Academia,,,5/29/23 20:51
LMS,,,Stanford University,Academia,Widrow and Hoff,6/30/60,Adaptive switching circuits (technical report),https://www.scirp.org/(S(351jmbntvnsjt1aadkposzje))/reference/ReferencesPapers.aspx?ReferenceID=547230,6.33E+03,Highly cited,,,,,,,,,,,,,,,,,,,Academia,,,5/29/23 20:51
#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!
#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!
#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!,#REF!